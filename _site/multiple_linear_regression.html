<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.4">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Albertus J. Smit">
<title>
&lt;span id="sec-multiple-linear-regression" class="quarto-section-identifier"&gt;&lt;span class="chapter-number"&gt;5&lt;/span&gt;&nbsp; &lt;span class="chapter-title"&gt;Multiple Linear Regression&lt;/span&gt;&lt;/span&gt; | Albertus J. Smit – The Biostatistics Book</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./generalised_linear_models.html" rel="next">
<link href="./polynomial_regression.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script><script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script><script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script><script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./logo.png" alt="" class="navbar-logo"></a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">The Biostatistics Book</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/ajsmit/BCB_Stats" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item quarto-navbar-tools-item" href="./The-Biostatistics-Book.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="./The-Biostatistics-Book.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
</div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-1">
<li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
</div>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part_A.html">Parametric Methods</a></li><li class="breadcrumb-item"><a href="./multiple_linear_regression.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part_A.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parametric Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Correlation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./simple_linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./polynomial_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Polynomial Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multiple_linear_regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./generalised_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Generalised Linear Models (GLM)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./non-linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonlinear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regularisation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regularisation Techniques</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part_B.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Non-Parametric Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./assumption_tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Testing Assumptions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quantile_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quantile Regression</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part_C.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Semi-Parametric Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./generalised_additive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Generalised Additive Models</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Appendix A</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
<li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link active" data-scroll-target="#multiple-linear-regression"><span class="header-section-number">5.1</span> Multiple Linear Regression</a></li>
  <li><a href="#nature-of-the-data" id="toc-nature-of-the-data" class="nav-link" data-scroll-target="#nature-of-the-data"><span class="header-section-number">5.2</span> Nature of the Data</a></li>
  <li><a href="#assumptions" id="toc-assumptions" class="nav-link" data-scroll-target="#assumptions"><span class="header-section-number">5.3</span> Assumptions</a></li>
  <li><a href="#outliers" id="toc-outliers" class="nav-link" data-scroll-target="#outliers"><span class="header-section-number">5.4</span> Outliers</a></li>
  <li><a href="#sec-r-function" id="toc-sec-r-function" class="nav-link" data-scroll-target="#sec-r-function"><span class="header-section-number">5.5</span> R Function</a></li>
  <li><a href="#sec-example1" id="toc-sec-example1" class="nav-link" data-scroll-target="#sec-example1"><span class="header-section-number">5.6</span> Example 1: The Seaweed Dataset</a></li>
  <li><a href="#sec-mlr-interaction" id="toc-sec-mlr-interaction" class="nav-link" data-scroll-target="#sec-mlr-interaction"><span class="header-section-number">5.7</span> Example 2: Interaction of Distance and Bioregion</a></li>
  <li><a href="#example-3-the-final-model" id="toc-example-3-the-final-model" class="nav-link" data-scroll-target="#example-3-the-final-model"><span class="header-section-number">5.8</span> Example 3: The Final Model</a></li>
  <li><a href="#sec-contrasts" id="toc-sec-contrasts" class="nav-link" data-scroll-target="#sec-contrasts"><span class="header-section-number">5.9</span> Alternative Categorical Variable Coding Schemes (Contrasts)</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">5.10</span> Exercises</a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ajsmit/BCB_Stats/edit/main/multiple_linear_regression.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part_A.html">Parametric Methods</a></li><li class="breadcrumb-item"><a href="./multiple_linear_regression.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-multiple-linear-regression" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://tangledbank.netlify.app">Albertus J. Smit</a> <a href="https://orcid.org/0000-0002-3799-6126" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://uwc.ac.za">
            University of the Western Cape
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header><p>In <a href="simple_linear_regression.html#sec-simple-linear-regression" class="quarto-xref"><span>Section 3.1</span></a> we have seen how to model the relationship between two variables using simple linear regression (SLR). However, in ecosystems, the relationship between the response variable and the explanatory variables is more complex and in many cases cannot be adequately captured by a single driver (i.e.&nbsp;influential or predictor variable). In such cases, multiple linear regression (MLR) can be used to model the relationship between the response variable and multiple explanatory variables.</p>
<section id="multiple-linear-regression" class="level2" data-number="5.1"><h2 data-number="5.1" class="anchored" data-anchor-id="multiple-linear-regression">
<span class="header-section-number">5.1</span> Multiple Linear Regression</h2>
<p>Multiple linear regression helps us answer questions such as:</p>
<ul>
<li>How do various environmental factors influence the population size of a species? Factors like average temperature, precipitation levels, and habitat area can be used to predict the population size of a species in a given region. Which of these factors are most important in determining the population size?</li>
<li>What are the determinants of plant growth in different ecosystems? Variables such as soil nutrient content, water availability, and light exposure can help predict the growth rate of plants in various ecosystems. How do these factors interact to influence plant growth?</li>
<li>How do genetic and environmental factors affect the spread of a disease in a population? The incidence of a disease might depend on factors like genetic susceptibility, exposure to pathogens, and environmental conditions (e.g., humidity and temperature). What is the relative importance of these factors in determining the spread of the disease?</li>
</ul>
<p>Multiple linear regression extends the simple linear regression model to include several independent variables. The model is expressed as: <span id="eq-mlr1"><span class="math display">Y_i = \alpha + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_k X_{ik} + \epsilon_i \tag{5.1}</span></span> Where:</p>
<ul>
<li>
<span class="math inline">Y_i</span> is the response variable for the <span class="math inline">i</span>-th observation,</li>
<li>
<span class="math inline">X_{i1}, X_{i2}, \ldots, X_{ik}</span> are the <span class="math inline">k</span> predictor variables for the <span class="math inline">i</span>-th observation,</li>
<li>
<span class="math inline">\alpha</span> is the intercept,</li>
<li>
<span class="math inline">\beta_1, \beta_2, \ldots, \beta_k</span> are the coefficients for the <span class="math inline">k</span> predictor variables, and</li>
<li>
<span class="math inline">\epsilon_i</span> is the error term for the <span class="math inline">i</span>-th observation (the residuals).</li>
</ul>
<p>When including a categorical variable in a multiple linear regression model, dummy (indicator) variables are used to represent the different levels of the categorical variable. Let’s assume we have a categorical variable <span class="math inline">C</span> with three levels: <span class="math inline">C_1</span>, <span class="math inline">C_2</span>, and <span class="math inline">C_3</span>. We can represent this categorical variable using two dummy variables:</p>
<ul>
<li>
<span class="math inline">D_1</span>: Equals 1 if <span class="math inline">C = C_2</span>, 0 otherwise.</li>
<li>
<span class="math inline">D_2</span>: Equals 1 if <span class="math inline">C = C_3</span>, 0 otherwise.</li>
</ul>
<p><span class="math inline">C_1</span> is considered the reference category and does not get a dummy variable. This way, we avoid multicollinearity (see <a href="#sec-multicollinearity" class="quarto-xref"><span>Section 5.6.4</span></a>). R’s <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function will automatically convert the categorical variables to dummy variables (sometimes called treatment coding). The first level of the alphabetically sorted categorical variable is taken as the reference level. See <a href="regularisation.html#sec-r-function" class="quarto-xref"><span>Section 8.5</span></a> for more information about how to include categorical variables in a multiple linear regression model. At the end of the chapter you’ll find alternative ways to assess categorical variables in a multiple linear regression model (<a href="#sec-contrasts" class="quarto-xref"><span>Section 5.9</span></a>).</p>
<p>Assume we also have <span class="math inline">k</span> continuous predictors <span class="math inline">X_{1}, X_{2}, \ldots, X_{k}</span>. The multiple linear regression model with these predictors and the categorical variable can be expressed as: <span id="eq-mlr2"><span class="math display">Y_i = \alpha + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_k X_{ik} + \gamma_1 D_{i1} + \gamma_2 D_{i2} + \epsilon_i \tag{5.2}</span></span> Where:</p>
<ul>
<li>
<span class="math inline">Y_i</span> is the dependent variable for observation <span class="math inline">i</span>.</li>
<li>
<span class="math inline">\alpha</span> is the intercept term.</li>
<li>
<span class="math inline">\beta_1, \beta_2, \ldots, \beta_k</span> are the coefficients for the continuous independent variables <span class="math inline">X_{i1}, X_{i2}, \ldots, X_{ik}</span>.</li>
<li>
<span class="math inline">D_{i1}</span> and <span class="math inline">D_{i2}</span> are the dummy variables for the categorical predictor <span class="math inline">C</span>.</li>
<li>
<span class="math inline">\gamma_1</span> and <span class="math inline">\gamma_2</span> are the coefficients for the dummy variables, representing the effect of levels <span class="math inline">C_2</span> and <span class="math inline">C_3</span> relative to the reference level <span class="math inline">C_1</span>.</li>
<li>
<span class="math inline">\epsilon_i</span> is the error term for observation <span class="math inline">i</span>.</li>
</ul></section><section id="nature-of-the-data" class="level2" data-number="5.2"><h2 data-number="5.2" class="anchored" data-anchor-id="nature-of-the-data">
<span class="header-section-number">5.2</span> Nature of the Data</h2>
<p>You are referred to the discussion in simple linear regression (<a href="simple_linear_regression.html#sec-simple-linear-regression" class="quarto-xref"><span>Section 3.1</span></a>). The only added consideration is that the data should be multivariate, i.e., it should contain more than one predictor variable. The predictor variables are generally continuous, but there may also be categorical variables.</p>
</section><section id="assumptions" class="level2" data-number="5.3"><h2 data-number="5.3" class="anchored" data-anchor-id="assumptions">
<span class="header-section-number">5.3</span> Assumptions</h2>
<p>Basically, this is as already discussed in simple linear regression (<a href="simple_linear_regression.html#sec-simple-linear-regression" class="quarto-xref"><span>Section 3.1</span></a>)—in multiple linear regression, the same assumptions apply to the response relative to each of the predictor variables. In <a href="#sec-diagnostics" class="quarto-xref"><span>Section 5.6.7</span></a> I will assess the assumptions in an example dataset. An additional consideration is that the predictors must not be highly correlated with each other (multicollinearity) (see <a href="#sec-multicollinearity" class="quarto-xref"><span>Section 5.6.4</span></a>).</p>
</section><section id="outliers" class="level2" data-number="5.4"><h2 data-number="5.4" class="anchored" data-anchor-id="outliers">
<span class="header-section-number">5.4</span> Outliers</h2>
<p>Again, this is as discussed in simple linear regression (<a href="simple_linear_regression.html#sec-simple-linear-regression" class="quarto-xref"><span>Section 3.1</span></a>). In multiple linear regression, the same considerations apply to the response relative to each of the predictor variables.</p>
</section><section id="sec-r-function" class="level2" data-number="5.5"><h2 data-number="5.5" class="anchored" data-anchor-id="sec-r-function">
<span class="header-section-number">5.5</span> R Function</h2>
<p>The <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function in R is used to fit a multiple linear regression model. The syntax is similar to that of the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function used for simple linear regression, but with multiple predictor variables. The function takes the basic form:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">lm</span>(formula, data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For a multiple linear regression with only continuous predictor variables (as in <a href="#eq-mlr1" class="quarto-xref">Equation&nbsp;<span>5.1</span></a>), the formula is:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="fu">lm</span>(response <span class="sc">~</span> predictor1 <span class="sc">+</span> predictor2 <span class="sc">+</span> ... <span class="sc">+</span> predictorN,</span>
<span id="cb2-2"><a href="#cb2-2"></a>   <span class="at">data =</span> dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Interaction effects are implemented by including the product of two variables in the formula. For example, to include an interaction between <code>predictor1</code> and <code>predictor2</code>, we can use:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="fu">lm</span>(response <span class="sc">~</span> predictor1 <span class="sc">*</span> predictor2, <span class="at">data =</span> dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When we have both continuous and categorical predictor variables (<a href="#eq-mlr2" class="quarto-xref">Equation&nbsp;<span>5.2</span></a>), the formula is:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="fu">lm</span>(response <span class="sc">~</span> continuous_predictor1 <span class="sc">+</span> continuous_predictor2 <span class="sc">+</span> ...</span>
<span id="cb4-2"><a href="#cb4-2"></a>   <span class="sc">+</span> continuous_predictorN <span class="sc">+</span> <span class="fu">factor</span>(categorical_predictor1) <span class="sc">+</span></span>
<span id="cb4-3"><a href="#cb4-3"></a>     <span class="fu">factor</span>(categorical_predictor2) <span class="sc">+</span> ...</span>
<span id="cb4-4"><a href="#cb4-4"></a>   <span class="sc">+</span> <span class="fu">factor</span>(categorical_predictorM),</span>
<span id="cb4-5"><a href="#cb4-5"></a>   <span class="at">data =</span> dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="sec-example1" class="level2 page-columns page-full" data-number="5.6"><h2 data-number="5.6" class="anchored" data-anchor-id="sec-example1">
<span class="header-section-number">5.6</span> Example 1: The Seaweed Dataset</h2>
<p>Load some <a href="https://tangledbank.netlify.app/data/seaweed/spp_df2.csv">data</a> produced in the analysis by <span class="citation" data-cites="smit2017seaweeds">Smit et al. (<a href="references.html#ref-smit2017seaweeds" role="doc-biblioref">2017</a>)</span>. Please refer to the chapter <a href="https://tangledbank.netlify.app/BCB743/06-deep_dive.html">Deep Dive into Gradients</a> on Tangled Bank for the data description.</p>
<p>This dataset is suitable for a multiple linear regression because it has continuous response variables (<span class="math inline">\beta_\text{sør}</span>, <span class="math inline">\beta_\text{sim}</span>, and <span class="math inline">\beta_\text{sne}</span>, the Sørenesen dissimilarity, the turnover component of <span class="math inline">\beta</span>-diversity, and the nestedness-resultant component of <span class="math inline">\beta</span>-diversity, respectively), continuous predictor variables (the mean climatological temperature for August, the mean climatological temperature for the year, the temperature range for February and August, and the SD of February and August), and a categorical variable (the bioregional classification of the samples).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a>sw <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"data/spp_df2.csv"</span>)</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="fu">rbind</span>(<span class="fu">head</span>(sw, <span class="dv">3</span>), <span class="fu">tail</span>(sw, <span class="dv">3</span>))[,<span class="sc">-</span><span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       dist  bio    augMean   febRange      febSD     augSD    annMean
1     0.000  BMP 0.00000000 0.00000000 0.00000000 0.0000000 0.00000000
2    51.138  BMP 0.05741369 0.09884404 0.16295271 0.3132800 0.01501846
3   104.443  BMP 0.15043904 0.34887754 0.09934163 0.4188239 0.02602247
968 102.649 ECTZ 0.41496099 0.11330069 0.24304493 0.7538546 0.52278161
969  49.912 ECTZ 0.17194242 0.05756093 0.18196664 0.3604341 0.24445006
970   0.000 ECTZ 0.00000000 0.00000000 0.00000000 0.0000000 0.00000000
              Y        Y1          Y2
1   0.000000000 0.0000000 0.000000000
2   0.003610108 0.0000000 0.003610108
3   0.003610108 0.0000000 0.003610108
968 0.198728140 0.1948882 0.003839961
969 0.069337442 0.0443038 0.025033645
970 0.000000000 0.0000000 0.000000000</code></pre>
</div>
</div>
<p>We will do a multiple linear regression analysis to understand the relationship between some of the environmental variables and the seaweed species. Specifically, we will consider only the variables <code>augMean</code>, <code>febRange</code>, <code>febSD</code>, <code>augSD</code>, and <code>annMean</code> as predictors of the species composition as measured by <span class="math inline">\beta_\text{sør}</span> (<code>Y</code> in the data file).</p>
<p>The model, which we will call <code>full_mod1</code> below, can be stated formally as <a href="#eq-full" class="quarto-xref">Equation&nbsp;<span>5.3</span></a>: <span id="eq-full"><span class="math display">Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + \beta_5 X_5 + \epsilon \tag{5.3}</span></span> Where:</p>
<ul>
<li>
<span class="math inline">Y</span> is the response variable, the mean Sørensen dissimilarity,</li>
<li>the predictors <span class="math inline">X_1</span>, <span class="math inline">X_2</span>, <span class="math inline">X_3</span>, <span class="math inline">X_4</span>, and <span class="math inline">X_5</span> correspond to <code>augMean</code>, <code>febRange</code>, <code>febSD</code>, <code>augSD</code>, and <code>annMean</code>, respectively, and</li>
<li>
<span class="math inline">\epsilon</span> is the error term.</li>
</ul>
<p>But before we jump into multiple linear regression, let’s warm up by first fitting some simple linear regressions.</p>
<section id="simple-linear-models" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="simple-linear-models">Simple Linear Models</h3>
<p>For interest sake, let’s fit simple linear models for each of the predictors against the response variable. Let’s look at relationships between the continuous predictors and the response in the East Coast Transition Zone (<code>ECTZ</code>), ignoring the other bioregions for now. We will first fit the simple linear models and then create scatter plots of the response variable <span class="math inline">\beta_\text{sør}</span> against each of the predictor variables. To these plots, we will add a best fit (regression) lines.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>sw_ectz <span class="ot">&lt;-</span> sw <span class="sc">|&gt;</span> <span class="fu">filter</span>(bio <span class="sc">==</span> <span class="st">"ECTZ"</span>)</span>
<span id="cb7-2"><a href="#cb7-2"></a></span>
<span id="cb7-3"><a href="#cb7-3"></a>predictors <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"augMean"</span>, <span class="st">"febRange"</span>, <span class="st">"febSD"</span>, <span class="st">"augSD"</span>, <span class="st">"annMean"</span>)</span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="co"># Fit models using purrr::map and store in a list</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>models <span class="ot">&lt;-</span> <span class="fu">map</span>(predictors, <span class="sc">~</span> <span class="fu">lm</span>(<span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">"Y ~"</span>, .x)),</span>
<span id="cb7-7"><a href="#cb7-7"></a>                               <span class="at">data =</span> sw_ectz))</span>
<span id="cb7-8"><a href="#cb7-8"></a></span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="fu">names</span>(models) <span class="ot">&lt;-</span> predictors</span>
<span id="cb7-10"><a href="#cb7-10"></a></span>
<span id="cb7-11"><a href="#cb7-11"></a>model_summaries <span class="ot">&lt;-</span> <span class="fu">map</span>(models, summary)</span>
<span id="cb7-12"><a href="#cb7-12"></a>model_summaries</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$augMean

Call:
lm(formula = as.formula(paste("Y ~", .x)), data = sw_ectz)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.180961 -0.059317 -0.008346  0.045695  0.192444 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.060104   0.007359   8.168 1.01e-14 ***
augMean     0.346011   0.010899  31.748  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.07721 on 287 degrees of freedom
Multiple R-squared:  0.7784,    Adjusted R-squared:  0.7776 
F-statistic:  1008 on 1 and 287 DF,  p-value: &lt; 2.2e-16


$febRange

Call:
lm(formula = as.formula(paste("Y ~", .x)), data = sw_ectz)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.21744 -0.08311 -0.01543  0.07536  0.25699 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.092722   0.009638   9.621   &lt;2e-16 ***
febRange    0.181546   0.008897  20.405   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1048 on 287 degrees of freedom
Multiple R-squared:  0.592, Adjusted R-squared:  0.5905 
F-statistic: 416.4 on 1 and 287 DF,  p-value: &lt; 2.2e-16


$febSD

Call:
lm(formula = as.formula(paste("Y ~", .x)), data = sw_ectz)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.24267 -0.10709 -0.02587  0.08888  0.39171 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.12018    0.01168   10.29   &lt;2e-16 ***
febSD        0.17166    0.01245   13.79   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.1272 on 287 degrees of freedom
Multiple R-squared:  0.3985,    Adjusted R-squared:  0.3964 
F-statistic: 190.1 on 1 and 287 DF,  p-value: &lt; 2.2e-16


$augSD

Call:
lm(formula = as.formula(paste("Y ~", .x)), data = sw_ectz)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.307683 -0.111051 -0.003922  0.086322  0.308041 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.12781    0.01231   10.38   &lt;2e-16 ***
augSD        0.08793    0.00720   12.21   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.133 on 287 degrees of freedom
Multiple R-squared:  0.3419,    Adjusted R-squared:  0.3396 
F-statistic: 149.1 on 1 and 287 DF,  p-value: &lt; 2.2e-16


$annMean

Call:
lm(formula = as.formula(paste("Y ~", .x)), data = sw_ectz)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.144251 -0.051607 -0.005023  0.045095  0.145173 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.053883   0.006309   8.541 7.94e-16 ***
annMean     0.332150   0.008667  38.325  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.0663 on 287 degrees of freedom
Multiple R-squared:  0.8365,    Adjusted R-squared:  0.836 
F-statistic:  1469 on 1 and 287 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The individual models show that, for each predictor, the estimate of the coefficients (for slope) and the test for the overall hypothesis are both significant (<span class="math inline">p &lt; 0.05</span> in all cases; refer to the model output). All the predictor variables are therefore good predictors of the structure of seaweed species composition along.</p>
<div class="cell page-columns page-full">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># Create individual plots for each predictor</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>plts1 <span class="ot">&lt;-</span> <span class="fu">map</span>(predictors, <span class="cf">function</span>(predictor) {</span>
<span id="cb9-3"><a href="#cb9-3"></a>  <span class="fu">ggplot</span>(sw_ectz, <span class="fu">aes_string</span>(<span class="at">x =</span> predictor, <span class="at">y =</span> <span class="st">"Y"</span>)) <span class="sc">+</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>    <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">1</span>, <span class="at">colour =</span> <span class="st">"dodgerblue4"</span>) <span class="sc">+</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>    <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">col =</span> <span class="st">"magenta"</span>, <span class="at">fill =</span> <span class="st">"pink"</span>) <span class="sc">+</span></span>
<span id="cb9-6"><a href="#cb9-6"></a>    <span class="fu">labs</span>(<span class="at">title =</span> <span class="fu">paste</span>(<span class="st">"Y vs"</span>, predictor),</span>
<span id="cb9-7"><a href="#cb9-7"></a>         <span class="at">x =</span> predictor,</span>
<span id="cb9-8"><a href="#cb9-8"></a>         <span class="at">y =</span> <span class="st">"Y"</span>) <span class="sc">+</span></span>
<span id="cb9-9"><a href="#cb9-9"></a>    <span class="fu">theme_bw</span>()</span>
<span id="cb9-10"><a href="#cb9-10"></a>})</span>
<span id="cb9-11"><a href="#cb9-11"></a></span>
<span id="cb9-12"><a href="#cb9-12"></a><span class="co"># Name the list elements for easy reference</span></span>
<span id="cb9-13"><a href="#cb9-13"></a><span class="fu">names</span>(plts1) <span class="ot">&lt;-</span> predictors</span>
<span id="cb9-14"><a href="#cb9-14"></a></span>
<span id="cb9-15"><a href="#cb9-15"></a>ggpubr<span class="sc">::</span><span class="fu">ggarrange</span>(<span class="at">plotlist =</span> plts1, <span class="at">ncol =</span> <span class="dv">2</span>,</span>
<span id="cb9-16"><a href="#cb9-16"></a>                  <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">labels =</span> <span class="st">"AUTO"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-slr1" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-slr1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="multiple_linear_regression_files/figure-html/fig-slr1-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-slr1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: Individual simple linear regressions fitted to the variables <code>augMean</code>, <code>febRange</code>, <code>febSD</code>, <code>augSD</code>, and <code>annMean</code> as predictors of the seaweed species composition as measured by the Sørensen dissimilarity, <code>Y</code>.
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-slr1" class="quarto-xref">Figure&nbsp;<span>5.1</span></a> is a series of scatter plots showing the relationship between the response variable <span class="math inline">\beta_\text{sør}</span> and each of the predictor variables. The blue line represents the linear regression fitted to the data. We see that the relationship between the response variable and each of the predictors is positive and linear. Each of the models are significant, as indicated by the <span class="math inline">p</span>-values in the model summaries. These simple models do not tell us how some predictors might act together to influence the response variable.</p>
<p>To consider combined effects and interactions between predictor variables, we must explore multiple linear regression models that include all the predictors. Multiple regression will give us a more integrated understanding of how various environmental variables jointly influence species composition along the coast. In doing so, we can control for confounding variables, improve model fit, deal with multicollinearity, test for interaction effects, and enhance predictive power.</p>
<p>We will fit this multiple regression model next.</p>
</section><section id="state-the-hypotheses-for-a-multiple-linear-regression" class="level3"><h3 class="anchored" data-anchor-id="state-the-hypotheses-for-a-multiple-linear-regression">State the Hypotheses for a Multiple Linear Regression</h3>
<p>As with all inferential statistics, we need to consider the hypotheses when performing multiple linear regression.</p>
<p>The null hypothesis (<span class="math inline">H_0</span>) states that there is no significant relationship between the Sørensen diversity index and any of the the climatological variables entered into the model, implying that the coefficients for all predictors are equal to zero. The alternative hypothesis (<span class="math inline">H_A</span>), on the other hand, states that there is a significant relationship between the Sørensen diversity index and the climatological variables, positing that at least one of the coefficients is not equal to zero.</p>
<p>The hypotheses can be divided into two kinds: those dealing with the main effects and the one assessing the overall model stated in <a href="#eq-full" class="quarto-xref">Equation&nbsp;<span>5.3</span></a>.</p>
<p><strong>Main effects hypotheses</strong></p>
<p>The main effects hypotheses test, for each predictor, <span class="math inline">X_i</span>, if the predictor has a significant effect on the response variable <span class="math inline">Y</span>.</p>
<p><span class="math inline">H_0</span>: There is no linear relationship between the environmental variables (<code>augMean</code>, <code>febRange</code>, <code>febSD</code>, <code>augSD</code>, and <code>annMean</code>) and the community composition as measured by <span class="math inline">\beta_\text{sør}</span> (in <code>Y</code>). Formally, for each predictor variable <span class="math inline">X_i</span>:</p>
<ul>
<li><span class="math inline">H_0: \beta_i = 0 \text{ for } i = 1, 2, 3, 4, 5</span></li>
</ul>
<p>Where <span class="math inline">\beta_i</span> are the coefficients of the predictors in the multiple linear regression model.</p>
<p><span class="math inline">H_A</span>: There is a linear relationship between the environmental variables (<code>augMean</code>, <code>febRange</code>, <code>febSD</code>, <code>augSD</code>, and <code>annMean</code>) and the species composition as measured by <span class="math inline">\beta_\text{sør}</span>:</p>
<ul>
<li><span class="math inline">H_A: \beta_i \neq 0 \text{ for } i = 1, 2, 3, 4, 5</span></li>
</ul>
<p><strong>Overall hypothesis</strong></p>
<p>In addition to testing the individual predictors, <span class="math inline">X_i</span>, we can also test a hypothesis about the overall significance of the model (<em>F</em>-test), which examines whether the model as a whole explains a significant amount of variance in the response variable <span class="math inline">Y</span>. A significant <em>F</em>-test would suggest that <em>at least one</em> predictor (excluding the intercept) in the model is likely to be significantly related to the response, but it requires further investigation of individual predictors and potential multicollinearity to fully understand the relationships. For the overall model hypothesis:</p>
<p>Null Hypothesis (<span class="math inline">H_0</span>):</p>
<ul>
<li><span class="math inline">H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = 0</span></li>
</ul>
<p>Alternative Hypothesis (<span class="math inline">H_A</span>):</p>
<ul>
<li><span class="math inline">H_A: \exists \, \beta_i \neq 0 \text{ for at least one } i</span></li>
</ul></section><section id="fit-the-model" class="level3"><h3 class="anchored" data-anchor-id="fit-the-model">Fit the Model</h3>
<p>We fit two models:</p>
<ul>
<li>a full model that includes an intercept term and the five environmental variables, and</li>
<li>a null model that includes only an intercept term.</li>
</ul>
<p>The reason the null model is included is to compare the full model with a model that has no predictors. This comparison will help us determine which of the predictors are useful in explaining the response variable—we will see this in action in the forward model selection process later on (<a href="#sec-forward-selection" class="quarto-xref"><span>Section 5.6.5</span></a>).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># Select only the variables that will be used in model building</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>sw_sub1 <span class="ot">&lt;-</span> sw_ectz[, <span class="fu">c</span>(<span class="st">"Y"</span>, <span class="st">"augMean"</span>, <span class="st">"febRange"</span>,</span>
<span id="cb10-3"><a href="#cb10-3"></a>                      <span class="st">"febSD"</span>, <span class="st">"augSD"</span>, <span class="st">"annMean"</span>)]</span>
<span id="cb10-4"><a href="#cb10-4"></a></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="co"># Fit the full and null models</span></span>
<span id="cb10-6"><a href="#cb10-6"></a>full_mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> augMean <span class="sc">+</span> febRange <span class="sc">+</span> febSD <span class="sc">+</span></span>
<span id="cb10-7"><a href="#cb10-7"></a>                 augSD <span class="sc">+</span> annMean, <span class="at">data =</span> sw_sub1)</span>
<span id="cb10-8"><a href="#cb10-8"></a>null_mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> sw_sub1)</span>
<span id="cb10-9"><a href="#cb10-9"></a></span>
<span id="cb10-10"><a href="#cb10-10"></a><span class="co"># Add fitted values from the full model to the dataframe</span></span>
<span id="cb10-11"><a href="#cb10-11"></a>sw_ectz<span class="sc">$</span>.fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(full_mod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="sec-multicollinearity" class="level3"><h3 class="anchored" data-anchor-id="sec-multicollinearity">Dealing With Multicollinearity</h3>
<p>Some of the predictor variables may be correlated with each other and this can lead to multicollinearity. When predictor variables are highly correlated, the model may not be able to distinguish the individual effects of each predictor. Consequently, the model becomes less precise and harder to interpret due to the coefficients’ inflated standard errors (<span class="citation" data-cites="graham2003confronting">Graham (<a href="references.html#ref-graham2003confronting" role="doc-biblioref">2003</a>)</span>). One can create a plot of pairwise correlations to visually inspect the correlation structure of the predictors. I’ll not do this here, but you can try it on your own.</p>
<p>A formal way to detect multicollinearity is to calculate the variance inflation factor (VIF) for each predictor variable. The VIF measures how much the variance of the estimated regression coefficients is increased due to multicollinearity. A VIF value greater than 5 or 10 indicates a problematic amount of multicollinearity.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a>initial_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="st">"Y ~ ."</span>)</span>
<span id="cb11-2"><a href="#cb11-2"></a></span>
<span id="cb11-3"><a href="#cb11-3"></a>threshold <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co"># Define a threshold for VIF values</span></span>
<span id="cb11-4"><a href="#cb11-4"></a></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="co"># Extract the names of the predictor variables</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>predictors <span class="ot">&lt;-</span> <span class="fu">names</span>(<span class="fu">vif</span>(full_mod1))</span>
<span id="cb11-7"><a href="#cb11-7"></a></span>
<span id="cb11-8"><a href="#cb11-8"></a><span class="co"># Iteratively remove collinear variables</span></span>
<span id="cb11-9"><a href="#cb11-9"></a><span class="cf">while</span> (<span class="cn">TRUE</span>) {</span>
<span id="cb11-10"><a href="#cb11-10"></a>  <span class="co"># Calculate VIF values</span></span>
<span id="cb11-11"><a href="#cb11-11"></a>  vif_values <span class="ot">&lt;-</span> <span class="fu">vif</span>(full_mod1)</span>
<span id="cb11-12"><a href="#cb11-12"></a>  <span class="fu">print</span>(vif_values) <span class="co"># Print VIF values for debugging</span></span>
<span id="cb11-13"><a href="#cb11-13"></a>  max_vif <span class="ot">&lt;-</span> <span class="fu">max</span>(vif_values)</span>
<span id="cb11-14"><a href="#cb11-14"></a>  </span>
<span id="cb11-15"><a href="#cb11-15"></a>  <span class="co"># Check if the maximum VIF is above the threshold</span></span>
<span id="cb11-16"><a href="#cb11-16"></a>  <span class="cf">if</span> (max_vif <span class="sc">&gt;</span> threshold) {</span>
<span id="cb11-17"><a href="#cb11-17"></a>    <span class="co"># Find the variable with the highest VIF</span></span>
<span id="cb11-18"><a href="#cb11-18"></a>    high_vif_var <span class="ot">&lt;-</span> <span class="fu">names</span>(<span class="fu">which.max</span>(vif_values))</span>
<span id="cb11-19"><a href="#cb11-19"></a>    <span class="fu">cat</span>(<span class="st">"Removing variable:"</span>,</span>
<span id="cb11-20"><a href="#cb11-20"></a>        high_vif_var,</span>
<span id="cb11-21"><a href="#cb11-21"></a>        <span class="st">"with VIF:"</span>,</span>
<span id="cb11-22"><a href="#cb11-22"></a>        max_vif,</span>
<span id="cb11-23"><a href="#cb11-23"></a>        <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb11-24"><a href="#cb11-24"></a>    </span>
<span id="cb11-25"><a href="#cb11-25"></a>    <span class="co"># Update the formula to exclude the high VIF variable</span></span>
<span id="cb11-26"><a href="#cb11-26"></a>    updated_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">"Y ~ . -"</span>, high_vif_var))</span>
<span id="cb11-27"><a href="#cb11-27"></a>    </span>
<span id="cb11-28"><a href="#cb11-28"></a>    <span class="co"># Refit the model without the high VIF variable</span></span>
<span id="cb11-29"><a href="#cb11-29"></a>    full_mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(updated_formula, <span class="at">data =</span> sw_sub1)</span>
<span id="cb11-30"><a href="#cb11-30"></a>    </span>
<span id="cb11-31"><a href="#cb11-31"></a>    <span class="co"># Update the environment data frame to reflect the removal</span></span>
<span id="cb11-32"><a href="#cb11-32"></a>    sw_sub1 <span class="ot">&lt;-</span> sw_sub1[, <span class="sc">!</span>(<span class="fu">names</span>(sw_sub1) <span class="sc">%in%</span> high_vif_var)]</span>
<span id="cb11-33"><a href="#cb11-33"></a>  } <span class="cf">else</span> {</span>
<span id="cb11-34"><a href="#cb11-34"></a>    <span class="cf">break</span></span>
<span id="cb11-35"><a href="#cb11-35"></a>  }</span>
<span id="cb11-36"><a href="#cb11-36"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  augMean  febRange     febSD     augSD   annMean 
27.947767 10.806635  8.765732  2.497739 31.061900 
Removing variable: annMean with VIF: 31.0619 
  augMean  febRange     febSD     augSD 
 2.290171 10.648752  8.637679  1.616390 
Removing variable: febRange with VIF: 10.64875 
 augMean    febSD    augSD 
1.423601 1.674397 1.585055 </code></pre>
</div>
</div>
<p>Regularisation techniques such as ridge regression, lasso regression, or elastic net can also be used to deal with multicollinearity. These advanced techniques add a penalty term to the regression model that shrinks the coefficients towards zero, which can help to reduce the impact of multicollinearity. However, these techniques are not covered in this guide. Please refer to <a href="regularisation.html" class="quarto-xref"><span>Chapter 8</span></a> for more information on regularisation techniques.</p>
</section><section id="sec-forward-selection" class="level3"><h3 class="anchored" data-anchor-id="sec-forward-selection">Perform Forward Selection</h3>
<p>It might be that not all of the variables included in the full model are necessary to explain the response variable. We can use a stepwise regression to select the best combination (subset) of predictors that best explains the response variable. To do this, we will use the <code>stepAIC</code> function that lives in the <code>MASS</code> package.</p>
<p><code>stepAIC()</code> works by starting with the null model and then adding predictors one by one, selecting the one that improves the model the most as seen in the reduction of the AIC values along the way. This process continues until no more predictors can be added to improve the model (i.e.&nbsp;to further reduce the AIC). Progress is tracked as the function runs.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># Perform forward selection</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">stepAIC</span>(null_mod1,</span>
<span id="cb13-3"><a href="#cb13-3"></a>                <span class="at">scope =</span> <span class="fu">list</span>(<span class="at">lower =</span> null_mod1, <span class="at">upper =</span> full_mod1),</span>
<span id="cb13-4"><a href="#cb13-4"></a>                <span class="at">direction =</span> <span class="st">"forward"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Start:  AIC=-1044.97
Y ~ 1

          Df Sum of Sq    RSS     AIC
+ augMean  1    6.0084 1.7108 -1478.4
+ febSD    1    3.0759 4.6433 -1189.9
+ augSD    1    2.6394 5.0797 -1163.9
&lt;none&gt;                 7.7192 -1045.0

Step:  AIC=-1478.41
Y ~ augMean

        Df Sum of Sq    RSS     AIC
+ febSD  1   0.36036 1.3504 -1544.8
+ augSD  1   0.31243 1.3984 -1534.7
&lt;none&gt;               1.7108 -1478.4

Step:  AIC=-1544.77
Y ~ augMean + febSD

        Df Sum of Sq    RSS     AIC
+ augSD  1   0.10568 1.2448 -1566.3
&lt;none&gt;               1.3504 -1544.8

Step:  AIC=-1566.32
Y ~ augMean + febSD + augSD</code></pre>
</div>
</div>
<p>The model selection process shows that as we add more variables to the model, the AIC value decreases. We can infer from this that the multiple regression model provides a better fit that simple linear models that use the variables in isolation.</p>
<p>We also see that <code>stepAIC()</code> has not removed any variables from the full model. Probably one reason for failing to remove any variables is that the VIF process has already accomplished this by virtue of dealing with multicollinearity. This means that all the variables retained in <code>mod1</code> are important in explaining the response variable.</p>
</section><section id="sec-added-variable-plots" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-added-variable-plots">Added-Variable Plots (Partial Regression Plots)</h3>
<p>Before looking at the output in more detail, I’ll introduce partial regression plots as a means to examine the relationship between the response variable and each predictor variable. Although they can be calculated by hand, the <strong>car</strong> package provides a convenient function, <code>avPlots()</code>, to create these plots.</p>
<p>Added variable plots are also sometimes called ‘partial regression plots’ or ‘individual coefficient plots.’ They are used to display the relationship between a response variable and an individual predictor variable while accounting for the effect of other predictor variables in a multiple regression model (the marginal effect).</p>
<div class="cell page-columns page-full">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># Create partial regression plots</span></span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="fu">avPlots</span>(mod1, <span class="at">col =</span> <span class="st">"dodgerblue4"</span>, <span class="at">col.lines =</span> <span class="st">"magenta"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-mod1-partial" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-mod1-partial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="multiple_linear_regression_files/figure-html/fig-mod1-partial-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-mod1-partial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.2: Partial regression plots for <code>mod1</code> with the selected variables <code>augMean</code>, <code>febSD</code>, and <code>augSD</code>.
</figcaption></figure>
</div>
</div>
</div>
<p>What insights can we draw from the added-variable plots? Although there are better ways to assess the model fit, we can already make some observations about the linearity of the model or the presence of outliers. The slope of the line in an added variable plot corresponds to the regression coefficient for that predictor in the full multiple regression model. Seen in this way, it visually indicates the magnitude and direction of each predictor’s effect. In <a href="#fig-mod1-partial" class="quarto-xref">Figure&nbsp;<span>5.2</span></a>, the added-variable plot for <code>augMean</code> shows a tighter clustering of points around the regression line and a strong linear relationship (steep slope) with the response variable; the plots for <code>febSD</code> and <code>augSD</code>, on the other hand, show a weaker response and more scatter about the regression line. Importantly, this suggests that <code>augMean</code> has a stronger and more unique contribution to the multiple-variable model than the other two variables.</p>
<p>There are also insights to be made about possible multicollinearity using added-variable plots. These plots are not a definitive test for multicollinearity, but they can provide some clues. Notably, if a predictor shows a strong relationship with the response variable in a simple correlation but appears to have little relationship in the added-variable plot, it might indicate collinearity with other predictors. This discrepancy suggests that the predictor’s effect on the response is being masked by the presence of other correlated predictors.</p>
</section><section id="sec-diagnostics" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-diagnostics">Model Diagnostics</h3>
<p>We are back in the territory of parametric statistics, so we need to check the assumptions of the multiple linear regression model (similar to those of simple linear regression). We can do this by making the various diagnostic plots. all of them consider various aspects of the residuals, which are simply the differences between the observed and predicted values.</p>
<p><strong>Diagnostic plots of final model</strong></p>
<p>You have been introduced to diagnostic plots in the context of simple linear regression (<a href="simple_linear_regression.html#sec-simple-linear-regression" class="quarto-xref"><span>Section 3.1</span></a>). They are also useful in multiple linear regression. Although <code>plot.lm()</code> can easily do this, here I use <code>autoplot()</code> from the <strong>ggfortify</strong> package. When applied to the final model, <code>mod1</code>, the plot will in its default setting show four diagnostic plots: residuals vs.&nbsp;fitted values, normal Q-Q plot, scale-location plot, and residuals vs.&nbsp;leverage plot. Note, this is for the full model inclusive of the combined contributions of all the predictors, so we will not see separate plots for each predictor as we have seen in the added-variable plots or component plus residual plots.</p>
<div class="cell page-columns page-full">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># Generate diagnostic plots </span></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="fu">autoplot</span>(mod1, <span class="at">shape =</span> <span class="dv">21</span>, <span class="at">colour =</span> <span class="st">"dodgerblue4"</span>,</span>
<span id="cb16-3"><a href="#cb16-3"></a>         <span class="at">smooth.colour =</span> <span class="st">"magenta"</span>) <span class="sc">+</span></span>
<span id="cb16-4"><a href="#cb16-4"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-mlr3" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-mlr3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="multiple_linear_regression_files/figure-html/fig-mlr3-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-mlr3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.3: Diagnostic plots to assess the fit of the final multiple linear regression model, <code>mod1</code>.
</figcaption></figure>
</div>
</div>
</div>
<p><strong>Residuals vs.&nbsp;Fitted Values:</strong> In this plot we can assess linearity and homoscedasticity of the residuals. If the seaweed gods were with us, we’d expect the points to be randomly scattered about a horizontal line situation at zero. This would indicate that the relationship between the predictors selected by the forward selection process (<code>augMean</code>, <code>febSD</code>, and <code>augSD</code>) and the response variable (<code>Y</code>) is linear, and the variance of the residuals is constant across the range of fitted values. In this plot, there’s a very slight curvature which might suggest a potential issue with the linearity assumption—it is minute and I’d suggest not worrying about it. The variance of the residuals seems to decrease slightly at higher fitted values, indicating a mild case of heteroscedasticity.</p>
<p><strong>Q-Q Plot (Quantile-Quantile Plot):</strong> This plot is used to check the normality of the residuals. The points should fall approximately along a straight diagonal line if the residuals are normally distributed. Here we see that the points generally follow the line although some deviations may be seen at the tails. These deviations are not that extreme and again I don’t think this is not a big concern.</p>
<p><strong>Scale-Location Plot:</strong> This plot should reveal potential issues with homoscedasticity. The square root of the standardised residuals is used here to make it easier to spot patterns, so we would like the points to be randomly scattered around the horizontal red line. Here, the line slopes slightly downward and this indicates that the variance of the residuals might decrease as the fitted values increase. We can also see evidence of this in a plot of the observed values vs.&nbsp;the predictors in <a href="#fig-mlr3" class="quarto-xref">Figure&nbsp;<span>5.3</span></a>.</p>
<p><strong>Residuals vs.&nbsp;Leverage:</strong> This diagnostic highlights influential points (outliers). Points with high leverage (far from the mean of the predictors) can be expected to exert a strong influence on the regression line, tilting it in some direction. Cook’s distance (indicated by the yellow line) helps identify such outliers. In our seaweed data a few points could have a high leverage, but since they don’t seem to cross the Cook’s distance thresholds, I doubt they are overly worrisome.</p>
<p>Considering that no glaring red flags were raised by the diagnostic plots, I doubt that they are severe enough to invalidate the model. However, if you cannot stand these small issues, you could i) consider transforming the predictor or response variables to address your concerns about heteroscedasticity, ii) investigate the outliers (high leverage points) to confirm if they are valid data points or errors, or iii) try robust regression methods that are less sensitive to outliers and heteroscedasticity.</p>
<p><strong>Component plus residual plots</strong></p>
<p>Component plus residual plots offer another way to assess the fit of the model in multiple regression models. Unlike simple linear regression where we only had one predictor variable, here we have several. So, we need to assure ourselves that there is a linear relationship between each predictor variable and the response variable (we could already see this in the added-variable plots in <a href="#sec-added-variable-plots" class="quarto-xref"><span>Section 5.6.6</span></a>). We can make component plus residual plots using the <code>crPlots()</code> function in the <strong>car</strong> package. It displays the relationship between the response variable and each predictor variable. If the relationship is linear, the points should be randomly scattered about a best fit line and the spline (in pink in <a href="#fig-mod1-crplots" class="quarto-xref">Figure&nbsp;<span>5.4</span></a>) should plot nearly on top of the linear regression line.</p>
<div class="cell page-columns page-full">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># Generate component plus residual plots</span></span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="fu">crPlots</span>(mod1, <span class="at">col =</span> <span class="st">"dodgerblue4"</span>, <span class="at">col.lines =</span> <span class="st">"magenta"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-mod1-crplots" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-mod1-crplots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="multiple_linear_regression_files/figure-html/fig-mod1-crplots-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-mod1-crplots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.4: Component plus residual diagnostic plots to assess the fit of the final multiple linear regression model, <code>mod1</code>.
</figcaption></figure>
</div>
</div>
</div>
</section><section id="sec-mod1-summary" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-mod1-summary">Understanding the Model Fit</h3>
<p>The above model selection process has led us to the <code>mod1</code> model, which can be stated formally as: <span id="eq-mod1"><span class="math display">Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \epsilon \tag{5.4}</span></span> Where:</p>
<ul>
<li>
<span class="math inline">Y</span>: The response variable, the mean Sørensen dissimilarity.</li>
<li>
<span class="math inline">X_1</span>, <span class="math inline">X_2</span>, and <span class="math inline">X_3</span>: The predictors corresponding to <code>augMean</code>, <code>febSD</code>, and <code>augSD</code>, respectively.</li>
<li>
<span class="math inline">\epsilon</span>: The error term.</li>
</ul>
<p>We have convinced ourselves that the model is a good fit for the data, and we can proceed to examine the model’s output. The fitted model can be explored in two ways: by applying the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function or by using the <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> function. The <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function provides a detailed output of the model, while the <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> function provides a table of deviance values that can be used to compare models.</p>
<p><strong>The model summary</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># Summary of the selected model</span></span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="fu">summary</span>(mod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Y ~ augMean + febSD + augSD, data = sw_sub1)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.153994 -0.049229 -0.006086  0.045947  0.148579 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.028365   0.007020   4.040 6.87e-05 ***
augMean     0.283335   0.011131  25.455  &lt; 2e-16 ***
febSD       0.049639   0.008370   5.930 8.73e-09 ***
augSD       0.022150   0.004503   4.919 1.47e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.06609 on 285 degrees of freedom
Multiple R-squared:  0.8387,    Adjusted R-squared:  0.837 
F-statistic: 494.1 on 3 and 285 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The first part of the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function’s output is the <code>Coefficients</code> section. This is where the main effects hypotheses are tested (this model does not have interactions—if there were, they’d appear here, too). The important components of the coefficients part of the model summary are:</p>
<ul>
<li>
<code>(Intercept)</code>: This row provides information about where the regression line intersects the <em>y</em>-axis.</li>
<li>Main Effects:
<ul>
<li>
<code>augMean</code>, <code>febSD</code>, and <code>augSD</code>: These rows give the model coefficients associated with the slopes of the regression lines fit to those predictor variables. They indicate the rate of change in the response variable for a one-unit change in the predictor variable.</li>
<li>
<code>Estimate</code>, <code>Std. Error</code>, <code>t value</code>, and <code>Pr(&gt;|t|)</code>: These columns contain the statistics used to interpret the hypotheses about the main effects. In the <code>Estimate</code> column are the coefficients for the <em>y</em>-intercept and the main effects’ slopes, and <code>Std. Error</code> indicates the variability of the estimate. The <code>t value</code> is obtained by dividing the coefficient by its standard error. The <em>p</em>-value tests the null hypothesis that the coefficient is equal to zero and significance codes are provided as a quick visual reference (their use is sometimes frowned upon by statistics purists). Using this information, we can quickly see that, for example, <code>augMean</code> has a coefficient of <span class="math inline">0.2833 \pm 0.0111</span> and the slope of the line is highly significant, i.e.&nbsp;there is a significant effect of <code>Y</code> due to the temperature gradient set up by <code>augMean</code>.</li>
</ul>
</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The intercept and slope coefficients
</div>
</div>
<div class="callout-body-container callout-body">
<p>The interpretation of the coefficients is a bit more complicated in multiple linear regression compared to what we are accustomed to in simple linear regression. Let us look at some greater detail at the intercept and the slope coefficients:</p>
<p>Intercept (<span class="math inline">\alpha</span>): ) The intercept is the expected value of the response variable, <span class="math inline">Y</span>, when all predictor variables are zero. It is not always meaningful, but it can be useful in some cases.</p>
<p>Slope Coefficients (<span class="math inline">\beta_1, \beta_2, \ldots, \beta_k</span>): Each slope coefficient, <span class="math inline">\beta_j</span>, represents the expected change in the response variable, <span class="math inline">Y</span>, for a one-unit increase in the predictor variable, <span class="math inline">X_j</span>, holding all other predictor variables constant. This partial effect interpretation implies that <span class="math inline">\beta_j</span> accounts for the direct contribution of <span class="math inline">X_j</span> to <span class="math inline">Y</span> while removing the confounding effects of other predictors in the model. <a href="#fig-mod1-partial" class="quarto-xref">Figure&nbsp;<span>5.2</span></a> provides a visual representation of this concept and isolates the effect of each predictor variable on the response variable.</p>
<p>Therefore, in the context of our model (<a href="#eq-mod1" class="quarto-xref">Equation&nbsp;<span>5.4</span></a>) for this analysis, the partial interpretation is as follows:</p>
<ul>
<li>
<span class="math inline">\beta_1</span>: Represents the change in <span class="math inline">Y</span> for a one-unit increase in <span class="math inline">X_1</span>, holding <span class="math inline">X_2</span> and <span class="math inline">X_3</span> constant.</li>
<li>
<span class="math inline">\beta_2</span>: Represents the change in <span class="math inline">Y</span> for a one-unit increase in <span class="math inline">X_2</span>, holding <span class="math inline">X_1</span> and <span class="math inline">X_3</span> constant.</li>
<li>
<span class="math inline">\beta_3</span>: Represents the change in <span class="math inline">Y</span> for a one-unit increase in <span class="math inline">X_3</span>, holding <span class="math inline">X_1</span> and <span class="math inline">X_2</span> constant.</li>
</ul>
</div>
</div>
<p>There are also several overall model fit statistics—it is here where you’ll find the information you need to assess the hypothesis about the overall significance of the model. <code>Residual standard error</code> indicates the average distance between observed and fitted values. <code>Multiple R-squared</code> and <code>Adjusted R-squared</code> values tell us something about the model’s goodness of fit. The latter adjusts for the number of predictors in the model, and is the one you must use and report in multiple linear regressions. As you also know, higher numbers approaching 1 are better, with 1 suggesting that the model perfectly captures all of the variability in the data. The <code>F-statistic</code> and its associated <em>p</em>-value test the overall significance of the model and examines whether all regression coefficients are simultaneously equal to zero. You can also use the brief overview of the residuals, but I don’t find this particularly helpful—best examine the residuals in a histogram.</p>
<p><strong>The ANOVA tables</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a><span class="fu">anova</span>(mod1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Response: Y
           Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
augMean     1 6.0084  6.0084 1375.660 &lt; 2.2e-16 ***
febSD       1 0.3604  0.3604   82.507 &lt; 2.2e-16 ***
augSD       1 0.1057  0.1057   24.196 1.473e-06 ***
Residuals 285 1.2448  0.0044                       
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>This function provides a sequential analysis of variance (Type I ANOVA) table for the regression model (see more about Type I ANOVA, below). As such, this function can also be used to compare nested models. Used on a single model, it gives a more interpretable breakdown of the variability in the response variable <code>Y</code> and assesses the contribution of each predictor variable in explaining this variability.</p>
<p>The ANOVA table firstly shows the degrees of freedom (<code>Df</code>) for each predictor variable added sequentially to the model, as well as the residuals. For each predictor, the degrees of freedom is typically 1. For the residuals, however, it represents the total number of observations minus the number of estimated parameters. The Sum of Squares (<code>Sum Sq</code>) indicates the variability in <code>Y</code> attributable to each predictor, and the mean sum of squares (<code>Mean Sq</code>) is the sum of squares divided by the degrees of freedom.</p>
<p>The <code>F value</code> is calculated as the ratio of the predictor’s mean square to the residual mean square tests. It is used in testing the null hypothesis that the predictor has no effect on <code>Y</code>. Whether or not we accept the alternative hypothesis (reject the null) is given by the <em>p</em>-value (<code>Pr(&gt;F)</code>) that goes with each <em>F</em>-statistic. You know how that works.</p>
<p>Because this is a sequential ANOVA, the amount of variance in <code>Y</code> explained by each predictor (or group of predictors) is calculated by adding the predictors to the model in sequence (as specified in the model formula). For example, the Sum of Squares for <code>augMean</code> (6.0084) represents the amount of variance explained by adding <code>augMean</code> to a model that doesn’t include any predictors yet. The Sum of Squares for <code>febSD</code> 0.3604) represents the amount of variance explained by adding <code>febSD</code> to a model that already includes <code>augMean</code>—this improvement indicates that <code>febSD</code> explains some of the variance in <code>Y</code> that <code>augMean</code> doesn’t.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Order in which predictors are assessed in multiple linear regression
</div>
</div>
<div class="callout-body-container callout-body">
<p>The interpretation of sequential ANOVA (Type I) is inherently dependent on the order in which predictors are entered. In <code>mod1</code> the order is first <code>augMean</code>, then <code>febSD</code>, and last comes <code>augSD</code>. This order might not be the most meaningful for interpreting the sequential sums of squares and their significance in the ANOVA table. How, then, does one decide on the order of predictors in the model?</p>
<ul>
<li>If you have a strong theoretical or causal basis for thinking that certain predictors influence others, you can enter them in that order.</li>
<li>If you have a hierarchy of predictors based on their importance or general vs.&nbsp;specific nature, you can enter them hierarchically.</li>
<li>You can manually fit models with different predictor orders and compare the ANOVA tables to see how the results change. This can be time-consuming but might offer insights into the sensitivity of your conclusions to the order of entry.</li>
<li>You can use automated model selection procedures, such as stepwise regression, to determine the best order of predictors. This is a more objective approach but can be criticised for being data-driven and not theory-driven.</li>
<li>Use Type II or Type III ANOVAs, which are are not order-dependent and can be used to assess the significance of predictors after accounting for all other predictors in the model. However, they have their own limitations and assumptions that need to be considered.</li>
</ul>
<p>My advice would be to have sound theoretical reasons for the order of predictors in the model.</p>
</div>
</div>
<p>Both ways of looking at the model fit of <code>mod1</code>—<code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> and <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code>—show that forward selection retained the variables <code>augMean</code>, <code>febSD</code>, and <code>augSD</code>. These three predictors should be used together to explain the response, <code>Y</code>.</p>
<p>Let’s make a plot of the full model with all the initial predictors and the selected model with the predictors chosen by the forward selection process.</p>
<div class="cell page-columns page-full">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># Add fitted values from the selected model to the dataframe</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>sw_ectz<span class="sc">$</span>.fitted_selected <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod1)</span>
<span id="cb22-3"><a href="#cb22-3"></a></span>
<span id="cb22-4"><a href="#cb22-4"></a><span class="co"># Create the plot of observed vs fitted values for the selected model</span></span>
<span id="cb22-5"><a href="#cb22-5"></a><span class="fu">ggplot</span>(sw_ectz, <span class="fu">aes</span>(<span class="at">x =</span> .fitted_selected, <span class="at">y =</span> Y)) <span class="sc">+</span></span>
<span id="cb22-6"><a href="#cb22-6"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">1</span>, <span class="at">colour =</span> <span class="st">"black"</span>, <span class="at">alpha =</span> <span class="fl">1.0</span>) <span class="sc">+</span></span>
<span id="cb22-7"><a href="#cb22-7"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> .fitted), <span class="at">colour =</span> <span class="st">"red"</span>,</span>
<span id="cb22-8"><a href="#cb22-8"></a>             <span class="at">shape =</span> <span class="dv">1</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb22-9"><a href="#cb22-9"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>,</span>
<span id="cb22-10"><a href="#cb22-10"></a>              <span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb22-11"><a href="#cb22-11"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Fitted Values"</span>,</span>
<span id="cb22-12"><a href="#cb22-12"></a>       <span class="at">y =</span> <span class="st">"Observed Values"</span>) <span class="sc">+</span></span>
<span id="cb22-13"><a href="#cb22-13"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-mlr2" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-mlr2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="multiple_linear_regression_files/figure-html/fig-mlr2-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-mlr2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.5: Plot of observed vs.&nbsp;predicted value obtained from the final multiple linear regression model (<code>mod</code>) with the selected variables <code>augMean</code>, <code>febSD</code>, and <code>augSD</code> as predictors (black points), and the initial model with also <code>annMean</code> and <code>febRange</code> (red points).
</figcaption></figure>
</div>
</div>
</div>
</section><section id="reporting" class="level3"><h3 class="anchored" data-anchor-id="reporting">Reporting</h3>
<p>A Results section should be written in a format sutable for inclusion in your report or publication. Present the results in a clear and concise manner, with tables and figures used to help substantiate your findings. The results should be interpreted in the context of the research question and the study design. The limitations of the analysis should also be discussed, along with any potential sources of bias or confounding. Here is an example.</p>
<p><strong>Results</strong></p>
<p>The model demonstrates a strong overall fit, as indicated by the high <span class="math inline">R^2</span> value of 0.839 and an adjusted <span class="math inline">R^2</span> of 0.837, suggesting that approximately 83.7% of the variance in the mean Sørensen dissimilarity is explained by the predictors <code>augMean</code>, <code>febSD</code>, and <code>augSD</code>. All predictors in the model are statistically significant, with <code>augMean</code> showing the strongest effect (<span class="math inline">\beta_1 = 0.283</span>, <span class="math inline">p &lt; 0.0001</span>) (<a href="#fig-mod1-partial" class="quarto-xref">Figure&nbsp;<span>5.2</span></a>). The predictors <code>febSD</code> and <code>augSD</code> also have significant positive relationships with the response variable (<span class="math inline">\beta_2 = 0.050</span>, <span class="math inline">p = 0.0001</span>; <span class="math inline">\beta_3 = 0.022</span>, <span class="math inline">p = 0.0001</span>). A sequential ANOVA further confirms the significance of each predictor variable in the model, with all <em>F</em>-values indicating that the inclusion of each predictor significantly improves the model fit (<span class="math inline">p &lt; 0.0001</span> in all cases). Our model therefore provides clear support for the mean temperatures in August, the standard deviation of temperatures in February, and the standard deviation of temperatures in August as strong predictors of the mean Sørensen dissimilarity, with each contributing uniquely to the explanation of variability in the response variable.</p>
</section></section><section id="sec-mlr-interaction" class="level2 page-columns page-full" data-number="5.7"><h2 data-number="5.7" class="anchored" data-anchor-id="sec-mlr-interaction">
<span class="header-section-number">5.7</span> Example 2: Interaction of Distance and Bioregion</h2>
<p>Our seaweed dataset includes two additional variables that we have not yet considered. These are the continuous variable <code>dist</code> which represents the geographic distance between the seaweed samples taken along the coast of South Africa, and the categorical variable <code>bio</code> which is the bioregional classification of the seaweed samples.</p>
<p>These two new variables lend themselves to a few interesting questions. For example:</p>
<ol type="1">
<li>Is the geographic distance between samples related to the Sørensens dissimilarity of the seaweed flora?</li>
<li>Does the average Sørensens dissimilarity vary among the bioregions to which the samples belong?</li>
<li>Is the effect of geographic distance on the Sørensens dissimilarity different for each bioregion?</li>
</ol>
<p>The most complex model is (3), the one that answers the question about whether the effect of <code>dist</code> on the response variable <span class="math inline">Y</span> is different for each bioregion. Questions (1) and (2) are subsets of this more inclusive question. To fully answer these quesitons, let’s first consider the full model, which includes an <em>interaction term</em> between the continuous predictor <code>dist</code> and the categorical predictor <code>bio</code>. When we finally test our model, we will also have to consider the simpler models that do not include the interaction term.</p>
<p>‘Interaction’ means that the effect of one predictor on the response variable is contingent on the value of another predictor. For example, we might have reason to suspect that the relationship of the Sørensens dissimilarity with the geographic distance between samples is different between the west coast compared to, say, the east coast. This is indeed a plausible expectation, but we will test this formally below.</p>
<p>The full multiple linear regression model with the interaction terms can be formally expressed as Equation :</p>
<span class="math display">\begin{align}
Y &amp;= \alpha + \beta_1 \text{dist} + \beta_2 \text{bio}_{\text{B-ATZ}} + \beta_3 \text{bio}_{\text{BMP}} \nonumber \\
  &amp;\quad + \beta_4 \text{bio}_{\text{ECTZ}} + \beta_5 (\text{dist} \times \text{bio}_{\text{B-ATZ}}) \nonumber \\
  &amp;\quad + \beta_6 (\text{dist} \times \text{bio}_{\text{BMP}}) + \beta_7 (\text{dist} \times \text{bio}_{\text{ECTZ}}) + \epsilon \label{mod2}
\end{align}</span>
<p>Where:</p>
<ul>
<li>
<span class="math inline">Y</span>: The response variable, the mean Sørensen dissimilarity.</li>
<li>
<span class="math inline">\alpha</span>: The intercept term.</li>
<li>
<span class="math inline">\text{dist}</span>: The continuous predictor variable representing distance.</li>
<li>
<span class="math inline">\text{bio}</span>: The categorical predictor variable representing bioregional classification with four levels: <code>AMP</code> (reference category), <code>B-ATZ</code>, <code>BMP</code>, and <code>ECTZ</code>.</li>
<li>
<span class="math inline">\text{bio}_\text{B-ATZ}, \text{bio}_\text{BMP}, \text{bio}_\text{ECTZ}</span>: Dummy variables for the bioregional classification, where:
<ul>
<li>
<span class="math inline">\text{bio}_\text{B-ATZ} = 1</span> if <code>bio</code> = <code>B-ATZ</code>, and 0 otherwise,</li>
<li>
<span class="math inline">\text{bio}_\text{BMP} = 1</span> if <code>bio</code> = <code>BMP</code>, and 0 otherwise, and</li>
<li>
<span class="math inline">\text{bio}_\text{ECTZ} = 1</span> if <code>bio</code> = <code>ECTZ</code>, and 0 otherwise.</li>
</ul>
</li>
<li>
<span class="math inline">\text{dist} \times \text{bio}_\text{B-ATZ}, \text{dist} \times \text{bio}_\text{BMP}, \text{dist} \times \text{bio}_\text{ECTZ}</span>: Interaction terms between distance and the bioregional classification dummy variables.</li>
<li>
<span class="math inline">\beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6, \beta_7</span>: The coefficients to be estimated for the main effects and interactions.</li>
<li>
<span class="math inline">\epsilon</span>: The error term.</li>
</ul>
<p>If this seems tricky, it is because of the dummy variable coding used to represent interactions in multiple linear regression. The <code>bio</code> variable is a categorical variable with four levels, so we need to create three dummy variables to represent the bioregional classification. The <code>dist</code> variable is then interacted with each of these dummy variables to create the interaction terms. The <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function in R takes care of this for us in a far less complicated model statement. I’ll explain the details around the interpretation of dummy variable coding when we look at the output of the model with the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function.</p>
<section id="state-the-hypotheses-for-a-multiple-linear-regression-with-interaction-terms" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="state-the-hypotheses-for-a-multiple-linear-regression-with-interaction-terms">State the Hypotheses for a Multiple Linear Regression with Interaction Terms</h3>
<p>Equation expands into the following series of hypotheses that concern the main effects, the interactions between the main effects, and the overall hypothesis:</p>
<p><strong>Main effects hypotheses</strong></p>
<p>In the main effects hypotheses we are concerned with the effect of each predictor variable on the response variable. For the main effect of distance we have the null:</p>
<ul>
<li><span class="math inline">H_0: \beta_1 = 0</span></li>
</ul>
<p>vs.&nbsp;the alternative:</p>
<ul>
<li><span class="math inline">H_A: \beta_1 \neq 0</span></li>
</ul>
<p>For the main effect of bioregional classification, the nulls are:</p>
<ul>
<li><span class="math inline">H_0: \beta_2 = 0 \quad (\text{bio}_{\text{B-ATZ}})</span></li>
<li><span class="math inline">H_0: \beta_3 = 0 \quad (\text{bio}_{\text{BMP}})</span></li>
<li><span class="math inline">H_0: \beta_4 = 0 \quad (\text{bio}_{\text{ECTZ}})</span></li>
</ul>
<p>vs.&nbsp;the alternatives:</p>
<ul>
<li><span class="math inline">H_A: \beta_2 \neq 0 \quad (\text{bio}_{\text{B-ATZ}})</span></li>
<li><span class="math inline">H_A: \beta_3 \neq 0 \quad (\text{bio}_{\text{BMP}})</span></li>
<li><span class="math inline">H_A: \beta_4 \neq 0 \quad (\text{bio}_{\text{ECTZ}})</span></li>
</ul>
<p><strong>Hypotheses about interactions</strong></p>
<p>This is where the hypothesis tests whether the effect of distance on the response variable is different for each bioregional classification. The null hypotheses are:</p>
<ul>
<li><span class="math inline">H_0: \beta_5 = 0 \quad (\text{dist} \times \text{bio}_{\text{B-ATZ}})</span></li>
<li><span class="math inline">H_0: \beta_6 = 0 \quad (\text{dist} \times \text{bio}_{\text{BMP}})</span></li>
<li><span class="math inline">H_0: \beta_7 = 0 \quad (\text{dist} \times \text{bio}_{\text{ECTZ}})</span></li>
</ul>
<p>vs.&nbsp;the alternatives:</p>
<ul>
<li><span class="math inline">H_A: \beta_5 \neq 0 \quad (\text{dist} \times \text{bio}_{\text{B-ATZ}})</span></li>
<li><span class="math inline">H_A: \beta_6 \neq 0 \quad (\text{dist} \times \text{bio}_{\text{BMP}})</span></li>
<li><span class="math inline">H_A: \beta_7 \neq 0 \quad (\text{dist} \times \text{bio}_{\text{ECTZ}})</span></li>
</ul>
<p><strong>Overall hypothesis</strong></p>
<p>The overall hypothesis states that all coefficients associated with the predictors (distance, bioregional categories, and their interactions) are equal to zero, therefore indicating no relationship between these predictors and the response variable, the Sørensen index. The null hypothesis is:</p>
<ul>
<li><span class="math inline">H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = \beta_7 = 0</span></li>
</ul>
<p>vs.&nbsp;the alternative:</p>
<ul>
<li><span class="math inline">H_A: \exists \, \beta_i \neq 0 \text{ for at least one } i</span></li>
</ul>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-mod2_main_effects" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-mod2_main_effects-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="multiple_linear_regression_files/figure-html/fig-mod2_main_effects-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-mod2_main_effects-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.6: Plot of main effects of A) distance along the coast and B) bioregional classification on the Sørensen dissimilarity index.
</figcaption></figure>
</div>
</div>
</div>
</section><section id="visualise-the-main-effects" class="level3"><h3 class="anchored" data-anchor-id="visualise-the-main-effects">Visualise the Main Effects</h3>
<p>To facilitate the interpretation of the main effects hypotheses and make an argument for why an interaction term might be necessary, I’ve visualised the main effects (<a href="#fig-mod2_main_effects" class="quarto-xref">Figure&nbsp;<span>5.6</span></a>). I see this as part of my exploratory data analysis ensemble of tests. We see that fitting a straight line to the <code>Y</code> vs.&nbsp;distance relationship seems unsatisfactory as there is too much scatter around that single line to adequately capture all the structure in the variability of the points. Colouring the points by bioregion reveals the hidden structure. The model could benefit from including an additional level of complexity: see how points in the same bioregion show less scatter compared to points in different bioregions.</p>
<p>Now look at the boxplots of the Sørensen dissimilarity index for each bioregional classification. It shows that the median values of the Sørensen dissimilarity index are different for each bioregion. Taken together, <a href="#fig-mod2_main_effects" class="quarto-xref">Figure&nbsp;<span>5.6</span></a> (A, B) provide a good indication that adding the bioregional classification might be an important predictor of the Sørensen dissimilarity index as a function of distance between pairs of sites along the coast.</p>
<p>Next, we will move ahead and fit the model inclusive of the distance along the coast and bioregion as per Equation .</p>
</section><section id="fit-and-assess-nested-models" class="level3"><h3 class="anchored" data-anchor-id="fit-and-assess-nested-models">Fit and Assess Nested Models</h3>
<p>I have a suspicion that the full model (<code>mod2</code>; see below) with the interaction terms will be a better fit than reduced models with only the effect due to distance (seen independently). How can we have greater certainty that we should indeed favour a slightly more complex model (with two predictors) over a simpler one with only (distance only)?</p>
<p>One way to do this is to use a nested model comparison. We will fit a reduced model (one slope for all bioregions) and compare this model to the full model (slopes are allowed to vary among bioregions).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="co"># Fit the linear regression model with only distance</span></span>
<span id="cb23-2"><a href="#cb23-2"></a>mod2a <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> dist, <span class="at">data =</span> sw)</span>
<span id="cb23-3"><a href="#cb23-3"></a></span>
<span id="cb23-4"><a href="#cb23-4"></a><span class="co"># Fit the multiple linear regression model with interaction terms</span></span>
<span id="cb23-5"><a href="#cb23-5"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> dist <span class="sc">*</span> bio, <span class="at">data =</span> sw)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This is a nested model where <code>mod2a</code> is nested within <code>mod2</code>. ‘Nested’ means that the reduced model is a subset of the full model. Nested models can be used to test hypotheses about the significance of the predictors in the full model—does adding more predictors to the model improve the fit? Comparing a nested model with a full model can be done with a sequential ANOVA, which is what the <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> function also does (in addition to its use in <a href="#sec-mod1-summary" class="quarto-xref"><span>Section 5.6.8</span></a>).</p>
<p>So, comparing <code>mod2a</code> to <code>mod2</code> with an <em>F</em>-test tests the significance of adding the <code>bio</code> and using it together with <code>dist</code>. The interaction is built into <code>mod2</code> but we are not yet testing the significance of the interaction terms. We will do that later.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="fu">anova</span>(mod2a, mod2, <span class="at">test =</span> <span class="st">"F"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: Y ~ dist
Model 2: Y ~ dist * bio
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1    968 7.7388                                  
2    962 2.2507  6    5.4881 390.95 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>The sequential ANOVA shows that there is significant merit to consider an interaction term in the model. This model would then allow us to have a separate slope for the Sørensen index as function of distance for each bioregion. The residual sum of squares (<code>RSS</code>) decreases from 7.7388 in Model 1 to 2.2507 in Model 2, which indicates that Model 2 explains a significantly larger proportion of the variance in the response variable. The <em>F</em>-test for comparing the two models yields an <em>F</em>-value of 390.95 with a highly significant <em>p</em>-value (&lt; 0.0001). The improvement in model fit due to the inclusion of the interaction term is therefore statistically significant.</p>
<p>The above analyses skirted around the questions stated in the beginning of <a href="#sec-mlr-interaction" class="quarto-xref"><span>Section 5.7</span></a>. I’ve provided statistical evidence that full model is a better fit than the reduced model (the sequential <em>F</em>-test tested this), so we should use both <code>dist</code> and <code>bio</code> in the model. I have not looked explicitly at the main effects of the predictors. However, we can easily address questions (1) and (2):</p>
<ul>
<li>Question 1: looking at the summary of <code>mod2a</code> tells us that the main effect of <code>dist</code> is a significant (<em>p</em> &lt; 0.0001) predictor of the Sørensen dissimilarity index.</li>
<li>Question 2: the main effect of <code>bio</code> is also significant (<em>p</em> &lt; 0.0001), which is what we’d see if we fit the model <code>mod2b &lt;- lm(Y ~ bio, data = sw)</code>.</li>
</ul>
<p>Question 3 warrants deeper investigation. Next, we will look at the interaction terms in the full model <code>mod2</code> to see if the effect of <code>dist</code> on <code>Y</code> is different for each level of <code>bio</code>.</p>
</section><section id="interpret-the-full-model" class="level3"><h3 class="anchored" data-anchor-id="interpret-the-full-model">Interpret the Full Model</h3>
<p><strong>The model summary</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="co"># Summary of the model</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="fu">summary</span>(mod2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Y ~ dist * bio, data = sw)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.112117 -0.030176 -0.004195  0.023698  0.233520 

Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    5.341e-03  4.177e-03   1.279   0.2013    
dist           3.530e-04  1.140e-05  30.958  &lt; 2e-16 ***
bioB-ATZ      -6.140e-03  1.659e-02  -0.370   0.7114    
bioBMP         3.820e-02  6.659e-03   5.737 1.29e-08 ***
bioECTZ        1.629e-02  6.447e-03   2.527   0.0117 *  
dist:bioB-ATZ  7.976e-04  1.875e-04   4.255 2.30e-05 ***
dist:bioBMP   -1.285e-04  2.065e-05  -6.222 7.31e-10 ***
dist:bioECTZ   4.213e-04  1.801e-05  23.392  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.04837 on 962 degrees of freedom
Multiple R-squared:  0.8607,    Adjusted R-squared:  0.8597 
F-statistic: 849.2 on 7 and 962 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>In the output returned by <code>summary(mod2)</code>, we need to pay special attention to the use of dummy variable encoding for the categorical predictor. The <code>Coefficients</code> section is similar to that of <code>mod1</code> (see <a href="#sec-mod1-summary" class="quarto-xref"><span>Section 5.6.8</span></a>), but now it includes the categorical predictor <code>bio*</code> and the interaction terms <code>dist:bio*</code> (<code>*</code> indicating the levels of the categorical variable). The <code>bio</code> variable has four levels, <code>BMP</code>, <code>B-ATZ</code>, <code>AMP</code>, and <code>ECTZ</code>, and <code>AMP</code> is selected as reference level. This decision to selected <code>AMP</code> as reference is entirely arbitrary, and alphabetical sorting offers a convenient approach to selecting the reference. The coefficients for the other levels of <code>bio</code> are interpreted as the sum of the response variable and the reference level.</p>
<p>The following are the key coefficients in the model summary:</p>
<ul>
<li>
<code>(Intercept)</code>: This is the estimated average value of <code>Y</code> when <code>dist</code> is zero and <code>bio</code> is the reference category (<code>AMP</code>). Its <em>p</em>-value (&gt; 0.05) suggests it’s not significantly different from zero.</li>
<li>Main Effects:
<ul>
<li>
<code>dist</code>: This represents the estimated change in <code>Y</code> for a one-unit increase in <code>dist</code> when the bioregion is the reference category, <code>AMP</code>. The highly significant <em>p</em>-value (&lt; 0.0001) indicates a strong effect of distance in the <code>AMP</code>.</li>
<li>
<code>bioB-ATZ</code>, <code>bioBMP</code>, <code>bioECTZ</code>: These are dummy variables representing different bioregions. Their coefficients indicate the difference in the average value of <code>Y</code> between each of these bioregions and the reference bioregion when <code>dist</code> is zero. Only <code>bioBMP</code> and <code>bioECTZ</code> are significantly different from the reference bioregion, <code>AMP</code>.</li>
</ul>
</li>
<li>Interaction Effects:
<ul>
<li>
<code>dist:bioB-ATZ</code>, <code>dist:bioBMP</code>, <code>dist:bioECTZ</code>: These interaction terms capture how the effect of <code>dist</code> on <code>Y</code> varies across different bioregions. For instance, <code>dist:bioB-ATZ</code> indicates the additional change in the effect of <code>dist</code> in the <code>B-ATZ</code> bioregion compared to the reference bioregion, <code>AMP</code>. All interaction terms are highly significant, suggesting the effect of distance is different across bioregions.</li>
</ul>
</li>
</ul>
<p>Given this explanation, we can now interpret the coefficients of, for example, the <code>bioB-ATZ</code> main effect and <code>dist:bioB-ATZ</code> interaction. Since <code>AMP</code> is the reference bioregion, its effect is absorbed into the intercept term. Therefore, the coefficient for <code>bioB-ATZ</code> directly reflects the difference we are interested in. The coefficient for <code>bioB-ATZ</code> is -0.0061 <span class="math inline">\pm</span> 0.0166 lower than that of the reference, but the associated <em>p</em>-value (&gt; 0.05) indicates that the average value of <code>Y</code> in the <code>B-ATZ</code> bioregion is not significantly different from the reference bioregion, <code>AMP</code>.</p>
<p>If we’d want to report the actual coefficient for <code>B-ATZ</code>, we’d calculate the sum of the coefficients for <code>(Intercept)</code> and <code>bioB-ATZ</code>. This would give us the estimated average value of <code>Y</code> in the <code>B-ATZ</code> bioregion when <code>dist</code> is zero. The associated SE is calculated as the square root of the sum of the squared SEs of the two coefficients. Therefore, the coefficient for <code>B-ATZ</code> is -8^{-4} <span class="math inline">\pm</span> 0.0171.</p>
<p>The coefficient of 8^{-4} for <code>dist:bioB-ATZ</code> indicates that the effect of distance on <code>Y</code> is 8^{-4} units greater in the <code>B-ATZ</code> bioregion compared to the <code>AMP</code> bioregion. The SE of 2^{-4} suggests a high level of precision in this estimate, and the <em>p</em>-value (&lt; 0.0001) indicates that this difference is statistically significant.</p>
<p>As before, to calculate the actual coefficient for <code>dist</code> in the <code>B-ATZ</code> bioregion, we’d sum the coefficients for <code>dist</code> and <code>dist:bioB-ATZ</code>. The associated SE of this sum is calculated as the square root of the sum of the squared SEs of the two coefficients. Therefore, the coefficient for <code>dist</code> in the <code>B-ATZ</code> bioregion is 0.0012 <span class="math inline">\pm</span> 2^{-4}.</p>
<p>Concerning the overall hypothesis, the <code>Adjusted R-squared</code> value of 0.8597 indicates that the model explains 85.97% of the variance in the response variable <code>Y</code>. The <code>F-statistic</code> and associated <code>p-value</code> (&lt; 0.0001) indicate that the model as a whole is highly significant, meaning at least one of the predictors (including interactions) has a significant effect on <code>Y</code>.</p>
<p><strong>The ANOVA table</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a><span class="co"># The ANOVA table</span></span>
<span id="cb28-2"><a href="#cb28-2"></a><span class="fu">anova</span>(mod2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Response: Y
           Df Sum Sq Mean Sq F value    Pr(&gt;F)    
dist        1 8.4199  8.4199 3598.79 &lt; 2.2e-16 ***
bio         3 3.6232  1.2077  516.21 &lt; 2.2e-16 ***
dist:bio    3 1.8648  0.6216  265.69 &lt; 2.2e-16 ***
Residuals 962 2.2507  0.0023                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>The ANOVA table’s interpretation is intuitive and simple: the <code>Pr(&gt;F)</code> column shows the <em>p</em>-value for each predictor in the model. The <code>dist</code> predictor has a highly significant effect on <code>Y</code> (&lt; 0.0001), as do all the bioregions and their interactions with <code>dist</code>. This confirms the results we obtained from the coefficients. We don’t need to overthink this result.</p>
</section></section><section id="example-3-the-final-model" class="level2 page-columns page-full" data-number="5.8"><h2 data-number="5.8" class="anchored" data-anchor-id="example-3-the-final-model">
<span class="header-section-number">5.8</span> Example 3: The Final Model</h2>
<p>I’ll now expand <code>mod1</code> to include <code>bio</code> as a predictor alongside <code>augMean</code>, <code>febSD</code>, and <code>augSD</code> (<code>mod1</code> was applied only to data pertaining to <code>ECTZ</code>, one of the four levels in <code>bio</code>).</p>
<span class="math display">\begin{align}
Y &amp;= \alpha + \beta_1 \text{augMean} + \beta_2 \text{febSD} + \beta_3 \text{augSD} \nonumber \\
  &amp;\quad + \beta_4 \text{bio}_{\text{B-ATZ}} + \beta_5 \text{bio}_{\text{BMP}} + \beta_6 \text{bio}_{\text{ECTZ}} \nonumber \\
  &amp;\quad + \beta_7 (\text{augMean} \times \text{bio}_{\text{B-ATZ}}) + \beta_8 (\text{augMean} \times \text{bio}_{\text{BMP}}) \nonumber \\
  &amp;\quad + \beta_9 (\text{augMean} \times \text{bio}_{\text{ECTZ}}) + \beta_{10} (\text{febSD} \times \text{bio}_{\text{B-ATZ}}) \nonumber \\
  &amp;\quad + \beta_{11} (\text{febSD} \times \text{bio}_{\text{BMP}}) + \beta_{12} (\text{febSD} \times \text{bio}_{\text{ECTZ}}) \nonumber \\
  &amp;\quad + \beta_{13} (\text{augSD} \times \text{bio}_{\text{B-ATZ}}) + \beta_{14} (\text{augSD} \times \text{bio}_{\text{BMP}}) \nonumber \\
  &amp;\quad + \beta_{15} (\text{augSD} \times \text{bio}_{\text{ECTZ}}) + \epsilon \label{mod3}
\end{align}</span>
<p>Where:</p>
<ul>
<li>
<span class="math inline">Y</span>: The response variable (mean Sørensen dissimilarity).</li>
<li>
<span class="math inline">\alpha</span>: The intercept term, representing the expected value of <code>Y</code> when all predictors are zero and <code>bio</code> is at the reference level <code>AMP</code>).</li>
<li>
<span class="math inline">\beta_1</span>: The coefficient for the main effect of <code>augMean.</code>
</li>
<li>
<span class="math inline">\beta_2</span>: The coefficient for the main effect of <code>febSD.</code>
</li>
<li>
<span class="math inline">\beta_3</span>: The coefficient for the main effect of <code>augSD.</code>
</li>
<li>
<span class="math inline">\beta_4, \beta_5, \beta_6</span>: The coefficients for the main effects of the categorical predictor <code>bio</code> (for levels <code>B-ATZ</code>, <code>BMP</code>, and <code>ECTZ</code> respectively, with <code>AMP</code> as the reference category).</li>
<li>
<span class="math inline">\beta_7, \beta_8, \beta_9</span>: The coefficients for the interaction effects between <code>augMean</code> and <code>bio</code> (for levels <code>B-ATZ</code>, <code>BMP</code>, and <code>ECTZ</code> respectively).</li>
<li>
<span class="math inline">\beta_{10}, \beta_{11}, \beta_{12}</span>: The coefficients for the interaction effects between <code>febSD</code> and <code>bio</code> (for levels <code>B-ATZ</code>, <code>BMP</code>, and <code>ECTZ</code> respectively).</li>
<li>
<span class="math inline">\beta_{13}, \beta_{14}, \beta_{15}</span>: The coefficients for the interaction effects between <code>augSD</code> and <code>bio</code> (for levels <code>B-ATZ</code>, <code>BMP</code>, and <code>ECTZ</code> respectively).</li>
<li>
<span class="math inline">\epsilon</span>: The error term, representing the unexplained variability in the response variable.</li>
</ul>
<p>In this multiple regression model, we aim to understand the complex and interacting relationships between the response variables and the set of predictors. It allows us to investigate not only the individual effects of the continuous predictors on <code>Y</code>, but also how these effects might vary across the different bioregions.</p>
<p>The model therefore incorporates interaction terms between each continuous predictor (<code>augMean</code>, <code>febSD</code>, and <code>augSD</code>) and the categorical variable <code>bio</code>. This allows us to assess whether the relationships between <code>augMean</code>, <code>febSD</code>, or <code>augSD</code> and <code>Y</code> change depending on the specific bioregion. Essentially, we are testing whether the slopes of these relationships are different in different bioregions.</p>
<p>Additionally, the model examines the main effects of the bioregions themselves on <code>Y</code>. This means we’re testing whether the average value of <code>Y</code> differs significantly across bioregions, after accounting for the influence of the continuous predictors.</p>
<p>This is how these different insights pertain to the model components:</p>
<ul>
<li>Main Effects: The coefficients for the main effects of <code>augMean</code>, <code>febSD</code>, and <code>augSD</code> represent the effect of each predictor when <code>bio</code> is at its reference level.</li>
<li>Coefficients for <code>bio</code>: The coefficients for <code>bio</code> (e.g., <span class="math inline">\beta_4 \text{bio}_{\text{B-ATZ}}</span>) represent the difference in the intercept for the corresponding level of <code>bio</code> compared to the reference level.</li>
<li>Interaction Terms: The interaction terms allow the slopes of <code>augMean</code>, <code>febSD</code>, and <code>augSD</code> to vary across the different levels of <code>bio</code>. For example, <span class="math inline">\beta_7 (\text{augMean} \times \text{bio}_{\text{B-ATZ}})</span> represents how the effect of <code>augMean</code> on <code>Y</code> changes when <code>bio</code> is <code>B-ATZ</code> compared to <code>AMP</code>.</li>
</ul>
<section id="state-the-hypotheses" class="level3"><h3 class="anchored" data-anchor-id="state-the-hypotheses">State the Hypotheses</h3>
<p><strong>Overall hypothesis</strong></p>
<p>I’ll only state the overall hypothesis for this model as the expansion of the individual hypotheses for each predictor and interactions (all the <span class="math inline">\beta</span>-coefficients in Equation ) is quite voluminous.</p>
<p>The null is that there is no relationship between the response variable <code>Y</code> and the predictors (including their interactions):</p>
<ul>
<li><span class="math inline">H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = \beta_7 = \beta_8 = \beta_9 = \beta_{10} = \beta_{11} = \beta_{12} = \beta_{13} = \beta_{14} = \beta_{15} = 0</span></li>
</ul>
<p>The alternative is that at least one predictor or interaction term has a significant relationship with the response variable <code>Y</code>:</p>
<ul>
<li><span class="math inline">H_A: \text{At least one } \beta_i \neq 0 \text{ for } i \in \{1, 2, ..., 15\}</span></li>
</ul></section><section id="fit-the-model-1" class="level3"><h3 class="anchored" data-anchor-id="fit-the-model-1">Fit the Model</h3>
<p>In <a href="#sec-example1" class="quarto-xref"><span>Section 5.6</span></a> I included the ECTZ seaweed flora in my analysis, but here I expand it to the full dataset. To assure myself that there is not a high degree of multicollinearity between the predictors, I have calculated the variance inflation factors (VIFs) for the full model (not shown). This allowed me to retain the same three predictors used in <code>mod1</code>, i.e.&nbsp;<code>augMean</code>, <code>febSD</code>, and <code>augSD</code>. This is the point of departure for <code>mod3</code>.</p>
<p>Now I fit the model with those three continuous predictors and their interactions with the categorical variable <code>bio</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># Make a dataframe with only the relevant columns</span></span>
<span id="cb30-2"><a href="#cb30-2"></a>sw_sub2 <span class="ot">&lt;-</span> sw <span class="sc">|&gt;</span></span>
<span id="cb30-3"><a href="#cb30-3"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(Y, augMean, febSD, augSD, bio)</span>
<span id="cb30-4"><a href="#cb30-4"></a></span>
<span id="cb30-5"><a href="#cb30-5"></a><span class="co"># Fit the multiple linear regression model with interaction terms</span></span>
<span id="cb30-6"><a href="#cb30-6"></a>full_mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> (augMean <span class="sc">+</span> febSD <span class="sc">+</span> augSD) <span class="sc">*</span> bio, <span class="at">data =</span> sw_sub2)</span>
<span id="cb30-7"><a href="#cb30-7"></a>full_mod3a <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> augMean <span class="sc">+</span> febSD <span class="sc">+</span> augSD, <span class="at">data =</span> sw_sub2)</span>
<span id="cb30-8"><a href="#cb30-8"></a>null_mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> sw_sub2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Model <code>full_mod3a</code> is similar to <code>full_mod3</code> but without the interaction terms. This will allow me to compare the two models and assess the importance of the interactions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="co"># Compare the models</span></span>
<span id="cb31-2"><a href="#cb31-2"></a><span class="fu">anova</span>(full_mod3, full_mod3a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Model 1: Y ~ (augMean + febSD + augSD) * bio
Model 2: Y ~ augMean + febSD + augSD
  Res.Df    RSS  Df Sum of Sq      F    Pr(&gt;F)    
1    954 3.5603                                   
2    966 5.6890 -12   -2.1288 47.535 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="fu">AIC</span>(full_mod3, full_mod3a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           df       AIC
full_mod3  17 -2652.498
full_mod3a  5 -2221.852</code></pre>
</div>
</div>
<p>The AIC value for <code>full_mod3</code> is lower than that of <code>full_mod3a</code>, indicating that including the interaction with <code>bio</code> is necessary. Likewise, the ANOVA test also shows that the full model (lower residual sum of squares) is significantly better than the reduced model.</p>
<p>I therefore use <code>full_mod3</code> going forward. This is a complex model so I have used the stepwise selection function, <code>stepAIC()</code>, to identify the most important predictors and interactions (code and output not shown). I hoped that this might have simplified the model somewhat, but the simplification I had hoped for did not materialise.</p>
</section><section id="interpret-the-model" class="level3"><h3 class="anchored" data-anchor-id="interpret-the-model">Interpret the Model</h3>
<p><strong>The model summary</strong></p>
<p>The model summary provides a detailed look at the individual predictors and their interactions in the model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a><span class="co"># Summary of the model</span></span>
<span id="cb35-2"><a href="#cb35-2"></a><span class="fu">summary</span>(mod3) <span class="co"># full_mod3 renamed to mod3 during stepAIC()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Y ~ augMean + bio + augSD + febSD + augMean:bio + 
    bio:augSD + bio:febSD, data = sw_sub2)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.15399 -0.03841 -0.01475  0.03464  0.24051 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       0.0299094  0.0062756   4.766 2.17e-06 ***
augMean           0.3441099  0.0158575  21.700  &lt; 2e-16 ***
bioB-ATZ         -0.0459611  0.0242519  -1.895 0.058374 .  
bioBMP            0.0160756  0.0100749   1.596 0.110906    
bioECTZ          -0.0015444  0.0090275  -0.171 0.864197    
augSD            -0.0059012  0.0034011  -1.735 0.083044 .  
febSD            -0.0006481  0.0027954  -0.232 0.816706    
augMean:bioB-ATZ -0.0461775  0.0874044  -0.528 0.597400    
augMean:bioBMP   -0.2406297  0.0211404 -11.382  &lt; 2e-16 ***
augMean:bioECTZ  -0.0607745  0.0189030  -3.215 0.001348 ** 
bioB-ATZ:augSD    0.0655983  0.0371033   1.768 0.077382 .  
bioBMP:augSD      0.0410220  0.0114706   3.576 0.000366 ***
bioECTZ:augSD     0.0280513  0.0053752   5.219 2.21e-07 ***
bioB-ATZ:febSD    0.0409425  0.0818927   0.500 0.617223    
bioBMP:febSD      0.0056433  0.0150126   0.376 0.707070    
bioECTZ:febSD     0.0502867  0.0082266   6.113 1.43e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.06109 on 954 degrees of freedom
Multiple R-squared:  0.7797,    Adjusted R-squared:  0.7762 
F-statistic: 225.1 on 15 and 954 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The first thing to notice is that the model function has been rewritten in the forward selection process (but none of the variables were deemed insignificant and removed):</p>
<ul>
<li>Initial specification: <code>Y ~ (augMean + febSD + augSD) * bio</code>
</li>
<li>Specification after <code>stepAIC()</code>: <code>Y ~ augMean + bio + augSD + febSD + augMean:bio + bio:augSD + bio:febSD</code>
</li>
</ul>
<p>Functionally, these two are identical, but the order in which the terms are presented differs. Although this has affected the order in which the coefficients are presented in the summary output, the coefficients are the same. The coefficients are:</p>
<ul>
<li>
<code>(Intercept)</code>: This is the estimated average value of <code>Y</code> when all predictor variables are zero and the observation is in the reference bioregion (<code>AMP</code>).</li>
<li>Main Effects:
<ul>
<li>
<code>augMean</code>: For every one-unit increase in <code>augMean</code>, <code>Y</code> increases by 0.3441, on average, assuming all other predictors are held constant. This effect is highly significant.</li>
<li>
<code>augSD</code> and <code>febSD</code>: The main effects of these variables are not statistically significant, suggesting they might not have a direct impact on <code>Y</code> when averaged across all bioregions.</li>
<li>
<code>bioB-ATZ</code>, <code>bioBMP</code>, <code>bioECTZ</code>: These coefficients represent the average difference in <code>Y</code> between each of these bioregions and the reference bioregion, when the continuous predictors are held at zero.</li>
</ul>
</li>
<li>Interaction Effects:
<ul>
<li>
<code>augMean</code> interactions: The significant interactions of <code>augMean</code> with bioregion indicate that the effect of <code>augMean</code> on <code>Y</code> varies across bioregions. Notably, the interaction with <code>bioBMP</code> has a strong, significant negative effect, suggesting that the positive effect of <code>augMean</code> is much weaker in this bioregion compared to the reference.</li>
<li>
<code>augSD</code> and <code>febSD</code> interactions: These interactions with bioregions are sometimes significant, providing good support for the alternative hypothesis that the effects of <code>augSD</code> and <code>febSD</code> on <code>Y</code> depend on the specific bioregion.</li>
</ul>
</li>
</ul>
<p>Since dummy coding returns differences with respect to reference levels, how would we calculate the actual coefficients for, say, <code>augMean</code>? Since there are significant interaction effects, we must consider the main effect of <code>augMean</code> in conjunction with bioregion.</p>
<p>For <code>bio = B-ATZ</code>:</p>
<ul>
<li><span class="math inline">\beta_{\text{augMean}} + \beta_{\text{augMean:bioB-ATZ}} = 0.3441099 + (-0.0461775) = 0.2979324</span></li>
</ul>
<p>For <code>bio = BMP</code>:</p>
<ul>
<li><span class="math inline">\beta_{\text{augMean}} + \beta_{\text{augMean:bioBMP}} = 0.3441099 + (-0.2406297) = 0.1034802</span></li>
</ul>
<p>For <code>bio = ECTZ</code>:</p>
<p><span class="math inline">\beta_{\text{augMean}} + \beta_{\text{augMean:bioECTZ}} = 0.3441099 + (-0.0607745) = 0.2833354</span></p>
<p>The respective SEs for these coefficients can be calculated using the formula for the standard error of the sum of two variables. For example:</p>
<ul>
<li><span class="math inline">SE_{\text{augMean}} = \sqrt{SE_{\text{augMean}}^2 + SE_{\text{augMean:bio}}^2}</span></li>
</ul>
<p><strong>The ANOVA table</strong></p>
<p>The ANOVA table assesses the overall significance of groups of predictors or the sequential addition of predictors to the model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a><span class="fu">anova</span>(mod3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Response: Y
             Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
augMean       1 9.9900  9.9900 2676.902 &lt; 2.2e-16 ***
bio           3 1.1901  0.3967  106.296 &lt; 2.2e-16 ***
augSD         1 0.1393  0.1393   37.331 1.451e-09 ***
febSD         1 0.0053  0.0053    1.422    0.2334    
augMean:bio   3 0.7910  0.2637   70.647 &lt; 2.2e-16 ***
bio:augSD     3 0.3426  0.1142   30.602 &lt; 2.2e-16 ***
bio:febSD     3 0.1401  0.0467   12.517 4.953e-08 ***
Residuals   954 3.5603  0.0037                       
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>The ANOVA table shows that the model is highly significant, with very low <em>p</em>-values throughout (&lt; 0.0001). This indicates that the model as a whole is a good fit for the data.</p>
</section><section id="reporting-1" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="reporting-1">Reporting</h3>
<p>Here is what the reporting of the findings could look like in the Results section in your favourite journal.</p>
<p><strong>Results</strong></p>
<p>A multiple linear regression model examining the effects of the August climatological mean temperature (<code>augMean</code>), the August and February climatological SD of temperature (<code>augSD</code> and <code>febSD</code>, respectively), and the bioregion classification (<code>bio</code>) on the response variable, the Sørensen dissimilarity (<code>Y</code>), including their interaction terms, revealed several significant findings (<a href="#tbl-results" class="quarto-xref">Table&nbsp;<span>5.1</span></a>). This model allows a separate regression slope for each predictor within the bioregions (<a href="#fig-mlr6" class="quarto-xref">Figure&nbsp;<span>5.7</span></a>). The model explains a substantial portion of the variance in <code>Y</code> (<span class="math inline">R^2 = 0.780</span>, adjusted <span class="math inline">R^2 = 0.776</span>), and the overall model fit is highly significant (<span class="math inline">F(15, 954) = 225.1</span>, <span class="math inline">p &lt; 0.0001</span>).</p>
<div id="tbl-results" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full"><div aria-describedby="tbl-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 38%">
<col style="width: 13%">
<col style="width: 15%">
<col style="width: 11%">
<col style="width: 21%">
</colgroup>
<thead><tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
<th>Std. Error</th>
<th>t value</th>
<th>P-value</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>(Intercept)</code></td>
<td>0.0299</td>
<td>0.0063</td>
<td>4.766</td>
<td>&lt; 0.0001 ***</td>
</tr>
<tr class="even">
<td><code>augMean</code></td>
<td>0.3441</td>
<td>0.0159</td>
<td>21.700</td>
<td>&lt; 0.0001 ***</td>
</tr>
<tr class="odd">
<td><code>bioB-ATZ</code></td>
<td>-0.0460</td>
<td>0.0243</td>
<td>-1.895</td>
<td>&gt; 0.05</td>
</tr>
<tr class="even">
<td><code>bioBMP</code></td>
<td>0.0161</td>
<td>0.0101</td>
<td>1.596</td>
<td>&gt; 0.05</td>
</tr>
<tr class="odd">
<td><code>bioECTZ</code></td>
<td>-0.0015</td>
<td>0.0090</td>
<td>-0.171</td>
<td>&gt; 0.05</td>
</tr>
<tr class="even">
<td><code>augSD</code></td>
<td>-0.0059</td>
<td>0.0034</td>
<td>-1.735</td>
<td>&gt; 0.05</td>
</tr>
<tr class="odd">
<td><code>febSD</code></td>
<td>-0.0006</td>
<td>0.0028</td>
<td>-0.232</td>
<td>&gt; 0.05</td>
</tr>
<tr class="even">
<td><code>augMean:bioB-ATZ</code></td>
<td>-0.0462</td>
<td>0.0874</td>
<td>-0.528</td>
<td>&gt; 0.05</td>
</tr>
<tr class="odd">
<td><code>augMean:bioBMP</code></td>
<td>-0.2406</td>
<td>0.0211</td>
<td>-11.382</td>
<td>&lt; 0.0005 ***</td>
</tr>
<tr class="even">
<td><code>augMean:bioECTZ</code></td>
<td>-0.0608</td>
<td>0.0189</td>
<td>-3.215</td>
<td>&lt; 0.005 **</td>
</tr>
<tr class="odd">
<td><code>bioB-ATZ:augSD</code></td>
<td>0.0656</td>
<td>0.0371</td>
<td>1.768</td>
<td>&gt; 0.05</td>
</tr>
<tr class="even">
<td><code>bioBMP:augSD</code></td>
<td>0.0410</td>
<td>0.0115</td>
<td>3.576</td>
<td>&lt; 0.0005 ***</td>
</tr>
<tr class="odd">
<td><code>bioECTZ:augSD</code></td>
<td>0.0281</td>
<td>0.0054</td>
<td>5.219</td>
<td>&lt; 0.0005 ***</td>
</tr>
<tr class="even">
<td><code>bioB-ATZ:febSD</code></td>
<td>0.0409</td>
<td>0.0819</td>
<td>0.500</td>
<td>&gt; 0.05</td>
</tr>
<tr class="odd">
<td><code>bioBMP:febSD</code></td>
<td>0.0056</td>
<td>0.0150</td>
<td>0.376</td>
<td>&gt; 0.05</td>
</tr>
<tr class="even">
<td><code>bioECTZ:febSD</code></td>
<td>0.0503</td>
<td>0.0082</td>
<td>6.113</td>
<td>&lt; 0.0005 ***</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-tbl margin-caption" id="tbl-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5.1: Summary of the multiple linear regression model examining the effects of <code>augMean</code>, <code>augSD</code>, <code>febSD</code>, and <code>bio</code> on <code>Y</code>.
</figcaption></figure>
</div>
<p>The main effect of <code>augMean</code> was highly significant (Estimate = 0.3441, <span class="math inline">p &lt; 0.0001</span>), indicating a strong positive relationship with <code>Y</code>. The interaction term <code>augMean:bioBMP</code> (Estimate = -0.2406, <span class="math inline">p &lt; 0.0001</span>) and <code>augMean:bioECTZ</code> (Estimate = -0.0608, <span class="math inline">p &lt; 0.005</span>) were also significant, suggesting that the effect of <code>augMean</code> on <code>Y</code> varies significantly for <code>BMP</code> and <code>ECTZ</code> bioregions compared to the reference category (<code>AMP</code>). The <code>bioBMP</code> (Estimate = 0.0161, <span class="math inline">p &gt; 0.05</span>) and <code>bioECTZ</code> (Estimate = -0.0015, <span class="math inline">p &gt; 0.05</span>) terms were not significant, indicating no significant difference from <code>AMP</code>.</p>
<p>For <code>augSD</code>, the main effect was not significant (Estimate = -0.0059, <span class="math inline">p &gt; 0.05</span>). Significant interaction terms for <code>bioBMP:augSD</code> (Estimate = 0.0410, <span class="math inline">p &lt; 0.001</span>) and <code>bioECTZ:augSD</code> (Estimate = 0.0281, <span class="math inline">p &lt; 0.0001</span>) indicate that the effect of <code>augSD</code> on <code>Y</code> varies by bioregion.</p>
<p>The main effect of <code>febSD</code> was not significant (Estimate = -0.0006, <span class="math inline">p &gt; 0.05</span>), suggesting no direct relationship with <code>Y</code>. However, the interaction term <code>bioECTZ:febSD</code> (Estimate = 0.0503, <span class="math inline">p = 0.0001</span>) was significant, indicating that the effect of <code>febSD</code> on <code>Y</code> differs for the <code>ECTZ</code> bioregion.</p>
<p>The ANOVA further highlights the overall significance of each predictor. <code>augMean</code> had a highly significant contribution to the model (<span class="math inline">F = 2676.902</span>, <span class="math inline">p &lt; 0.0001</span>), as did <code>bio</code> (<span class="math inline">F = 106.296</span>, <span class="math inline">p &lt; 0.0001</span>), and their interactions (<code>augMean:bio</code>, <span class="math inline">F = 70.647</span>, <span class="math inline">p &lt; 0.0001</span>; <code>bio:augSD</code>, <span class="math inline">F = 30.602</span>, <span class="math inline">p &lt; 0.0001</span>; <code>bio:febSD</code>, <span class="math inline">F = 12.517</span>, <span class="math inline">p = 4.953 \times 10^{-8}</span>). The main effect of <code>augSD</code> was also significant (<span class="math inline">F = 37.331</span>, <span class="math inline">p = 1.451 \times 10^{-9}</span>), while <code>febSD</code> did not significantly contribute to the model on its own (<span class="math inline">F = 1.422</span>, <span class="math inline">p = 0.2334</span>).</p>
<p>These findings suggest that the effects of <code>augMean</code>, <code>augSD</code>, and <code>febSD</code> on <code>Y</code> are influenced by the bioregional classification, with significant variations in the relationships depending on the specific bioregion.</p>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-mlr6" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-mlr6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="multiple_linear_regression_files/figure-html/fig-mlr6-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-mlr6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.7: Individual linear regression fit to the variables <code>augMean</code>, <code>febSD</code>, and <code>augSD</code> for each bioregion as predictors of the seaweed species composition.
</figcaption></figure>
</div>
</div>
</div>
</section></section><section id="sec-contrasts" class="level2" data-number="5.9"><h2 data-number="5.9" class="anchored" data-anchor-id="sec-contrasts">
<span class="header-section-number">5.9</span> Alternative Categorical Variable Coding Schemes (Contrasts)</h2>
<p>Throughout the book, we have used dummy variable coding the specify the categorical variables in the multiple linear regression models. But, should dummy variable coding not be to your liking, there are other coding schemes that can be used to represent the categorical variables. These alternative coding schemes are known as contrasts. The choice of contrast coding can affect the interpretation of the regression coefficients.</p>
<p>I’ll provide some synthetic data to illustrate a few different contrasts. The data consist of a continuous variable <code>x</code>, a categorical variable <code>cat_var</code> with four levels, and a response variable <code>y</code> that has some relationship with <code>x</code> and <code>cat_var</code>. I’ll use dummy variable coding as the reference (haha!).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a><span class="fu">head</span>(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           y           x cat_var
1  0.6667876 -0.56047565       B
2  1.3086873 -0.23017749       B
3  0.4496192  1.55870831       D
4  2.1326402  0.07050839       A
5 -2.8608771  0.12928774       D
6  0.1497346  1.71506499       D</code></pre>
</div>
</div>
<p>Categorical variable coding (any scheme) only affects the interpretation of the categorical variable main effects and their interactions, so I’ll not discuss the coefficient associated with the continuous variable <code>x</code> (the slope) in the model throughout the explanations offered below.</p>
<p><strong>Dummy Variable Coding (Treatment Contrasts)</strong></p>
<p>This is the most commonly used coding scheme, and <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>’s default. One level is the reference category (<code>A</code>) and the other levels are compared against it. Contrast matrices can be assigned and/or inspected using the <code><a href="https://rdrr.io/r/stats/contrasts.html">contrasts()</a></code> function. For the dummy coding, the reference level <code>A</code> will remain 0 and the other levels will be independently coded as 1 in three columns. You’ll now understand why, when we have four levels within a categorical variable, we only need three dummy variables to represent them.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1"></a><span class="co"># Dummy coding (treatment coding) ... default</span></span>
<span id="cb41-2"><a href="#cb41-2"></a><span class="fu">contrasts</span>(data<span class="sc">$</span>cat_var)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  B C D
A 0 0 0
B 1 0 0
C 0 1 0
D 0 0 1</code></pre>
</div>
</div>
<p>When we have four levels in a categorical variable, there are three dummy variable columns in the contrast matrix. The first row, consisting of all zeros (0, 0, 0), represents the reference level, which in this case is <code>A</code>. The other rows represent the different levels of the categorical variable, with a 1 in the respective column indicating that level. For example, level <code>A</code> is represented by (0, 0, 0), <code>B</code> by (1, 0, 0), <code>C</code> by (0, 1, 0), and <code>D</code> by (0, 0, 1). In the regression model, these contrasts are used to estimate the differences between each level and the reference level. Specifically, the first contrast column indicates that the coefficient for this column will represent the difference between the mean of the response variable for level <code>B</code> and the mean for the reference level <code>A</code>, holding all other variables constant. Similarly, the second and third columns represent the differences between levels <code>C</code> and <code>A</code>, and <code>D</code> and <code>A</code>, respectively. This coding allows for a straightforward interpretation of how each level of the categorical variable affects the response variable relative to the reference level.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1"></a>model_dummy <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> cat_var, <span class="at">data =</span> data)</span>
<span id="cb43-2"><a href="#cb43-2"></a><span class="fu">summary</span>(model_dummy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x + cat_var, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.6615 -0.6297 -0.1494  0.4978  2.9305 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   2.8176     0.1635  17.232  &lt; 2e-16 ***
x             1.8274     0.1040  17.572  &lt; 2e-16 ***
cat_varB     -1.7201     0.2499  -6.883 6.24e-10 ***
cat_varC     -3.9056     0.2678 -14.586  &lt; 2e-16 ***
cat_varD     -5.4880     0.2512 -21.850  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9246 on 95 degrees of freedom
Multiple R-squared:  0.887, Adjusted R-squared:  0.8822 
F-statistic: 186.4 on 4 and 95 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The model summary shows that the coefficients for <code>cat_varB</code>, <code>cat_varC</code>, and <code>cat_varD</code> represent the differences in the mean of the response variable <code>y</code> between the reference category <code>A</code> and categories <code>B</code>, <code>C</code>, and <code>D</code>, respectively, while controlling for the effect of the continuous variable <code>x</code>.</p>
<p>Interpretation:</p>
<ul>
<li>
<code>(Intercept)</code> (2.8176): The intercept represents the estimated mean value of the response (<code>y</code>) when <code>x</code> is zero and the categorical variable is at the reference level <code>A</code>. This is the baseline from which other categories are compared.</li>
<li>
<code>x</code> (1.8274): For each one-unit increase in <code>x</code>, <code>y</code> is expected to increase by 1.8274 units, holding the categorical variable constant. This effect is consistent across all levels of the categorical variable because the model does not have an interaction effect present.</li>
<li>
<code>cat_varB</code> (-1.7201): On average, the value of <code>y</code> for level <code>B</code> is 1.7201 units lower than that for the reference level <code>A</code>, when <code>x</code> is held constant. This corresponds to the (1, 0, 0) row in the contrast matrix.</li>
<li>
<code>cat_varC</code> (-3.9056): Similarly, on average, the value of <code>y</code> for level <code>C</code> is 3.9056 units lower than that for the reference level, when <code>x</code> is held constant. This corresponds to the (0, 1, 0) row in the contrast matrix.</li>
<li>
<code>cat_varD</code> (-5.4880): Lastly, on average, the value of <code>y</code> for level <code>D</code> is 5.4880 units lower compared to the reference , when <code>x</code> is held constant. This is row (0, 0, 1) row in the contrast matrix.</li>
</ul>
<p>All these coefficients are highly significant (<em>p</em> &lt; 0.0001), indicating strong evidence for differences between each category and the reference category <code>A</code>.</p>
<p>The model explains a large proportion of the variance in <code>y</code> (Adjusted <em>R</em>-squared: 0.8822), suggesting a good fit. The <em>F</em>-statistic (186.4) with a very low <em>p</em>-value (&lt; 0.0001) indicates that the model as a whole is statistically significant.</p>
<p>If you want to change the reference level, you can use the <code><a href="https://rdrr.io/r/stats/relevel.html">relevel()</a></code> function. For example, to change the reference level of <code>cat_var</code> variable to <code>C_2</code>, you can use:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1"></a><span class="co"># Set "C" as the reference level for cat_var</span></span>
<span id="cb45-2"><a href="#cb45-2"></a>data<span class="sc">$</span>cat_var <span class="ot">&lt;-</span> <span class="fu">relevel</span>(data<span class="sc">$</span>cat_var, <span class="at">ref =</span> <span class="st">"C"</span>)</span>
<span id="cb45-3"><a href="#cb45-3"></a><span class="fu">contrasts</span>(data<span class="sc">$</span>cat_var)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  A B D
C 0 0 0
A 1 0 0
B 0 1 0
D 0 0 1</code></pre>
</div>
</div>
<p>This may be useful when you want to compare the other levels to a different reference level.</p>
<p><strong>Effect Coding (Sum Contrasts)</strong></p>
<p>This coding method compares the levels of a categorical variable to the overall mean of the dependent variable. The coefficients represent the difference between each level and the grand mean. Instead of using 0 and 1 as we did with dummy variable coding, effect coding uses -1, 0, and 1 to represent the different levels of the categorical variable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1"></a><span class="co"># Reset the reference level to "A"</span></span>
<span id="cb47-2"><a href="#cb47-2"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x, cat_var)</span>
<span id="cb47-3"><a href="#cb47-3"></a></span>
<span id="cb47-4"><a href="#cb47-4"></a><span class="co"># Effect coding</span></span>
<span id="cb47-5"><a href="#cb47-5"></a><span class="fu">contrasts</span>(data<span class="sc">$</span>cat_var) <span class="ot">&lt;-</span> <span class="fu">contr.sum</span>(<span class="dv">4</span>)</span>
<span id="cb47-6"><a href="#cb47-6"></a><span class="fu">contrasts</span>(data<span class="sc">$</span>cat_var)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  [,1] [,2] [,3]
A    1    0    0
B    0    1    0
C    0    0    1
D   -1   -1   -1</code></pre>
</div>
</div>
<p>In effect coding (sum contrasts), each level of the categorical variable is compared to the overall mean rather than a specific reference category. This contrast matrix with four levels (A, B, C, D) and three columns can be interpreted as follows:</p>
<ul>
<li>Level <code>A</code> (1, 0, 0): The first row indicates that level <code>A</code> is included in the first contrast (<code>cat_var1</code>), which means the mean of level <code>A</code> is being compared to the overall mean. Since the other columns are zero, level <code>A</code> does not contribute to the other contrasts.</li>
<li>Level <code>B</code> (0, 1, 0): The second row indicates that level <code>B</code> is included in the second contrast (<code>cat_var2</code>). The mean of level <code>B</code> is being compared to the overall mean, and it does not contribute to the other contrasts.</li>
<li>Level <code>C</code> (0, 0, 1): The third row indicates that level <code>C</code> is included in the third contrast (<code>cat_var3</code>). The mean of level C is being compared to the overall mean, and it does not contribute to the other contrasts.</li>
<li>Level <code>D</code> (-1, -1, -1): The fourth row is a balancing row, ensuring that the sum of the contrasts for each level equals zero. This indicates that level D is being compared to the overall mean indirectly by balancing the contributions of levels <code>A</code>, <code>B</code>, and <code>C</code>.</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1"></a>model_effect <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> cat_var, <span class="at">data =</span> data)</span>
<span id="cb49-2"><a href="#cb49-2"></a><span class="fu">summary</span>(model_effect)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x + cat_var, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.6615 -0.6297 -0.1494  0.4978  2.9305 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.03921    0.09452   0.415    0.679    
x            1.82741    0.10400  17.572  &lt; 2e-16 ***
cat_var1     2.77844    0.14968  18.563  &lt; 2e-16 ***
cat_var2     1.05832    0.16329   6.481 4.04e-09 ***
cat_var3    -1.12720    0.17765  -6.345 7.53e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9246 on 95 degrees of freedom
Multiple R-squared:  0.887, Adjusted R-squared:  0.8822 
F-statistic: 186.4 on 4 and 95 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Interpretation:</p>
<ul>
<li>
<code>(Intercept)</code> 0.03921: The intercept represents the grand mean of the response variable (<code>y</code>). Since the intercept is not statistically significant (<em>p</em> &gt; 0.05), it indicates that the overall mean is not significantly different from zero when considering the average effect of all levels of the categorical variable.</li>
<li>
<code>x</code> (1.82741): For each one-unit increase in (<code>x</code>), the response (<code>y</code>) increases by approximately 1.82741 units. This effect is highly significant (<em>p</em> &lt; 0.0001).</li>
<li>
<code>cat_var1</code> (2.77844): Level <code>A</code> has a mean (<code>y</code>) that is 2.77844 units higher than the grand mean. This effect is highly significant (<em>p</em> &lt; 0.0001).</li>
<li>
<code>cat_var2</code> (1.05832): Level <code>B</code> has a mean (<code>y</code>) that is 1.05832 units higher than the grand mean. This effect is also highly significant (<em>p</em> &lt; 0.0001).</li>
<li>
<code>cat_var3</code> (-1.12720): Level <code>C</code> has a mean (<code>y</code>) that is 1.12720 units lower than the grand mean. This effect is highly significant (p &lt; 0.0001).</li>
</ul>
<p>All these coefficients are highly significant (<em>p</em> &lt; 0.0001), indicating strong evidence for differences between each category and the overall mean of all levels.</p>
<p>The model explains a large proportion of the variance in <code>y</code> (Adjusted <em>R</em>-squared: 0.8822), suggesting a good fit. The <em>F</em>-statistic (186.4) with a very low <em>p</em>-value (&lt; 0.0001) indicates that the model as a whole is statistically significant.</p>
<p><strong>Helmert Coding</strong></p>
<p>Helmert coding compares each level of a categorical variable to the mean of the subsequent levels. It is useful for testing ordered differences.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1"></a><span class="co"># Helmert coding</span></span>
<span id="cb51-2"><a href="#cb51-2"></a><span class="fu">contrasts</span>(data<span class="sc">$</span>cat_var) <span class="ot">&lt;-</span> <span class="fu">contr.helmert</span>(<span class="dv">4</span>)</span>
<span id="cb51-3"><a href="#cb51-3"></a><span class="fu">contrasts</span>(data<span class="sc">$</span>cat_var)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  [,1] [,2] [,3]
A   -1   -1   -1
B    1   -1   -1
C    0    2   -1
D    0    0    3</code></pre>
</div>
</div>
<p>The contrast matrix for a categorical variable with four levels (<code>A</code>, <code>B</code>, <code>C</code>, <code>D</code>) and three columns can be interpreted as follows:</p>
<ul>
<li>Level <code>A</code> (-1, -1, -1): Level <code>A</code> is compared to the mean of levels <code>B</code>, <code>C</code>, and <code>D</code>. The negative values indicate that level A is being subtracted in these comparisons.</li>
<li>Level <code>B</code> (1, -1, -1): Level <code>B</code> is compared to the mean of levels <code>C</code> and <code>D</code>. The positive value in the first column indicates that level <code>B</code> is being added in this comparison.</li>
<li>Level <code>C</code> (0, 2, -1): Level <code>C</code> is compared to the mean of level <code>D</code>. The positive value in the second column indicates that level <code>C</code> is being added in this comparison, while the negative value in the third column is part of the comparison for subsequent levels.</li>
<li>Level <code>D</code> (0, 0, 3): Level <code>D</code> is compared on its own in the final contrast. The positive value in the third column indicates that level <code>D</code> is being added in this comparison.</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1"></a>model_helmert <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> cat_var, <span class="at">data =</span> data)</span>
<span id="cb53-2"><a href="#cb53-2"></a><span class="fu">summary</span>(model_helmert)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x + cat_var, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.6615 -0.6297 -0.1494  0.4978  2.9305 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.03921    0.09452   0.415    0.679    
x            1.82741    0.10400  17.572  &lt; 2e-16 ***
cat_var1    -0.86006    0.12495  -6.883 6.24e-10 ***
cat_var2    -1.01519    0.08206 -12.371  &lt; 2e-16 ***
cat_var3    -0.90319    0.05477 -16.491  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9246 on 95 degrees of freedom
Multiple R-squared:  0.887, Adjusted R-squared:  0.8822 
F-statistic: 186.4 on 4 and 95 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Interpretation:</p>
<ul>
<li>
<code>(Intercept)</code> (0.03921): The grand mean of <code>y</code> when <code>x</code> is zero.</li>
<li>
<code>x</code> (1.82741): For each unit increase in x , y increases by 1.82741 units.</li>
<li>
<code>cat_var1</code> (-0.86006): The mean of level <code>A</code> is 0.86006 units lower than the combined mean of levels <code>B</code>, <code>C</code>, and <code>D</code>.</li>
<li>
<code>cat_var2</code> (-1.01519): The mean of level <code>B</code> is 1.01519 units lower than the combined mean of levels <code>C</code> and <code>D</code>.</li>
<li>
<code>cat_var3</code> (-0.90319): The mean of level <code>C</code> is 0.90319 units lower than the mean of level <code>D</code>.</li>
</ul>
<p>The interpretation of the overall model remains more-or-less similar to before:</p>
<p>All these coefficients are highly significant (<em>p</em> &lt; 0.0001), indicating strong evidence for differences between each level and the overall mean of all subsequent levels.</p>
<p>The model explains a large proportion of the variance in <code>y</code> (Adjusted <em>R</em>-squared: 0.8822), suggesting a good fit. The <em>F</em>-statistic (186.4) with a very low <em>p</em>-value (&lt; 0.0001) indicates that the model as a whole is statistically significant.</p>
</section><section id="exercises" class="level2" data-number="5.10"><h2 data-number="5.10" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">5.10</span> Exercises</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Task G
</div>
</div>
<div class="callout-body-container callout-body">
<p>Use the data loaded at the start of this chapter for this task.</p>
<p>In this task you will develop data analysis, undertake model building, and provide an interpretation of the findings. Your goal is to explore the species composition and assembly processes of the seaweed flora around the coast of South Africa. See <span class="citation" data-cites="smit2017seaweeds">Smit et al. (<a href="references.html#ref-smit2017seaweeds" role="doc-biblioref">2017</a>)</span> for more information about the data and the analysis.</p>
<ol type="a">
<li>
<p><strong>Analysis:</strong> Please develop multiple linear regression models for the seaweed species composition (<span class="math inline">\beta_\text{sim}</span> and <span class="math inline">\beta_\text{sne}</span>, i.e.&nbsp;columns called <code>Y1</code> and <code>Y2</code>, respectively) using the all the predictors in this dataset. At the end, the final model(s) that best describe(s) the species assembly processes operating along the South African coast should be presented. The final model may/may not contain all the predictors in the dataset, and it is your goal to justify the variable and model selection.</p>
<ul>
<li>Accomplishing a) will require that you work through the whole model-building process as outlined in the chapter. This includes the following steps:
<ul>
<li>Data exploration and visualisation (EDA)</li>
<li>Model building (providing hypothesis statements, variable selection using VIF and forward selection, comparisons of nested models, justifications for model selection)</li>
<li>Model diagnostics</li>
<li>Explanation of <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> and <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> outputs</li>
<li>Producing the Results section</li>
<li><strong>[60%]</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Interpretation:</strong> Once you have arrived at the best model, discuss your findings in the light of the appropriate ecological hypotheses that explain the relationships between the predictors and the seaweed species composition. Include insights drawn from the analysis of <span class="math inline">\beta_\text{sør}</span> that I developed in this chapter, and also rely on the theory you have developed for the lecture material the class presented in Task A2.</p>
<ul>
<li>Accomplishing b) is thus all about model interpretation and discussing the ecological relevance of the results.</li>
<li><strong>[40%]</strong></li>
</ul>
</li>
</ol>
<p>The format of this task is a Quarto file that will be converted to an HTML file. The HTML file will contain the graphs, all calculations, and the text sections. The task should be written up as a publication (i.e.&nbsp;use appropriate headings) using a journal style of your choice. Aside from this, there are no limitations.</p>
</div>
</div>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-graham2003confronting" class="csl-entry" role="listitem">
Graham MH (2003) Confronting multicollinearity in ecological multiple regression. Ecology 84:2809–2815.
</div>
<div id="ref-smit2017seaweeds" class="csl-entry" role="listitem">
Smit AJ, Bolton JJ, Anderson RJ (2017) Seaweeds in two oceans: Beta-diversity. <span>Frontiers in Marine Science</span> 4:404.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./polynomial_regression.html" class="pagination-link" aria-label="Polynomial Regression">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Polynomial Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./generalised_linear_models.html" class="pagination-link" aria-label="Generalised Linear Models (GLM)">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Generalised Linear Models (GLM)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb55" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb55-1"><a href="#cb55-1"></a><span class="fu"># Multiple Linear Regression {#sec-multiple-linear-regression}</span></span>
<span id="cb55-2"><a href="#cb55-2"></a></span>
<span id="cb55-3"><a href="#cb55-3"></a>In @sec-simple-linear-regression we have seen how to model the relationship between two variables using simple linear regression (SLR). However, in ecosystems, the relationship between the response variable and the explanatory variables is more complex and in many cases cannot be adequately captured by a single driver (i.e. influential or predictor variable). In such cases, multiple linear regression (MLR) can be used to model the relationship between the response variable and multiple explanatory variables. </span>
<span id="cb55-4"><a href="#cb55-4"></a></span>
<span id="cb55-5"><a href="#cb55-5"></a><span class="fu">## Multiple Linear Regression</span></span>
<span id="cb55-6"><a href="#cb55-6"></a></span>
<span id="cb55-7"><a href="#cb55-7"></a>Multiple linear regression helps us answer questions such as:</span>
<span id="cb55-8"><a href="#cb55-8"></a></span>
<span id="cb55-9"><a href="#cb55-9"></a><span class="ss">* </span>How do various environmental factors influence the population size of a species? Factors like average temperature, precipitation levels, and habitat area can be used to predict the population size of a species in a given region. Which of these factors are most important in determining the population size?</span>
<span id="cb55-10"><a href="#cb55-10"></a><span class="ss">* </span>What are the determinants of plant growth in different ecosystems? Variables such as soil nutrient content, water availability, and light exposure can help predict the growth rate of plants in various ecosystems. How do these factors interact to influence plant growth?</span>
<span id="cb55-11"><a href="#cb55-11"></a><span class="ss">* </span>How do genetic and environmental factors affect the spread of a disease in a population? The incidence of a disease might depend on factors like genetic susceptibility, exposure to pathogens, and environmental conditions (e.g., humidity and temperature). What is the relative importance of these factors in determining the spread of the disease?</span>
<span id="cb55-12"><a href="#cb55-12"></a></span>
<span id="cb55-13"><a href="#cb55-13"></a>Multiple linear regression extends the simple linear regression model to include several independent variables. The model is expressed as:</span>
<span id="cb55-14"><a href="#cb55-14"></a>$$Y_i = \alpha + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_k X_{ik} + \epsilon_i$$ {#eq-mlr1}</span>
<span id="cb55-15"><a href="#cb55-15"></a>Where:</span>
<span id="cb55-16"><a href="#cb55-16"></a></span>
<span id="cb55-17"><a href="#cb55-17"></a><span class="ss">* </span>$Y_i$ is the response variable for the $i$-th observation,</span>
<span id="cb55-18"><a href="#cb55-18"></a><span class="ss">* </span>$X_{i1}, X_{i2}, \ldots, X_{ik}$ are the $k$ predictor variables for the $i$-th observation,</span>
<span id="cb55-19"><a href="#cb55-19"></a><span class="ss">* </span>$\alpha$ is the intercept,</span>
<span id="cb55-20"><a href="#cb55-20"></a><span class="ss">* </span>$\beta_1, \beta_2, \ldots, \beta_k$ are the coefficients for the $k$ predictor variables, and</span>
<span id="cb55-21"><a href="#cb55-21"></a><span class="ss">* </span>$\epsilon_i$ is the error term for the $i$-th observation (the residuals).</span>
<span id="cb55-22"><a href="#cb55-22"></a></span>
<span id="cb55-23"><a href="#cb55-23"></a>When including a categorical variable in a multiple linear regression model, dummy (indicator) variables are used to represent the different levels of the categorical variable. Let's assume we have a categorical variable $C$ with three levels: $C_1$, $C_2$, and $C_3$. We can represent this categorical variable using two dummy variables:</span>
<span id="cb55-24"><a href="#cb55-24"></a></span>
<span id="cb55-25"><a href="#cb55-25"></a><span class="ss">* </span>$D_1$: Equals 1 if $C = C_2$, 0 otherwise.</span>
<span id="cb55-26"><a href="#cb55-26"></a><span class="ss">* </span>$D_2$: Equals 1 if $C = C_3$, 0 otherwise.</span>
<span id="cb55-27"><a href="#cb55-27"></a></span>
<span id="cb55-28"><a href="#cb55-28"></a>$C_1$ is considered the reference category and does not get a dummy variable. This way, we avoid multicollinearity (see @sec-multicollinearity). R's <span class="in">`lm()`</span> function will automatically convert the categorical variables to dummy variables (sometimes called treatment coding). The first level of the alphabetically sorted categorical variable is taken as the reference level. See @sec-r-function for more information about how to include categorical variables in a multiple linear regression model. At the end of the chapter you'll find alternative ways to assess categorical variables in a multiple linear regression model (@sec-contrasts).</span>
<span id="cb55-29"><a href="#cb55-29"></a></span>
<span id="cb55-30"><a href="#cb55-30"></a>Assume we also have $k$ continuous predictors $X_{1}, X_{2}, \ldots, X_{k}$. The multiple linear regression model with these predictors and the categorical variable can be expressed as:</span>
<span id="cb55-31"><a href="#cb55-31"></a>$$Y_i = \alpha + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_k X_{ik} + \gamma_1 D_{i1} + \gamma_2 D_{i2} + \epsilon_i$$ {#eq-mlr2}</span>
<span id="cb55-32"><a href="#cb55-32"></a>Where:</span>
<span id="cb55-33"><a href="#cb55-33"></a></span>
<span id="cb55-34"><a href="#cb55-34"></a><span class="ss">* </span>$Y_i$ is the dependent variable for observation $i$.</span>
<span id="cb55-35"><a href="#cb55-35"></a><span class="ss">* </span>$\alpha$ is the intercept term.</span>
<span id="cb55-36"><a href="#cb55-36"></a><span class="ss">* </span>$\beta_1, \beta_2, \ldots, \beta_k$ are the coefficients for the continuous independent variables $X_{i1}, X_{i2}, \ldots, X_{ik}$.</span>
<span id="cb55-37"><a href="#cb55-37"></a><span class="ss">* </span>$D_{i1}$ and $D_{i2}$ are the dummy variables for the categorical predictor $C$.</span>
<span id="cb55-38"><a href="#cb55-38"></a><span class="ss">* </span>$\gamma_1$ and $\gamma_2$ are the coefficients for the dummy variables, representing the effect of levels $C_2$ and $C_3$ relative to the reference level $C_1$.</span>
<span id="cb55-39"><a href="#cb55-39"></a><span class="ss">* </span>$\epsilon_i$ is the error term for observation $i$.</span>
<span id="cb55-40"><a href="#cb55-40"></a></span>
<span id="cb55-41"><a href="#cb55-41"></a><span class="fu">## Nature of the Data</span></span>
<span id="cb55-42"><a href="#cb55-42"></a></span>
<span id="cb55-43"><a href="#cb55-43"></a>You are referred to the discussion in simple linear regression (@sec-simple-linear-regression). The only added consideration is that the data should be multivariate, i.e., it should contain more than one predictor variable. The predictor variables are generally continuous, but there may also be categorical variables.</span>
<span id="cb55-44"><a href="#cb55-44"></a></span>
<span id="cb55-45"><a href="#cb55-45"></a><span class="fu">## Assumptions</span></span>
<span id="cb55-46"><a href="#cb55-46"></a></span>
<span id="cb55-47"><a href="#cb55-47"></a>Basically, this is as already discussed in simple linear regression (@sec-simple-linear-regression)---in multiple linear regression, the same assumptions apply to the response relative to each of the predictor variables. In @sec-diagnostics I will assess the assumptions in an example dataset. An additional consideration is that the predictors must not be highly correlated with each other (multicollinearity) (see @sec-multicollinearity).</span>
<span id="cb55-48"><a href="#cb55-48"></a></span>
<span id="cb55-49"><a href="#cb55-49"></a><span class="fu">## Outliers</span></span>
<span id="cb55-50"><a href="#cb55-50"></a></span>
<span id="cb55-51"><a href="#cb55-51"></a>Again, this is as discussed in simple linear regression (@sec-simple-linear-regression). In multiple linear regression, the same considerations apply to the response relative to each of the predictor variables.</span>
<span id="cb55-52"><a href="#cb55-52"></a></span>
<span id="cb55-53"><a href="#cb55-53"></a><span class="fu">## R Function {#sec-r-function}</span></span>
<span id="cb55-54"><a href="#cb55-54"></a></span>
<span id="cb55-55"><a href="#cb55-55"></a>The <span class="in">`lm()`</span> function in R is used to fit a multiple linear regression model. The syntax is similar to that of the <span class="in">`lm()`</span> function used for simple linear regression, but with multiple predictor variables. The function takes the basic form:</span>
<span id="cb55-56"><a href="#cb55-56"></a></span>
<span id="cb55-57"><a href="#cb55-57"></a><span class="in">```r</span></span>
<span id="cb55-58"><a href="#cb55-58"></a><span class="fu">lm</span>(formula, data)</span>
<span id="cb55-59"><a href="#cb55-59"></a><span class="in">```</span></span>
<span id="cb55-60"><a href="#cb55-60"></a></span>
<span id="cb55-61"><a href="#cb55-61"></a>For a multiple linear regression with only continuous predictor variables (as in @eq-mlr1), the formula is:</span>
<span id="cb55-62"><a href="#cb55-62"></a></span>
<span id="cb55-65"><a href="#cb55-65"></a><span class="in">```{r}</span></span>
<span id="cb55-66"><a href="#cb55-66"></a><span class="co">#| eval: false</span></span>
<span id="cb55-67"><a href="#cb55-67"></a><span class="fu">lm</span>(response <span class="sc">~</span> predictor1 <span class="sc">+</span> predictor2 <span class="sc">+</span> ... <span class="sc">+</span> predictorN,</span>
<span id="cb55-68"><a href="#cb55-68"></a>   <span class="at">data =</span> dataset)</span>
<span id="cb55-69"><a href="#cb55-69"></a><span class="in">```</span></span>
<span id="cb55-70"><a href="#cb55-70"></a></span>
<span id="cb55-71"><a href="#cb55-71"></a>Interaction effects are implemented by including the product of two variables in the formula. For example, to include an interaction between <span class="in">`predictor1`</span> and <span class="in">`predictor2`</span>, we can use:</span>
<span id="cb55-72"><a href="#cb55-72"></a></span>
<span id="cb55-75"><a href="#cb55-75"></a><span class="in">```{r}</span></span>
<span id="cb55-76"><a href="#cb55-76"></a><span class="co">#| eval: false</span></span>
<span id="cb55-77"><a href="#cb55-77"></a><span class="fu">lm</span>(response <span class="sc">~</span> predictor1 <span class="sc">*</span> predictor2, <span class="at">data =</span> dataset)</span>
<span id="cb55-78"><a href="#cb55-78"></a><span class="in">```</span></span>
<span id="cb55-79"><a href="#cb55-79"></a></span>
<span id="cb55-80"><a href="#cb55-80"></a>When we have both continuous and categorical predictor variables (@eq-mlr2), the formula is:</span>
<span id="cb55-81"><a href="#cb55-81"></a></span>
<span id="cb55-84"><a href="#cb55-84"></a><span class="in">```{r}</span></span>
<span id="cb55-85"><a href="#cb55-85"></a><span class="co">#| eval: false</span></span>
<span id="cb55-86"><a href="#cb55-86"></a><span class="fu">lm</span>(response <span class="sc">~</span> continuous_predictor1 <span class="sc">+</span> continuous_predictor2 <span class="sc">+</span> ...</span>
<span id="cb55-87"><a href="#cb55-87"></a>   <span class="sc">+</span> continuous_predictorN <span class="sc">+</span> <span class="fu">factor</span>(categorical_predictor1) <span class="sc">+</span></span>
<span id="cb55-88"><a href="#cb55-88"></a>     <span class="fu">factor</span>(categorical_predictor2) <span class="sc">+</span> ...</span>
<span id="cb55-89"><a href="#cb55-89"></a>   <span class="sc">+</span> <span class="fu">factor</span>(categorical_predictorM),</span>
<span id="cb55-90"><a href="#cb55-90"></a>   <span class="at">data =</span> dataset)</span>
<span id="cb55-91"><a href="#cb55-91"></a><span class="in">```</span></span>
<span id="cb55-92"><a href="#cb55-92"></a></span>
<span id="cb55-93"><a href="#cb55-93"></a></span>
<span id="cb55-94"><a href="#cb55-94"></a><span class="fu">## Example 1: The Seaweed Dataset {#sec-example1}</span></span>
<span id="cb55-95"><a href="#cb55-95"></a></span>
<span id="cb55-98"><a href="#cb55-98"></a><span class="in">```{r}</span></span>
<span id="cb55-99"><a href="#cb55-99"></a><span class="co">#| echo: FALSE</span></span>
<span id="cb55-100"><a href="#cb55-100"></a></span>
<span id="cb55-101"><a href="#cb55-101"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb55-102"><a href="#cb55-102"></a><span class="fu">library</span>(vegan)</span>
<span id="cb55-103"><a href="#cb55-103"></a><span class="fu">library</span>(ggpubr)</span>
<span id="cb55-104"><a href="#cb55-104"></a><span class="fu">library</span>(viridis)</span>
<span id="cb55-105"><a href="#cb55-105"></a><span class="fu">library</span>(MASS) <span class="co"># for the stepAIC() function</span></span>
<span id="cb55-106"><a href="#cb55-106"></a><span class="fu">library</span>(car) <span class="co"># for the vif() function, among others</span></span>
<span id="cb55-107"><a href="#cb55-107"></a><span class="fu">library</span>(ggfortify) <span class="co"># for autoplot()</span></span>
<span id="cb55-108"><a href="#cb55-108"></a><span class="fu">palette</span>(<span class="fu">viridis</span>(<span class="dv">8</span>))</span>
<span id="cb55-109"><a href="#cb55-109"></a><span class="in">```</span></span>
<span id="cb55-110"><a href="#cb55-110"></a></span>
<span id="cb55-111"><a href="#cb55-111"></a>Load some <span class="co">[</span><span class="ot">data</span><span class="co">](https://tangledbank.netlify.app/data/seaweed/spp_df2.csv)</span> produced in the analysis by @smit2017seaweeds. Please refer to the chapter <span class="co">[</span><span class="ot">Deep Dive into Gradients</span><span class="co">](https://tangledbank.netlify.app/BCB743/06-deep_dive.html)</span> on Tangled Bank for the data description.</span>
<span id="cb55-112"><a href="#cb55-112"></a></span>
<span id="cb55-113"><a href="#cb55-113"></a>This dataset is suitable for a multiple linear regression because it has continuous response variables ($\beta_\text{sør}$, $\beta_\text{sim}$, and $\beta_\text{sne}$, the Sørenesen dissimilarity, the turnover component of $\beta$-diversity, and the nestedness-resultant component of $\beta$-diversity, respectively), continuous predictor variables (the mean climatological temperature for August, the mean climatological temperature for the year, the temperature range for February and August, and the SD of February and August), and a categorical variable (the bioregional classification of the samples).</span>
<span id="cb55-114"><a href="#cb55-114"></a></span>
<span id="cb55-117"><a href="#cb55-117"></a><span class="in">```{r}</span></span>
<span id="cb55-118"><a href="#cb55-118"></a>sw <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"data/spp_df2.csv"</span>)</span>
<span id="cb55-119"><a href="#cb55-119"></a><span class="fu">rbind</span>(<span class="fu">head</span>(sw, <span class="dv">3</span>), <span class="fu">tail</span>(sw, <span class="dv">3</span>))[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb55-120"><a href="#cb55-120"></a><span class="in">```</span></span>
<span id="cb55-121"><a href="#cb55-121"></a></span>
<span id="cb55-122"><a href="#cb55-122"></a>We will do a multiple linear regression analysis to understand the relationship between some of the environmental variables and the seaweed species. Specifically, we will consider only the variables <span class="in">`augMean`</span>, <span class="in">`febRange`</span>, <span class="in">`febSD`</span>, <span class="in">`augSD`</span>, and <span class="in">`annMean`</span> as predictors of the species composition as measured by $\beta_\text{sør}$ (<span class="in">`Y`</span> in the data file).</span>
<span id="cb55-123"><a href="#cb55-123"></a></span>
<span id="cb55-124"><a href="#cb55-124"></a>The model, which we will call <span class="in">`full_mod1`</span> below, can be stated formally as @eq-full:</span>
<span id="cb55-125"><a href="#cb55-125"></a>$$Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + \beta_5 X_5 + \epsilon$$ {#eq-full}</span>
<span id="cb55-126"><a href="#cb55-126"></a>Where:</span>
<span id="cb55-127"><a href="#cb55-127"></a></span>
<span id="cb55-128"><a href="#cb55-128"></a><span class="ss">* </span>$Y$ is the response variable, the mean Sørensen dissimilarity,</span>
<span id="cb55-129"><a href="#cb55-129"></a><span class="ss">* </span>the predictors $X_1$, $X_2$, $X_3$, $X_4$, and $X_5$ correspond to <span class="in">`augMean`</span>, <span class="in">`febRange`</span>, <span class="in">`febSD`</span>, <span class="in">`augSD`</span>, and <span class="in">`annMean`</span>, respectively, and </span>
<span id="cb55-130"><a href="#cb55-130"></a><span class="ss">* </span>$\epsilon$ is the error term. </span>
<span id="cb55-131"><a href="#cb55-131"></a></span>
<span id="cb55-132"><a href="#cb55-132"></a>But before we jump into multiple linear regression, let's warm up by first fitting some simple linear regressions.</span>
<span id="cb55-133"><a href="#cb55-133"></a></span>
<span id="cb55-134"><a href="#cb55-134"></a><span class="fu">### Simple Linear Models</span></span>
<span id="cb55-135"><a href="#cb55-135"></a></span>
<span id="cb55-136"><a href="#cb55-136"></a>For interest sake, let's fit simple linear models for each of the predictors against the response variable. Let's look at relationships between the continuous predictors and the response in the East Coast Transition Zone (<span class="in">`ECTZ`</span>), ignoring the other bioregions for now. We will first fit the simple linear models and then create scatter plots of the response variable $\beta_\text{sør}$ against each of the predictor variables. To these plots, we will add a best fit (regression) lines.</span>
<span id="cb55-137"><a href="#cb55-137"></a></span>
<span id="cb55-140"><a href="#cb55-140"></a><span class="in">```{r}</span></span>
<span id="cb55-141"><a href="#cb55-141"></a>sw_ectz <span class="ot">&lt;-</span> sw <span class="sc">|&gt;</span> <span class="fu">filter</span>(bio <span class="sc">==</span> <span class="st">"ECTZ"</span>)</span>
<span id="cb55-142"><a href="#cb55-142"></a></span>
<span id="cb55-143"><a href="#cb55-143"></a>predictors <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"augMean"</span>, <span class="st">"febRange"</span>, <span class="st">"febSD"</span>, <span class="st">"augSD"</span>, <span class="st">"annMean"</span>)</span>
<span id="cb55-144"><a href="#cb55-144"></a></span>
<span id="cb55-145"><a href="#cb55-145"></a><span class="co"># Fit models using purrr::map and store in a list</span></span>
<span id="cb55-146"><a href="#cb55-146"></a>models <span class="ot">&lt;-</span> <span class="fu">map</span>(predictors, <span class="sc">~</span> <span class="fu">lm</span>(<span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">"Y ~"</span>, .x)),</span>
<span id="cb55-147"><a href="#cb55-147"></a>                               <span class="at">data =</span> sw_ectz))</span>
<span id="cb55-148"><a href="#cb55-148"></a></span>
<span id="cb55-149"><a href="#cb55-149"></a><span class="fu">names</span>(models) <span class="ot">&lt;-</span> predictors</span>
<span id="cb55-150"><a href="#cb55-150"></a></span>
<span id="cb55-151"><a href="#cb55-151"></a>model_summaries <span class="ot">&lt;-</span> <span class="fu">map</span>(models, summary)</span>
<span id="cb55-152"><a href="#cb55-152"></a>model_summaries</span>
<span id="cb55-153"><a href="#cb55-153"></a><span class="in">```</span></span>
<span id="cb55-154"><a href="#cb55-154"></a></span>
<span id="cb55-155"><a href="#cb55-155"></a>The individual models show that, for each predictor, the estimate of the coefficients (for slope) and the test for the overall hypothesis are both significant ($p &lt; 0.05$ in all cases; refer to the model output). All the predictor variables are therefore good predictors of the structure of seaweed species composition along.</span>
<span id="cb55-156"><a href="#cb55-156"></a></span>
<span id="cb55-159"><a href="#cb55-159"></a><span class="in">```{r}</span></span>
<span id="cb55-160"><a href="#cb55-160"></a><span class="co">#| fig-width: 10</span></span>
<span id="cb55-161"><a href="#cb55-161"></a><span class="co">#| fig-height: 7.5</span></span>
<span id="cb55-162"><a href="#cb55-162"></a><span class="co">#| fig.cap: "Individual simple linear regressions fitted to the variables `augMean`, `febRange`, `febSD`, `augSD`, and `annMean` as predictors of the seaweed species composition as measured by the Sørensen dissimilarity, `Y`."</span></span>
<span id="cb55-163"><a href="#cb55-163"></a><span class="co">#| label: fig-slr1</span></span>
<span id="cb55-164"><a href="#cb55-164"></a><span class="co"># Create individual plots for each predictor</span></span>
<span id="cb55-165"><a href="#cb55-165"></a>plts1 <span class="ot">&lt;-</span> <span class="fu">map</span>(predictors, <span class="cf">function</span>(predictor) {</span>
<span id="cb55-166"><a href="#cb55-166"></a>  <span class="fu">ggplot</span>(sw_ectz, <span class="fu">aes_string</span>(<span class="at">x =</span> predictor, <span class="at">y =</span> <span class="st">"Y"</span>)) <span class="sc">+</span></span>
<span id="cb55-167"><a href="#cb55-167"></a>    <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">1</span>, <span class="at">colour =</span> <span class="st">"dodgerblue4"</span>) <span class="sc">+</span></span>
<span id="cb55-168"><a href="#cb55-168"></a>    <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">col =</span> <span class="st">"magenta"</span>, <span class="at">fill =</span> <span class="st">"pink"</span>) <span class="sc">+</span></span>
<span id="cb55-169"><a href="#cb55-169"></a>    <span class="fu">labs</span>(<span class="at">title =</span> <span class="fu">paste</span>(<span class="st">"Y vs"</span>, predictor),</span>
<span id="cb55-170"><a href="#cb55-170"></a>         <span class="at">x =</span> predictor,</span>
<span id="cb55-171"><a href="#cb55-171"></a>         <span class="at">y =</span> <span class="st">"Y"</span>) <span class="sc">+</span></span>
<span id="cb55-172"><a href="#cb55-172"></a>    <span class="fu">theme_bw</span>()</span>
<span id="cb55-173"><a href="#cb55-173"></a>})</span>
<span id="cb55-174"><a href="#cb55-174"></a></span>
<span id="cb55-175"><a href="#cb55-175"></a><span class="co"># Name the list elements for easy reference</span></span>
<span id="cb55-176"><a href="#cb55-176"></a><span class="fu">names</span>(plts1) <span class="ot">&lt;-</span> predictors</span>
<span id="cb55-177"><a href="#cb55-177"></a></span>
<span id="cb55-178"><a href="#cb55-178"></a>ggpubr<span class="sc">::</span><span class="fu">ggarrange</span>(<span class="at">plotlist =</span> plts1, <span class="at">ncol =</span> <span class="dv">2</span>,</span>
<span id="cb55-179"><a href="#cb55-179"></a>                  <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">labels =</span> <span class="st">"AUTO"</span>)</span>
<span id="cb55-180"><a href="#cb55-180"></a><span class="in">```</span></span>
<span id="cb55-181"><a href="#cb55-181"></a></span>
<span id="cb55-182"><a href="#cb55-182"></a>@fig-slr1 is a series of scatter plots showing the relationship between the response variable $\beta_\text{sør}$ and each of the predictor variables. The blue line represents the linear regression fitted to the data. We see that the relationship between the response variable and each of the predictors is positive and linear. Each of the models are significant, as indicated by the $p$-values in the model summaries. These simple models do not tell us how some predictors might act together to influence the response variable.</span>
<span id="cb55-183"><a href="#cb55-183"></a></span>
<span id="cb55-184"><a href="#cb55-184"></a>To consider combined effects and interactions between predictor variables, we must explore multiple linear regression models that include all the predictors. Multiple regression will give us a more integrated understanding of how various environmental variables jointly influence species composition along the coast. In doing so, we can control for confounding variables, improve model fit, deal with multicollinearity, test for interaction effects, and enhance predictive power.</span>
<span id="cb55-185"><a href="#cb55-185"></a></span>
<span id="cb55-186"><a href="#cb55-186"></a>We will fit this multiple regression model next. </span>
<span id="cb55-187"><a href="#cb55-187"></a></span>
<span id="cb55-188"><a href="#cb55-188"></a><span class="fu">### State the Hypotheses for a Multiple Linear Regression</span></span>
<span id="cb55-189"><a href="#cb55-189"></a></span>
<span id="cb55-190"><a href="#cb55-190"></a>As with all inferential statistics, we need to consider the hypotheses when performing multiple linear regression.</span>
<span id="cb55-191"><a href="#cb55-191"></a></span>
<span id="cb55-192"><a href="#cb55-192"></a>The null hypothesis ($H_0$) states that there is no significant relationship between the Sørensen diversity index and any of the the climatological variables entered into the model, implying that the coefficients for all predictors are equal to zero. The alternative hypothesis ($H_A$), on the other hand, states that there is a significant relationship between the Sørensen diversity index and the climatological variables, positing that at least one of the coefficients is not equal to zero.</span>
<span id="cb55-193"><a href="#cb55-193"></a></span>
<span id="cb55-194"><a href="#cb55-194"></a>The hypotheses can be divided into two kinds: those dealing with the main effects and the one assessing the overall model stated in @eq-full.</span>
<span id="cb55-195"><a href="#cb55-195"></a></span>
<span id="cb55-196"><a href="#cb55-196"></a>**Main effects hypotheses**</span>
<span id="cb55-197"><a href="#cb55-197"></a></span>
<span id="cb55-198"><a href="#cb55-198"></a>The main effects hypotheses test, for each predictor, $X_i$, if the predictor has a significant effect on the response variable $Y$. </span>
<span id="cb55-199"><a href="#cb55-199"></a></span>
<span id="cb55-200"><a href="#cb55-200"></a>$H_0$: There is no linear relationship between the environmental variables (<span class="in">`augMean`</span>, <span class="in">`febRange`</span>, <span class="in">`febSD`</span>, <span class="in">`augSD`</span>, and <span class="in">`annMean`</span>) and the community composition as measured by $\beta_\text{sør}$ (in <span class="in">`Y`</span>). Formally, for each predictor variable $X_i$:</span>
<span id="cb55-201"><a href="#cb55-201"></a></span>
<span id="cb55-202"><a href="#cb55-202"></a><span class="ss">* </span>$H_0: \beta_i = 0 \text{ for } i = 1, 2, 3, 4, 5$</span>
<span id="cb55-203"><a href="#cb55-203"></a></span>
<span id="cb55-204"><a href="#cb55-204"></a>Where $\beta_i$ are the coefficients of the predictors in the multiple linear regression model.</span>
<span id="cb55-205"><a href="#cb55-205"></a></span>
<span id="cb55-206"><a href="#cb55-206"></a>$H_A$: There is a linear relationship between the environmental variables (<span class="in">`augMean`</span>, <span class="in">`febRange`</span>, <span class="in">`febSD`</span>, <span class="in">`augSD`</span>, and <span class="in">`annMean`</span>) and the species composition as measured by $\beta_\text{sør}$:</span>
<span id="cb55-207"><a href="#cb55-207"></a></span>
<span id="cb55-208"><a href="#cb55-208"></a><span class="ss">* </span>$H_A: \beta_i \neq 0 \text{ for } i = 1, 2, 3, 4, 5$</span>
<span id="cb55-209"><a href="#cb55-209"></a></span>
<span id="cb55-210"><a href="#cb55-210"></a>**Overall hypothesis**</span>
<span id="cb55-211"><a href="#cb55-211"></a></span>
<span id="cb55-212"><a href="#cb55-212"></a>In addition to testing the individual predictors, $X_i$, we can also test a hypothesis about the overall significance of the model (*F*-test), which examines whether the model as a whole explains a significant amount of variance in the response variable $Y$. A significant *F*-test would suggest that *at least one* predictor (excluding the intercept) in the model is likely to be significantly related to the response, but it requires further investigation of individual predictors and potential multicollinearity to fully understand the relationships. For the overall model hypothesis:</span>
<span id="cb55-213"><a href="#cb55-213"></a></span>
<span id="cb55-214"><a href="#cb55-214"></a>Null Hypothesis ($H_0$):</span>
<span id="cb55-215"><a href="#cb55-215"></a></span>
<span id="cb55-216"><a href="#cb55-216"></a><span class="ss">* </span>$H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = 0$</span>
<span id="cb55-217"><a href="#cb55-217"></a></span>
<span id="cb55-218"><a href="#cb55-218"></a>Alternative Hypothesis ($H_A$):</span>
<span id="cb55-219"><a href="#cb55-219"></a></span>
<span id="cb55-220"><a href="#cb55-220"></a><span class="ss">* </span>$H_A: \exists \, \beta_i \neq 0 \text{ for at least one } i$</span>
<span id="cb55-221"><a href="#cb55-221"></a></span>
<span id="cb55-222"><a href="#cb55-222"></a><span class="fu">### Fit the Model</span></span>
<span id="cb55-223"><a href="#cb55-223"></a></span>
<span id="cb55-224"><a href="#cb55-224"></a>We fit two models:</span>
<span id="cb55-225"><a href="#cb55-225"></a></span>
<span id="cb55-226"><a href="#cb55-226"></a><span class="ss">* </span>a full model that includes an intercept term and the five environmental variables, and</span>
<span id="cb55-227"><a href="#cb55-227"></a><span class="ss">* </span>a null model that includes only an intercept term.</span>
<span id="cb55-228"><a href="#cb55-228"></a></span>
<span id="cb55-229"><a href="#cb55-229"></a>The reason the null model is included is to compare the full model with a model that has no predictors. This comparison will help us determine which of the predictors are useful in explaining the response variable---we will see this in action in the forward model selection process later on (@sec-forward-selection).</span>
<span id="cb55-230"><a href="#cb55-230"></a></span>
<span id="cb55-233"><a href="#cb55-233"></a><span class="in">```{r}</span></span>
<span id="cb55-234"><a href="#cb55-234"></a><span class="co"># Select only the variables that will be used in model building</span></span>
<span id="cb55-235"><a href="#cb55-235"></a>sw_sub1 <span class="ot">&lt;-</span> sw_ectz[, <span class="fu">c</span>(<span class="st">"Y"</span>, <span class="st">"augMean"</span>, <span class="st">"febRange"</span>,</span>
<span id="cb55-236"><a href="#cb55-236"></a>                      <span class="st">"febSD"</span>, <span class="st">"augSD"</span>, <span class="st">"annMean"</span>)]</span>
<span id="cb55-237"><a href="#cb55-237"></a></span>
<span id="cb55-238"><a href="#cb55-238"></a><span class="co"># Fit the full and null models</span></span>
<span id="cb55-239"><a href="#cb55-239"></a>full_mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> augMean <span class="sc">+</span> febRange <span class="sc">+</span> febSD <span class="sc">+</span></span>
<span id="cb55-240"><a href="#cb55-240"></a>                 augSD <span class="sc">+</span> annMean, <span class="at">data =</span> sw_sub1)</span>
<span id="cb55-241"><a href="#cb55-241"></a>null_mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> sw_sub1)</span>
<span id="cb55-242"><a href="#cb55-242"></a></span>
<span id="cb55-243"><a href="#cb55-243"></a><span class="co"># Add fitted values from the full model to the dataframe</span></span>
<span id="cb55-244"><a href="#cb55-244"></a>sw_ectz<span class="sc">$</span>.fitted <span class="ot">&lt;-</span> <span class="fu">fitted</span>(full_mod1)</span>
<span id="cb55-245"><a href="#cb55-245"></a><span class="in">```</span></span>
<span id="cb55-246"><a href="#cb55-246"></a></span>
<span id="cb55-247"><a href="#cb55-247"></a><span class="fu">### Dealing With Multicollinearity {#sec-multicollinearity}</span></span>
<span id="cb55-248"><a href="#cb55-248"></a></span>
<span id="cb55-249"><a href="#cb55-249"></a>Some of the predictor variables may be correlated with each other and this can lead to multicollinearity. When predictor variables are highly correlated, the model may not be able to distinguish the individual effects of each predictor. Consequently, the model becomes less precise and harder to interpret due to the coefficients' inflated standard errors (@graham2003confronting). One can create a plot of pairwise correlations to visually inspect the correlation structure of the predictors. I'll not do this here, but you can try it on your own. </span>
<span id="cb55-250"><a href="#cb55-250"></a></span>
<span id="cb55-251"><a href="#cb55-251"></a>A formal way to detect multicollinearity is to calculate the variance inflation factor (VIF) for each predictor variable. The VIF measures how much the variance of the estimated regression coefficients is increased due to multicollinearity. A VIF value greater than 5 or 10 indicates a problematic amount of multicollinearity.</span>
<span id="cb55-252"><a href="#cb55-252"></a></span>
<span id="cb55-255"><a href="#cb55-255"></a><span class="in">```{r}</span></span>
<span id="cb55-256"><a href="#cb55-256"></a>initial_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="st">"Y ~ ."</span>)</span>
<span id="cb55-257"><a href="#cb55-257"></a></span>
<span id="cb55-258"><a href="#cb55-258"></a>threshold <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co"># Define a threshold for VIF values</span></span>
<span id="cb55-259"><a href="#cb55-259"></a></span>
<span id="cb55-260"><a href="#cb55-260"></a><span class="co"># Extract the names of the predictor variables</span></span>
<span id="cb55-261"><a href="#cb55-261"></a>predictors <span class="ot">&lt;-</span> <span class="fu">names</span>(<span class="fu">vif</span>(full_mod1))</span>
<span id="cb55-262"><a href="#cb55-262"></a></span>
<span id="cb55-263"><a href="#cb55-263"></a><span class="co"># Iteratively remove collinear variables</span></span>
<span id="cb55-264"><a href="#cb55-264"></a><span class="cf">while</span> (<span class="cn">TRUE</span>) {</span>
<span id="cb55-265"><a href="#cb55-265"></a>  <span class="co"># Calculate VIF values</span></span>
<span id="cb55-266"><a href="#cb55-266"></a>  vif_values <span class="ot">&lt;-</span> <span class="fu">vif</span>(full_mod1)</span>
<span id="cb55-267"><a href="#cb55-267"></a>  <span class="fu">print</span>(vif_values) <span class="co"># Print VIF values for debugging</span></span>
<span id="cb55-268"><a href="#cb55-268"></a>  max_vif <span class="ot">&lt;-</span> <span class="fu">max</span>(vif_values)</span>
<span id="cb55-269"><a href="#cb55-269"></a>  </span>
<span id="cb55-270"><a href="#cb55-270"></a>  <span class="co"># Check if the maximum VIF is above the threshold</span></span>
<span id="cb55-271"><a href="#cb55-271"></a>  <span class="cf">if</span> (max_vif <span class="sc">&gt;</span> threshold) {</span>
<span id="cb55-272"><a href="#cb55-272"></a>    <span class="co"># Find the variable with the highest VIF</span></span>
<span id="cb55-273"><a href="#cb55-273"></a>    high_vif_var <span class="ot">&lt;-</span> <span class="fu">names</span>(<span class="fu">which.max</span>(vif_values))</span>
<span id="cb55-274"><a href="#cb55-274"></a>    <span class="fu">cat</span>(<span class="st">"Removing variable:"</span>,</span>
<span id="cb55-275"><a href="#cb55-275"></a>        high_vif_var,</span>
<span id="cb55-276"><a href="#cb55-276"></a>        <span class="st">"with VIF:"</span>,</span>
<span id="cb55-277"><a href="#cb55-277"></a>        max_vif,</span>
<span id="cb55-278"><a href="#cb55-278"></a>        <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb55-279"><a href="#cb55-279"></a>    </span>
<span id="cb55-280"><a href="#cb55-280"></a>    <span class="co"># Update the formula to exclude the high VIF variable</span></span>
<span id="cb55-281"><a href="#cb55-281"></a>    updated_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">"Y ~ . -"</span>, high_vif_var))</span>
<span id="cb55-282"><a href="#cb55-282"></a>    </span>
<span id="cb55-283"><a href="#cb55-283"></a>    <span class="co"># Refit the model without the high VIF variable</span></span>
<span id="cb55-284"><a href="#cb55-284"></a>    full_mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(updated_formula, <span class="at">data =</span> sw_sub1)</span>
<span id="cb55-285"><a href="#cb55-285"></a>    </span>
<span id="cb55-286"><a href="#cb55-286"></a>    <span class="co"># Update the environment data frame to reflect the removal</span></span>
<span id="cb55-287"><a href="#cb55-287"></a>    sw_sub1 <span class="ot">&lt;-</span> sw_sub1[, <span class="sc">!</span>(<span class="fu">names</span>(sw_sub1) <span class="sc">%in%</span> high_vif_var)]</span>
<span id="cb55-288"><a href="#cb55-288"></a>  } <span class="cf">else</span> {</span>
<span id="cb55-289"><a href="#cb55-289"></a>    <span class="cf">break</span></span>
<span id="cb55-290"><a href="#cb55-290"></a>  }</span>
<span id="cb55-291"><a href="#cb55-291"></a>}</span>
<span id="cb55-292"><a href="#cb55-292"></a><span class="in">```</span></span>
<span id="cb55-293"><a href="#cb55-293"></a></span>
<span id="cb55-294"><a href="#cb55-294"></a>Regularisation techniques such as ridge regression, lasso regression, or elastic net can also be used to deal with multicollinearity. These advanced techniques add a penalty term to the regression model that shrinks the coefficients towards zero, which can help to reduce the impact of multicollinearity. However, these techniques are not covered in this guide. Please refer to @sec-regularisation for more information on regularisation techniques.</span>
<span id="cb55-295"><a href="#cb55-295"></a></span>
<span id="cb55-296"><a href="#cb55-296"></a><span class="fu">### Perform Forward Selection {#sec-forward-selection}</span></span>
<span id="cb55-297"><a href="#cb55-297"></a></span>
<span id="cb55-298"><a href="#cb55-298"></a>It might be that not all of the variables included in the full model are necessary to explain the response variable. We can use a stepwise regression to select the best combination (subset) of predictors that best explains the response variable. To do this, we will use the <span class="in">`stepAIC`</span> function that lives in the <span class="in">`MASS`</span> package.</span>
<span id="cb55-299"><a href="#cb55-299"></a></span>
<span id="cb55-300"><a href="#cb55-300"></a><span class="in">`stepAIC()`</span> works by starting with the null model and then adding predictors one by one, selecting the one that improves the model the most as seen in the reduction of the AIC values along the way. This process continues until no more predictors can be added to improve the model (i.e. to further reduce the AIC). Progress is tracked as the function runs.</span>
<span id="cb55-301"><a href="#cb55-301"></a></span>
<span id="cb55-304"><a href="#cb55-304"></a><span class="in">```{r}</span></span>
<span id="cb55-305"><a href="#cb55-305"></a><span class="co"># Perform forward selection</span></span>
<span id="cb55-306"><a href="#cb55-306"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">stepAIC</span>(null_mod1,</span>
<span id="cb55-307"><a href="#cb55-307"></a>                <span class="at">scope =</span> <span class="fu">list</span>(<span class="at">lower =</span> null_mod1, <span class="at">upper =</span> full_mod1),</span>
<span id="cb55-308"><a href="#cb55-308"></a>                <span class="at">direction =</span> <span class="st">"forward"</span>)</span>
<span id="cb55-309"><a href="#cb55-309"></a><span class="in">```</span></span>
<span id="cb55-310"><a href="#cb55-310"></a></span>
<span id="cb55-311"><a href="#cb55-311"></a>The model selection process shows that as we add more variables to the model, the AIC value decreases. We can infer from this that the multiple regression model provides a better fit that simple linear models that use the variables in isolation.</span>
<span id="cb55-312"><a href="#cb55-312"></a></span>
<span id="cb55-313"><a href="#cb55-313"></a>We also see that <span class="in">`stepAIC()`</span> has not removed any variables from the full model. Probably one reason for failing to remove any variables is that the VIF process has already accomplished this by virtue of dealing with multicollinearity. This means that all the variables retained in <span class="in">`mod1`</span> are important in explaining the response variable. </span>
<span id="cb55-314"><a href="#cb55-314"></a></span>
<span id="cb55-315"><a href="#cb55-315"></a><span class="fu">### Added-Variable Plots (Partial Regression Plots) {#sec-added-variable-plots}</span></span>
<span id="cb55-316"><a href="#cb55-316"></a></span>
<span id="cb55-317"><a href="#cb55-317"></a>Before looking at the output in more detail, I'll introduce partial regression plots as a means to examine the relationship between the response variable and each predictor variable. Although they can be calculated by hand, the **car** package provides a convenient function, <span class="in">`avPlots()`</span>, to create these plots.</span>
<span id="cb55-318"><a href="#cb55-318"></a></span>
<span id="cb55-319"><a href="#cb55-319"></a>Added variable plots are also sometimes called 'partial regression plots' or 'individual coefficient plots.' They are used to display the relationship between a response variable and an individual predictor variable while accounting for the effect of other predictor variables in a multiple regression model (the marginal effect).</span>
<span id="cb55-320"><a href="#cb55-320"></a></span>
<span id="cb55-323"><a href="#cb55-323"></a><span class="in">```{r}</span></span>
<span id="cb55-324"><a href="#cb55-324"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb55-325"><a href="#cb55-325"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb55-326"><a href="#cb55-326"></a><span class="co">#| fig.cap: "Partial regression plots for `mod1` with the selected variables `augMean`, `febSD`, and `augSD`."</span></span>
<span id="cb55-327"><a href="#cb55-327"></a><span class="co">#| label: fig-mod1-partial</span></span>
<span id="cb55-328"><a href="#cb55-328"></a></span>
<span id="cb55-329"><a href="#cb55-329"></a><span class="co"># Create partial regression plots</span></span>
<span id="cb55-330"><a href="#cb55-330"></a><span class="fu">avPlots</span>(mod1, <span class="at">col =</span> <span class="st">"dodgerblue4"</span>, <span class="at">col.lines =</span> <span class="st">"magenta"</span>)</span>
<span id="cb55-331"><a href="#cb55-331"></a><span class="in">```</span></span>
<span id="cb55-332"><a href="#cb55-332"></a></span>
<span id="cb55-333"><a href="#cb55-333"></a>What insights can we draw from the added-variable plots? Although there are better ways to assess the model fit, we can already make some observations about the linearity of the model or the presence of outliers. The slope of the line in an added variable plot corresponds to the regression coefficient for that predictor in the full multiple regression model. Seen in this way, it visually indicates the magnitude and direction of each predictor's effect. In @fig-mod1-partial, the added-variable plot for <span class="in">`augMean`</span> shows a tighter clustering of points around the regression line and a strong linear relationship (steep slope) with the response variable; the plots for <span class="in">`febSD`</span> and <span class="in">`augSD`</span>, on the other hand, show a weaker response and more scatter about the regression line. Importantly, this suggests that <span class="in">`augMean`</span> has a stronger and more unique contribution to the multiple-variable model than the other two variables.</span>
<span id="cb55-334"><a href="#cb55-334"></a></span>
<span id="cb55-335"><a href="#cb55-335"></a>There are also insights to be made about possible multicollinearity using added-variable plots. These plots are not a definitive test for multicollinearity, but they can provide some clues. Notably, if a predictor shows a strong relationship with the response variable in a simple correlation but appears to have little relationship in the added-variable plot, it might indicate collinearity with other predictors. This discrepancy suggests that the predictor's effect on the response is being masked by the presence of other correlated predictors.</span>
<span id="cb55-336"><a href="#cb55-336"></a></span>
<span id="cb55-337"><a href="#cb55-337"></a><span class="fu">### Model Diagnostics {#sec-diagnostics}</span></span>
<span id="cb55-338"><a href="#cb55-338"></a></span>
<span id="cb55-339"><a href="#cb55-339"></a>We are back in the territory of parametric statistics, so we need to check the assumptions of the multiple linear regression model (similar to those of simple linear regression). We can do this by making the various diagnostic plots. all of them consider various aspects of the residuals, which are simply the differences between the observed and predicted values. </span>
<span id="cb55-340"><a href="#cb55-340"></a></span>
<span id="cb55-341"><a href="#cb55-341"></a>**Diagnostic plots of final model**</span>
<span id="cb55-342"><a href="#cb55-342"></a></span>
<span id="cb55-343"><a href="#cb55-343"></a>You have been introduced to diagnostic plots in the context of simple linear regression (@sec-simple-linear-regression). They are also useful in multiple linear regression. Although <span class="in">`plot.lm()`</span> can easily do this, here I use <span class="in">`autoplot()`</span> from the **ggfortify** package. When applied to the final model, <span class="in">`mod1`</span>, the plot will in its default setting show four diagnostic plots: residuals vs. fitted values, normal Q-Q plot, scale-location plot, and residuals vs. leverage plot. Note, this is for the full model inclusive of the combined contributions of all the predictors, so we will not see separate plots for each predictor as we have seen in the added-variable plots or component plus residual plots.</span>
<span id="cb55-344"><a href="#cb55-344"></a></span>
<span id="cb55-347"><a href="#cb55-347"></a><span class="in">```{r}</span></span>
<span id="cb55-348"><a href="#cb55-348"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb55-349"><a href="#cb55-349"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb55-350"><a href="#cb55-350"></a><span class="co">#| fig.cap: "Diagnostic plots to assess the fit of the final multiple linear regression model, `mod1`."</span></span>
<span id="cb55-351"><a href="#cb55-351"></a><span class="co">#| label: fig-mlr3</span></span>
<span id="cb55-352"><a href="#cb55-352"></a></span>
<span id="cb55-353"><a href="#cb55-353"></a><span class="co"># Generate diagnostic plots </span></span>
<span id="cb55-354"><a href="#cb55-354"></a><span class="fu">autoplot</span>(mod1, <span class="at">shape =</span> <span class="dv">21</span>, <span class="at">colour =</span> <span class="st">"dodgerblue4"</span>,</span>
<span id="cb55-355"><a href="#cb55-355"></a>         <span class="at">smooth.colour =</span> <span class="st">"magenta"</span>) <span class="sc">+</span></span>
<span id="cb55-356"><a href="#cb55-356"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb55-357"><a href="#cb55-357"></a><span class="in">```</span></span>
<span id="cb55-358"><a href="#cb55-358"></a></span>
<span id="cb55-359"><a href="#cb55-359"></a>**Residuals vs. Fitted Values:** In this plot we can assess linearity and homoscedasticity of the residuals. If the seaweed gods were with us, we'd expect the points to be randomly scattered about a horizontal line situation at zero. This would indicate that the relationship between the predictors selected by the forward selection process (<span class="in">`augMean`</span>, <span class="in">`febSD`</span>, and <span class="in">`augSD`</span>) and the response variable (<span class="in">`Y`</span>) is linear, and the variance of the residuals is constant across the range of fitted values. In this plot, there's a very slight curvature which might suggest a potential issue with the linearity assumption---it is minute and I'd suggest not worrying about it. The variance of the residuals seems to decrease slightly at higher fitted values, indicating a mild case of heteroscedasticity.</span>
<span id="cb55-360"><a href="#cb55-360"></a></span>
<span id="cb55-361"><a href="#cb55-361"></a>**Q-Q Plot (Quantile-Quantile Plot):** This plot is used to check the normality of the residuals. The points should fall approximately along a straight diagonal line if the residuals are normally distributed. Here we see that the points generally follow the line although some deviations may be seen at the tails. These deviations are not that extreme and again I don't think this is not a big concern.</span>
<span id="cb55-362"><a href="#cb55-362"></a></span>
<span id="cb55-363"><a href="#cb55-363"></a>**Scale-Location Plot:** This plot should reveal potential issues with homoscedasticity. The square root of the standardised residuals is used here to make it easier to spot patterns, so we would like the points to be randomly scattered around the horizontal red line.  Here, the line slopes slightly downward and this indicates that the variance of the residuals might decrease as the fitted values increase. We can also see evidence of this in a plot of the observed values vs. the predictors in @fig-mlr3.</span>
<span id="cb55-364"><a href="#cb55-364"></a></span>
<span id="cb55-365"><a href="#cb55-365"></a>**Residuals vs. Leverage:** This diagnostic highlights influential points (outliers). Points with high leverage (far from the mean of the predictors) can be expected to exert a strong influence on the regression line, tilting it in some direction. Cook's distance (indicated by the yellow line) helps identify such outliers. In our seaweed data a few points could have a high leverage, but since they don't seem to cross the Cook's distance thresholds, I doubt they are overly worrisome.</span>
<span id="cb55-366"><a href="#cb55-366"></a></span>
<span id="cb55-367"><a href="#cb55-367"></a>Considering that no glaring red flags were raised by the diagnostic plots, I doubt that they are severe enough to invalidate the model. However, if you cannot stand these small issues, you could i) consider transforming the predictor or response variables to address your concerns about heteroscedasticity, ii) investigate the outliers (high leverage points) to confirm if they are valid data points or errors, or iii) try robust regression methods that are less sensitive to outliers and heteroscedasticity.</span>
<span id="cb55-368"><a href="#cb55-368"></a></span>
<span id="cb55-369"><a href="#cb55-369"></a>**Component plus residual plots**</span>
<span id="cb55-370"><a href="#cb55-370"></a></span>
<span id="cb55-371"><a href="#cb55-371"></a>Component plus residual plots offer another way to assess the fit of the model in multiple regression models. Unlike simple linear regression where we only had one predictor variable, here we have several. So, we need to assure ourselves that there is a linear relationship between each predictor variable and the response variable (we could already see this in the added-variable plots in @sec-added-variable-plots). We can make component plus residual plots using the <span class="in">`crPlots()`</span> function in the **car** package. It displays the relationship between the response variable and each predictor variable. If the relationship is linear, the points should be randomly scattered about a best fit line and the spline (in pink in @fig-mod1-crplots) should plot nearly on top of the linear regression line. </span>
<span id="cb55-372"><a href="#cb55-372"></a></span>
<span id="cb55-375"><a href="#cb55-375"></a><span class="in">```{r}</span></span>
<span id="cb55-376"><a href="#cb55-376"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb55-377"><a href="#cb55-377"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb55-378"><a href="#cb55-378"></a><span class="co">#| fig.cap: "Component plus residual diagnostic plots to assess the fit of the final multiple linear regression model, `mod1`."</span></span>
<span id="cb55-379"><a href="#cb55-379"></a><span class="co">#| label: fig-mod1-crplots</span></span>
<span id="cb55-380"><a href="#cb55-380"></a></span>
<span id="cb55-381"><a href="#cb55-381"></a><span class="co"># Generate component plus residual plots</span></span>
<span id="cb55-382"><a href="#cb55-382"></a><span class="fu">crPlots</span>(mod1, <span class="at">col =</span> <span class="st">"dodgerblue4"</span>, <span class="at">col.lines =</span> <span class="st">"magenta"</span>)</span>
<span id="cb55-383"><a href="#cb55-383"></a><span class="in">```</span></span>
<span id="cb55-384"><a href="#cb55-384"></a></span>
<span id="cb55-385"><a href="#cb55-385"></a><span class="fu">### Understanding the Model Fit {#sec-mod1-summary}</span></span>
<span id="cb55-386"><a href="#cb55-386"></a></span>
<span id="cb55-387"><a href="#cb55-387"></a>The above model selection process has led us to the <span class="in">`mod1`</span> model, which can be stated formally as:</span>
<span id="cb55-388"><a href="#cb55-388"></a>$$Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \epsilon$$ {#eq-mod1}</span>
<span id="cb55-389"><a href="#cb55-389"></a>Where:</span>
<span id="cb55-390"><a href="#cb55-390"></a></span>
<span id="cb55-391"><a href="#cb55-391"></a><span class="ss">* </span>$Y$: The response variable, the mean Sørensen dissimilarity.</span>
<span id="cb55-392"><a href="#cb55-392"></a><span class="ss">* </span>$X_1$, $X_2$, and $X_3$: The predictors corresponding to <span class="in">`augMean`</span>, <span class="in">`febSD`</span>, and <span class="in">`augSD`</span>, respectively.</span>
<span id="cb55-393"><a href="#cb55-393"></a><span class="ss">* </span>$\epsilon$: The error term. </span>
<span id="cb55-394"><a href="#cb55-394"></a></span>
<span id="cb55-395"><a href="#cb55-395"></a>We have convinced ourselves that the model is a good fit for the data, and we can proceed to examine the model's output. The fitted model can be explored in two ways: by applying the <span class="in">`summary()`</span> function or by using the <span class="in">`anova()`</span> function. The <span class="in">`summary()`</span> function provides a detailed output of the model, while the <span class="in">`anova()`</span> function provides a table of deviance values that can be used to compare models.</span>
<span id="cb55-396"><a href="#cb55-396"></a></span>
<span id="cb55-397"><a href="#cb55-397"></a>**The model summary** </span>
<span id="cb55-398"><a href="#cb55-398"></a></span>
<span id="cb55-401"><a href="#cb55-401"></a><span class="in">```{r}</span></span>
<span id="cb55-402"><a href="#cb55-402"></a><span class="co"># Summary of the selected model</span></span>
<span id="cb55-403"><a href="#cb55-403"></a><span class="fu">summary</span>(mod1)</span>
<span id="cb55-404"><a href="#cb55-404"></a><span class="in">```</span></span>
<span id="cb55-405"><a href="#cb55-405"></a></span>
<span id="cb55-406"><a href="#cb55-406"></a>The first part of the <span class="in">`summary()`</span> function's output is the <span class="in">`Coefficients`</span> section. This is where the main effects hypotheses are tested (this model does not have interactions---if there were, they'd appear here, too). The important components of the coefficients part of the model summary are:</span>
<span id="cb55-407"><a href="#cb55-407"></a></span>
<span id="cb55-408"><a href="#cb55-408"></a><span class="ss">- </span><span class="in">`(Intercept)`</span>: This row provides information about where the regression line intersects the *y*-axis. </span>
<span id="cb55-409"><a href="#cb55-409"></a><span class="ss">- </span>Main Effects:</span>
<span id="cb55-410"><a href="#cb55-410"></a><span class="ss">    - </span><span class="in">`augMean`</span>, <span class="in">`febSD`</span>, and <span class="in">`augSD`</span>: These rows give the model coefficients associated with the slopes of the regression lines fit to those predictor variables. They indicate the rate of change in the response variable for a one-unit change in the predictor variable. </span>
<span id="cb55-411"><a href="#cb55-411"></a><span class="ss">    - </span><span class="in">`Estimate`</span>, <span class="in">`Std. Error`</span>, <span class="in">`t value`</span>, and <span class="in">`Pr(&gt;|t|)`</span>: These columns contain the statistics used to interpret the hypotheses about the main effects. In the <span class="in">`Estimate`</span> column are the coefficients for the *y*-intercept and the main effects' slopes, and `Std. Error` indicates the variability of the estimate. The `t value` is obtained by dividing the coefficient by its standard error. The *p*-value tests the null hypothesis that the coefficient is equal to zero and significance codes are provided as a quick visual reference (their use is sometimes frowned upon by statistics purists). Using this information, we can quickly see that, for example, <span class="in">`augMean`</span> has a coefficient of $0.2833 \pm 0.0111$ and the slope of the line is highly significant, i.e. there is a significant effect of <span class="in">`Y`</span> due to the temperature gradient set up by <span class="in">`augMean`</span>. </span>
<span id="cb55-412"><a href="#cb55-412"></a></span>
<span id="cb55-413"><a href="#cb55-413"></a>::: callout-note</span>
<span id="cb55-414"><a href="#cb55-414"></a><span class="fu">## The intercept and slope coefficients</span></span>
<span id="cb55-415"><a href="#cb55-415"></a></span>
<span id="cb55-416"><a href="#cb55-416"></a>The interpretation of the coefficients is a bit more complicated in multiple linear regression compared to what we are accustomed to in simple linear regression. Let us look at some greater detail at the intercept and the slope coefficients:</span>
<span id="cb55-417"><a href="#cb55-417"></a></span>
<span id="cb55-418"><a href="#cb55-418"></a>Intercept ($\alpha$): ) The intercept is the expected value of the response variable, $Y$, when all predictor variables are zero. It is not always meaningful, but it can be useful in some cases. </span>
<span id="cb55-419"><a href="#cb55-419"></a></span>
<span id="cb55-420"><a href="#cb55-420"></a>Slope Coefficients ($\beta_1, \beta_2, \ldots, \beta_k$): Each slope coefficient, $\beta_j$, represents the expected change in the response variable, $Y$, for a one-unit increase in the predictor variable, $X_j$, holding all other predictor variables constant. This partial effect interpretation implies that $\beta_j$ accounts for the direct contribution of $X_j$ to $Y$ while removing the confounding effects of other predictors in the model. @fig-mod1-partial provides a visual representation of this concept and isolates the effect of each predictor variable on the response variable.</span>
<span id="cb55-421"><a href="#cb55-421"></a></span>
<span id="cb55-422"><a href="#cb55-422"></a>Therefore, in the context of our model (@eq-mod1) for this analysis, the partial interpretation is as follows:</span>
<span id="cb55-423"><a href="#cb55-423"></a></span>
<span id="cb55-424"><a href="#cb55-424"></a><span class="ss">* </span>$\beta_1$: Represents the change in $Y$ for a one-unit increase in $X_1$, holding $X_2$ and $X_3$ constant.</span>
<span id="cb55-425"><a href="#cb55-425"></a><span class="ss">* </span>$\beta_2$: Represents the change in $Y$ for a one-unit increase in $X_2$, holding $X_1$ and $X_3$ constant.</span>
<span id="cb55-426"><a href="#cb55-426"></a><span class="ss">* </span>$\beta_3$: Represents the change in $Y$ for a one-unit increase in $X_3$, holding $X_1$ and $X_2$ constant.</span>
<span id="cb55-427"><a href="#cb55-427"></a>:::</span>
<span id="cb55-428"><a href="#cb55-428"></a></span>
<span id="cb55-429"><a href="#cb55-429"></a>There are also several overall model fit statistics---it is here where you'll find the information you need to assess the hypothesis about the overall significance of the model. <span class="in">`Residual standard error`</span> indicates the average distance between observed and fitted values. <span class="in">`Multiple R-squared`</span> and <span class="in">`Adjusted R-squared`</span> values tell us something about the model's goodness of fit. The latter adjusts for the number of predictors in the model, and is the one you must use and report in multiple linear regressions. As you also know, higher numbers approaching 1 are better, with 1 suggesting that the model perfectly captures all of the variability in the data. The <span class="in">`F-statistic`</span> and its associated *p*-value test the overall significance of the model and examines whether all regression coefficients are simultaneously equal to zero. You can also use the brief overview of the residuals, but I don't find this particularly helpful---best examine the residuals in a histogram.</span>
<span id="cb55-430"><a href="#cb55-430"></a></span>
<span id="cb55-431"><a href="#cb55-431"></a>**The ANOVA tables**</span>
<span id="cb55-432"><a href="#cb55-432"></a></span>
<span id="cb55-435"><a href="#cb55-435"></a><span class="in">```{r}</span></span>
<span id="cb55-436"><a href="#cb55-436"></a><span class="fu">anova</span>(mod1)</span>
<span id="cb55-437"><a href="#cb55-437"></a><span class="in">```</span></span>
<span id="cb55-438"><a href="#cb55-438"></a></span>
<span id="cb55-439"><a href="#cb55-439"></a>This function provides a sequential analysis of variance (Type I ANOVA) table for the regression model (see more about Type I ANOVA, below). As such, this function can also be used to compare nested models. Used on a single model, it gives a more interpretable breakdown of the variability in the response variable <span class="in">`Y`</span> and assesses the contribution of each predictor variable in explaining this variability. </span>
<span id="cb55-440"><a href="#cb55-440"></a></span>
<span id="cb55-441"><a href="#cb55-441"></a>The ANOVA table firstly shows the degrees of freedom (<span class="in">`Df`</span>) for each predictor variable added sequentially to the model, as well as the residuals. For each predictor, the degrees of freedom is typically 1. For the residuals, however, it represents the total number of observations minus the number of estimated parameters. The Sum of Squares (<span class="in">`Sum Sq`</span>) indicates the variability in <span class="in">`Y`</span> attributable to each predictor, and the mean sum of squares (<span class="in">`Mean Sq`</span>) is the sum of squares divided by the degrees of freedom. </span>
<span id="cb55-442"><a href="#cb55-442"></a></span>
<span id="cb55-443"><a href="#cb55-443"></a>The <span class="in">`F value`</span> is calculated as the ratio of the predictor's mean square to the residual mean square tests. It is used in testing the null hypothesis that the predictor has no effect on <span class="in">`Y`</span>. Whether or not we accept the alternative hypothesis (reject the null) is given by the *p*-value (`Pr(&gt;F)`) that goes with each *F*-statistic. You know how that works.</span>
<span id="cb55-444"><a href="#cb55-444"></a></span>
<span id="cb55-445"><a href="#cb55-445"></a>Because this is a sequential ANOVA, the amount of variance in <span class="in">`Y`</span> explained by each predictor (or group of predictors) is calculated by adding the predictors to the model in sequence (as specified in the model formula). For example, the Sum of Squares for <span class="in">`augMean`</span> (6.0084) represents the amount of variance explained by adding <span class="in">`augMean`</span> to a model that doesn't include any predictors yet. The Sum of Squares for <span class="in">`febSD`</span> 0.3604) represents the amount of variance explained by adding <span class="in">`febSD`</span> to a model that already includes <span class="in">`augMean`</span>---this improvement indicates that <span class="in">`febSD`</span> explains some of the variance in <span class="in">`Y`</span> that <span class="in">`augMean`</span> doesn't. </span>
<span id="cb55-446"><a href="#cb55-446"></a></span>
<span id="cb55-447"><a href="#cb55-447"></a>::: callout-note</span>
<span id="cb55-448"><a href="#cb55-448"></a><span class="fu">## Order in which predictors are assessed in multiple linear regression</span></span>
<span id="cb55-449"><a href="#cb55-449"></a></span>
<span id="cb55-450"><a href="#cb55-450"></a>The interpretation of sequential ANOVA (Type I) is inherently dependent on the order in which predictors are entered. In <span class="in">`mod1`</span> the order is first <span class="in">`augMean`</span>, then <span class="in">`febSD`</span>, and last comes <span class="in">`augSD`</span>. This order might not be the most meaningful for interpreting the sequential sums of squares and their significance in the ANOVA table. How, then, does one decide on the order of predictors in the model?</span>
<span id="cb55-451"><a href="#cb55-451"></a></span>
<span id="cb55-452"><a href="#cb55-452"></a><span class="ss">* </span>If you have a strong theoretical or causal basis for thinking that certain predictors influence others, you can enter them in that order.</span>
<span id="cb55-453"><a href="#cb55-453"></a><span class="ss">* </span>If you have a hierarchy of predictors based on their importance or general vs. specific nature, you can enter them hierarchically.</span>
<span id="cb55-454"><a href="#cb55-454"></a><span class="ss">* </span>You can manually fit models with different predictor orders and compare the ANOVA tables to see how the results change. This can be time-consuming but might offer insights into the sensitivity of your conclusions to the order of entry.</span>
<span id="cb55-455"><a href="#cb55-455"></a><span class="ss">* </span>You can use automated model selection procedures, such as stepwise regression, to determine the best order of predictors. This is a more objective approach but can be criticised for being data-driven and not theory-driven.</span>
<span id="cb55-456"><a href="#cb55-456"></a><span class="ss">* </span>Use Type II or Type III ANOVAs, which are are not order-dependent and can be used to assess the significance of predictors after accounting for all other predictors in the model. However, they have their own limitations and assumptions that need to be considered.</span>
<span id="cb55-457"><a href="#cb55-457"></a></span>
<span id="cb55-458"><a href="#cb55-458"></a>My advice would be to have sound theoretical reasons for the order of predictors in the model.</span>
<span id="cb55-459"><a href="#cb55-459"></a>:::</span>
<span id="cb55-460"><a href="#cb55-460"></a></span>
<span id="cb55-461"><a href="#cb55-461"></a>Both ways of looking at the model fit of <span class="in">`mod1`</span>---<span class="in">`summary()`</span> and <span class="in">`anova()`</span>---show that forward selection retained the variables <span class="in">`augMean`</span>, <span class="in">`febSD`</span>, and <span class="in">`augSD`</span>. These three predictors should be used together to explain the response, <span class="in">`Y`</span>. </span>
<span id="cb55-462"><a href="#cb55-462"></a></span>
<span id="cb55-463"><a href="#cb55-463"></a>Let's make a plot of the full model with all the initial predictors and the selected model with the predictors chosen by the forward selection process.</span>
<span id="cb55-464"><a href="#cb55-464"></a></span>
<span id="cb55-467"><a href="#cb55-467"></a><span class="in">```{r}</span></span>
<span id="cb55-468"><a href="#cb55-468"></a><span class="co">#| fig-width: 3.5</span></span>
<span id="cb55-469"><a href="#cb55-469"></a><span class="co">#| fig-height: 2.15</span></span>
<span id="cb55-470"><a href="#cb55-470"></a><span class="co">#| fig.cap: "Plot of observed vs. predicted value obtained from the final multiple linear regression model (`mod`) with the selected variables `augMean`, `febSD`, and `augSD` as predictors (black points), and the initial model with also `annMean` and `febRange` (red points)."</span></span>
<span id="cb55-471"><a href="#cb55-471"></a><span class="co">#| label: fig-mlr2</span></span>
<span id="cb55-472"><a href="#cb55-472"></a></span>
<span id="cb55-473"><a href="#cb55-473"></a><span class="co"># Add fitted values from the selected model to the dataframe</span></span>
<span id="cb55-474"><a href="#cb55-474"></a>sw_ectz<span class="sc">$</span>.fitted_selected <span class="ot">&lt;-</span> <span class="fu">fitted</span>(mod1)</span>
<span id="cb55-475"><a href="#cb55-475"></a></span>
<span id="cb55-476"><a href="#cb55-476"></a><span class="co"># Create the plot of observed vs fitted values for the selected model</span></span>
<span id="cb55-477"><a href="#cb55-477"></a><span class="fu">ggplot</span>(sw_ectz, <span class="fu">aes</span>(<span class="at">x =</span> .fitted_selected, <span class="at">y =</span> Y)) <span class="sc">+</span></span>
<span id="cb55-478"><a href="#cb55-478"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">1</span>, <span class="at">colour =</span> <span class="st">"black"</span>, <span class="at">alpha =</span> <span class="fl">1.0</span>) <span class="sc">+</span></span>
<span id="cb55-479"><a href="#cb55-479"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> .fitted), <span class="at">colour =</span> <span class="st">"red"</span>,</span>
<span id="cb55-480"><a href="#cb55-480"></a>             <span class="at">shape =</span> <span class="dv">1</span>, <span class="at">alpha =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb55-481"><a href="#cb55-481"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>,</span>
<span id="cb55-482"><a href="#cb55-482"></a>              <span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb55-483"><a href="#cb55-483"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Fitted Values"</span>,</span>
<span id="cb55-484"><a href="#cb55-484"></a>       <span class="at">y =</span> <span class="st">"Observed Values"</span>) <span class="sc">+</span></span>
<span id="cb55-485"><a href="#cb55-485"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb55-486"><a href="#cb55-486"></a><span class="in">```</span></span>
<span id="cb55-487"><a href="#cb55-487"></a></span>
<span id="cb55-488"><a href="#cb55-488"></a><span class="fu">### Reporting</span></span>
<span id="cb55-489"><a href="#cb55-489"></a></span>
<span id="cb55-490"><a href="#cb55-490"></a>A Results section should be written in a format sutable for inclusion in your report or publication. Present the results in a clear and concise manner, with tables and figures used to help substantiate your findings. The results should be interpreted in the context of the research question and the study design. The limitations of the analysis should also be discussed, along with any potential sources of bias or confounding. Here is an example.</span>
<span id="cb55-491"><a href="#cb55-491"></a></span>
<span id="cb55-492"><a href="#cb55-492"></a>**Results**</span>
<span id="cb55-493"><a href="#cb55-493"></a></span>
<span id="cb55-494"><a href="#cb55-494"></a>The model demonstrates a strong overall fit, as indicated by the high $R^2$ value of 0.839 and an adjusted $R^2$ of 0.837, suggesting that approximately 83.7% of the variance in the mean Sørensen dissimilarity is explained by the predictors <span class="in">`augMean`</span>, <span class="in">`febSD`</span>, and <span class="in">`augSD`</span>. All predictors in the model are statistically significant, with <span class="in">`augMean`</span> showing the strongest effect ($\beta_1 = 0.283$, $p &lt; 0.0001$) (@fig-mod1-partial). The predictors <span class="in">`febSD`</span> and <span class="in">`augSD`</span> also have significant positive relationships with the response variable ($\beta_2 = 0.050$, $p = 0.0001$; $\beta_3 = 0.022$, $p = 0.0001$). A sequential ANOVA further confirms the significance of each predictor variable in the model, with all *F*-values indicating that the inclusion of each predictor significantly improves the model fit ($p &lt; 0.0001$ in all cases). Our model therefore provides clear support for the mean temperatures in August, the standard deviation of temperatures in February, and the standard deviation of temperatures in August as strong predictors of the mean Sørensen dissimilarity, with each contributing uniquely to the explanation of variability in the response variable.</span>
<span id="cb55-495"><a href="#cb55-495"></a></span>
<span id="cb55-496"><a href="#cb55-496"></a><span class="fu">## Example 2: Interaction of Distance and Bioregion {#sec-mlr-interaction}</span></span>
<span id="cb55-497"><a href="#cb55-497"></a></span>
<span id="cb55-498"><a href="#cb55-498"></a>Our seaweed dataset includes two additional variables that we have not yet considered. These are the continuous variable <span class="in">`dist`</span> which represents the geographic distance between the seaweed samples taken along the coast of South Africa, and the categorical variable <span class="in">`bio`</span> which is the bioregional classification of the seaweed samples.</span>
<span id="cb55-499"><a href="#cb55-499"></a></span>
<span id="cb55-500"><a href="#cb55-500"></a>These two new variables lend themselves to a few interesting questions. For example:</span>
<span id="cb55-501"><a href="#cb55-501"></a></span>
<span id="cb55-502"><a href="#cb55-502"></a><span class="ss">1. </span>Is the geographic distance between samples related to the Sørensens dissimilarity of the seaweed flora?</span>
<span id="cb55-503"><a href="#cb55-503"></a><span class="ss">2. </span>Does the average Sørensens dissimilarity vary among the bioregions to which the samples belong?</span>
<span id="cb55-504"><a href="#cb55-504"></a><span class="ss">3. </span>Is the effect of geographic distance on the Sørensens dissimilarity different for each bioregion?</span>
<span id="cb55-505"><a href="#cb55-505"></a></span>
<span id="cb55-506"><a href="#cb55-506"></a>The most complex model is (3), the one that answers the question about whether the effect of <span class="in">`dist`</span> on the response variable $Y$ is different for each bioregion. Questions (1) and (2) are subsets of this more inclusive question. To fully answer these quesitons, let's first consider the full model, which includes an *interaction term* between the continuous predictor <span class="in">`dist`</span> and the categorical predictor <span class="in">`bio`</span>. When we finally test our model, we will also have to consider the simpler models that do not include the interaction term.</span>
<span id="cb55-507"><a href="#cb55-507"></a></span>
<span id="cb55-508"><a href="#cb55-508"></a>'Interaction' means that the effect of one predictor on the response variable is contingent on the value of another predictor. For example, we might have reason to suspect that the relationship of the Sørensens dissimilarity with the geographic distance between samples is different between the west coast compared to, say, the east coast. This is indeed a plausible expectation, but we will test this formally below.</span>
<span id="cb55-509"><a href="#cb55-509"></a></span>
<span id="cb55-510"><a href="#cb55-510"></a>The full multiple linear regression model with the interaction terms can be formally expressed as Equation \ref{mod2}:</span>
<span id="cb55-511"><a href="#cb55-511"></a><span class="in">```{=latex}</span></span>
<span id="cb55-512"><a href="#cb55-512"></a><span class="in">\begin{align}</span></span>
<span id="cb55-513"><a href="#cb55-513"></a><span class="in">Y &amp;= \alpha + \beta_1 \text{dist} + \beta_2 \text{bio}_{\text{B-ATZ}} + \beta_3 \text{bio}_{\text{BMP}} \nonumber \\</span></span>
<span id="cb55-514"><a href="#cb55-514"></a><span class="in">  &amp;\quad + \beta_4 \text{bio}_{\text{ECTZ}} + \beta_5 (\text{dist} \times \text{bio}_{\text{B-ATZ}}) \nonumber \\</span></span>
<span id="cb55-515"><a href="#cb55-515"></a><span class="in">  &amp;\quad + \beta_6 (\text{dist} \times \text{bio}_{\text{BMP}}) + \beta_7 (\text{dist} \times \text{bio}_{\text{ECTZ}}) + \epsilon \label{mod2}</span></span>
<span id="cb55-516"><a href="#cb55-516"></a><span class="in">\end{align}</span></span>
<span id="cb55-517"><a href="#cb55-517"></a><span class="in">```</span></span>
<span id="cb55-518"><a href="#cb55-518"></a>Where:</span>
<span id="cb55-519"><a href="#cb55-519"></a></span>
<span id="cb55-520"><a href="#cb55-520"></a><span class="ss">* </span>$Y$: The response variable, the mean Sørensen dissimilarity.</span>
<span id="cb55-521"><a href="#cb55-521"></a><span class="ss">* </span>$\alpha$: The intercept term.</span>
<span id="cb55-522"><a href="#cb55-522"></a><span class="ss">* </span>$\text{dist}$: The continuous predictor variable representing distance.</span>
<span id="cb55-523"><a href="#cb55-523"></a><span class="ss">* </span>$\text{bio}$: The categorical predictor variable representing bioregional classification with four levels: <span class="in">`AMP`</span> (reference category), <span class="in">`B-ATZ`</span>, <span class="in">`BMP`</span>, and <span class="in">`ECTZ`</span>.</span>
<span id="cb55-524"><a href="#cb55-524"></a><span class="ss">* </span>$\text{bio}_\text{B-ATZ}, \text{bio}_\text{BMP}, \text{bio}_\text{ECTZ}$: Dummy variables for the bioregional classification, where:</span>
<span id="cb55-525"><a href="#cb55-525"></a><span class="ss">  * </span>$\text{bio}_\text{B-ATZ} = 1$ if <span class="in">`bio`</span> = <span class="in">`B-ATZ`</span>, and 0 otherwise,</span>
<span id="cb55-526"><a href="#cb55-526"></a><span class="ss">  * </span>$\text{bio}_\text{BMP} = 1$ if <span class="in">`bio`</span> = <span class="in">`BMP`</span>, and 0 otherwise, and</span>
<span id="cb55-527"><a href="#cb55-527"></a><span class="ss">  * </span>$\text{bio}_\text{ECTZ} = 1$ if <span class="in">`bio`</span> = <span class="in">`ECTZ`</span>, and 0 otherwise.</span>
<span id="cb55-528"><a href="#cb55-528"></a><span class="ss">* </span>$\text{dist} \times \text{bio}_\text{B-ATZ}, \text{dist} \times \text{bio}_\text{BMP}, \text{dist} \times \text{bio}_\text{ECTZ}$: Interaction terms between distance and the bioregional classification dummy variables.</span>
<span id="cb55-529"><a href="#cb55-529"></a><span class="ss">* </span>$\beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6, \beta_7$: The coefficients to be estimated for the main effects and interactions.</span>
<span id="cb55-530"><a href="#cb55-530"></a><span class="ss">* </span>$\epsilon$: The error term.</span>
<span id="cb55-531"><a href="#cb55-531"></a></span>
<span id="cb55-532"><a href="#cb55-532"></a>If this seems tricky, it is because of the dummy variable coding used to represent interactions in multiple linear regression. The <span class="in">`bio`</span> variable is a categorical variable with four levels, so we need to create three dummy variables to represent the bioregional classification. The <span class="in">`dist`</span> variable is then interacted with each of these dummy variables to create the interaction terms. The <span class="in">`lm()`</span> function in R takes care of this for us in a far less complicated model statement. I'll explain the details around the interpretation of dummy variable coding when we look at the output of the model with the <span class="in">`summary()`</span> function.</span>
<span id="cb55-533"><a href="#cb55-533"></a></span>
<span id="cb55-534"><a href="#cb55-534"></a><span class="fu">### State the Hypotheses for a Multiple Linear Regression with Interaction Terms</span></span>
<span id="cb55-535"><a href="#cb55-535"></a></span>
<span id="cb55-536"><a href="#cb55-536"></a>Equation \ref{mod2} expands into the following series of hypotheses that concern the main effects, the interactions between the main effects, and the overall hypothesis:</span>
<span id="cb55-537"><a href="#cb55-537"></a></span>
<span id="cb55-538"><a href="#cb55-538"></a>**Main effects hypotheses**</span>
<span id="cb55-539"><a href="#cb55-539"></a></span>
<span id="cb55-540"><a href="#cb55-540"></a>In the main effects hypotheses we are concerned with the effect of each predictor variable on the response variable. For the main effect of distance we have the null:</span>
<span id="cb55-541"><a href="#cb55-541"></a></span>
<span id="cb55-542"><a href="#cb55-542"></a><span class="ss">* </span>$H_0: \beta_1 = 0$</span>
<span id="cb55-543"><a href="#cb55-543"></a></span>
<span id="cb55-544"><a href="#cb55-544"></a>vs. the alternative:</span>
<span id="cb55-545"><a href="#cb55-545"></a></span>
<span id="cb55-546"><a href="#cb55-546"></a><span class="ss">* </span>$H_A: \beta_1 \neq 0$</span>
<span id="cb55-547"><a href="#cb55-547"></a></span>
<span id="cb55-548"><a href="#cb55-548"></a>For the main effect of bioregional classification, the nulls are:</span>
<span id="cb55-549"><a href="#cb55-549"></a></span>
<span id="cb55-550"><a href="#cb55-550"></a><span class="ss">* </span>$H_0: \beta_2 = 0 \quad (\text{bio}_{\text{B-ATZ}})$</span>
<span id="cb55-551"><a href="#cb55-551"></a><span class="ss">* </span>$H_0: \beta_3 = 0 \quad (\text{bio}_{\text{BMP}})$</span>
<span id="cb55-552"><a href="#cb55-552"></a><span class="ss">* </span>$H_0: \beta_4 = 0 \quad (\text{bio}_{\text{ECTZ}})$</span>
<span id="cb55-553"><a href="#cb55-553"></a></span>
<span id="cb55-554"><a href="#cb55-554"></a>vs. the alternatives:</span>
<span id="cb55-555"><a href="#cb55-555"></a></span>
<span id="cb55-556"><a href="#cb55-556"></a><span class="ss">* </span>$H_A: \beta_2 \neq 0 \quad (\text{bio}_{\text{B-ATZ}})$</span>
<span id="cb55-557"><a href="#cb55-557"></a><span class="ss">* </span>$H_A: \beta_3 \neq 0 \quad (\text{bio}_{\text{BMP}})$</span>
<span id="cb55-558"><a href="#cb55-558"></a><span class="ss">* </span>$H_A: \beta_4 \neq 0 \quad (\text{bio}_{\text{ECTZ}})$</span>
<span id="cb55-559"><a href="#cb55-559"></a></span>
<span id="cb55-560"><a href="#cb55-560"></a>**Hypotheses about interactions**</span>
<span id="cb55-561"><a href="#cb55-561"></a></span>
<span id="cb55-562"><a href="#cb55-562"></a>This is where the hypothesis tests whether the effect of distance on the response variable is different for each bioregional classification. The null hypotheses are:</span>
<span id="cb55-563"><a href="#cb55-563"></a></span>
<span id="cb55-564"><a href="#cb55-564"></a><span class="ss">* </span>$H_0: \beta_5 = 0 \quad (\text{dist} \times \text{bio}_{\text{B-ATZ}})$</span>
<span id="cb55-565"><a href="#cb55-565"></a><span class="ss">* </span>$H_0: \beta_6 = 0 \quad (\text{dist} \times \text{bio}_{\text{BMP}})$</span>
<span id="cb55-566"><a href="#cb55-566"></a><span class="ss">* </span>$H_0: \beta_7 = 0 \quad (\text{dist} \times \text{bio}_{\text{ECTZ}})$</span>
<span id="cb55-567"><a href="#cb55-567"></a></span>
<span id="cb55-568"><a href="#cb55-568"></a>vs. the alternatives:</span>
<span id="cb55-569"><a href="#cb55-569"></a></span>
<span id="cb55-570"><a href="#cb55-570"></a><span class="ss">* </span>$H_A: \beta_5 \neq 0 \quad (\text{dist} \times \text{bio}_{\text{B-ATZ}})$</span>
<span id="cb55-571"><a href="#cb55-571"></a><span class="ss">* </span>$H_A: \beta_6 \neq 0 \quad (\text{dist} \times \text{bio}_{\text{BMP}})$</span>
<span id="cb55-572"><a href="#cb55-572"></a><span class="ss">* </span>$H_A: \beta_7 \neq 0 \quad (\text{dist} \times \text{bio}_{\text{ECTZ}})$</span>
<span id="cb55-573"><a href="#cb55-573"></a></span>
<span id="cb55-574"><a href="#cb55-574"></a>**Overall hypothesis**</span>
<span id="cb55-575"><a href="#cb55-575"></a></span>
<span id="cb55-576"><a href="#cb55-576"></a>The overall hypothesis states that all coefficients associated with the predictors (distance, bioregional categories, and their interactions) are equal to zero, therefore indicating no relationship between these predictors and the response variable, the Sørensen index. The null hypothesis is:</span>
<span id="cb55-577"><a href="#cb55-577"></a></span>
<span id="cb55-578"><a href="#cb55-578"></a><span class="ss">* </span>$H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = \beta_7 = 0$</span>
<span id="cb55-579"><a href="#cb55-579"></a></span>
<span id="cb55-580"><a href="#cb55-580"></a>vs. the alternative:</span>
<span id="cb55-581"><a href="#cb55-581"></a></span>
<span id="cb55-582"><a href="#cb55-582"></a><span class="ss">* </span>$H_A: \exists \, \beta_i \neq 0 \text{ for at least one } i$</span>
<span id="cb55-583"><a href="#cb55-583"></a></span>
<span id="cb55-586"><a href="#cb55-586"></a><span class="in">```{r}</span></span>
<span id="cb55-587"><a href="#cb55-587"></a><span class="co">#| echo: false</span></span>
<span id="cb55-588"><a href="#cb55-588"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb55-589"><a href="#cb55-589"></a><span class="co">#| fig-height: 2.5</span></span>
<span id="cb55-590"><a href="#cb55-590"></a><span class="co">#| fig.cap: "Plot of main effects of A) distance along the coast and B) bioregional classification on the Sørensen dissimilarity index."</span></span>
<span id="cb55-591"><a href="#cb55-591"></a><span class="co">#| label: fig-mod2_main_effects</span></span>
<span id="cb55-592"><a href="#cb55-592"></a></span>
<span id="cb55-593"><a href="#cb55-593"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(sw, <span class="fu">aes</span>(<span class="at">x =</span> dist, <span class="at">y =</span> Y)) <span class="sc">+</span></span>
<span id="cb55-594"><a href="#cb55-594"></a>  <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">1</span>, <span class="fu">aes</span>(<span class="at">colour =</span> bio)) <span class="sc">+</span></span>
<span id="cb55-595"><a href="#cb55-595"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">TRUE</span>,</span>
<span id="cb55-596"><a href="#cb55-596"></a>              <span class="at">colour =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"grey60"</span>,</span>
<span id="cb55-597"><a href="#cb55-597"></a>              <span class="at">linewidth =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb55-598"><a href="#cb55-598"></a>  <span class="fu">scale_colour_brewer</span>(<span class="at">palette =</span> <span class="st">"Set1"</span>) <span class="sc">+</span></span>
<span id="cb55-599"><a href="#cb55-599"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Distance (km)"</span>,</span>
<span id="cb55-600"><a href="#cb55-600"></a>       <span class="at">y =</span> <span class="st">"Sørensen Dissimilarity"</span>) <span class="sc">+</span></span>
<span id="cb55-601"><a href="#cb55-601"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb55-602"><a href="#cb55-602"></a></span>
<span id="cb55-603"><a href="#cb55-603"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(sw, <span class="fu">aes</span>(<span class="at">x =</span> bio, <span class="at">y =</span> Y)) <span class="sc">+</span></span>
<span id="cb55-604"><a href="#cb55-604"></a>  <span class="fu">geom_boxplot</span>(<span class="fu">aes</span>(<span class="at">colour =</span> bio)) <span class="sc">+</span></span>
<span id="cb55-605"><a href="#cb55-605"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Bioregional Classification"</span>,</span>
<span id="cb55-606"><a href="#cb55-606"></a>       <span class="at">y =</span> <span class="st">"Sørensen Dissimilarity"</span>) <span class="sc">+</span></span>
<span id="cb55-607"><a href="#cb55-607"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb55-608"><a href="#cb55-608"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span>
<span id="cb55-609"><a href="#cb55-609"></a></span>
<span id="cb55-610"><a href="#cb55-610"></a><span class="fu">ggarrange</span>(p1, p2, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">labels =</span> <span class="st">"AUTO"</span>)</span>
<span id="cb55-611"><a href="#cb55-611"></a><span class="in">```</span></span>
<span id="cb55-612"><a href="#cb55-612"></a></span>
<span id="cb55-613"><a href="#cb55-613"></a><span class="fu">### Visualise the Main Effects</span></span>
<span id="cb55-614"><a href="#cb55-614"></a></span>
<span id="cb55-615"><a href="#cb55-615"></a>To facilitate the interpretation of the main effects hypotheses and make an argument for why an interaction term might be necessary, I've visualised the main effects (@fig-mod2_main_effects). I see this as part of my exploratory data analysis ensemble of tests. We see that fitting a straight line to the <span class="in">`Y`</span> vs. distance relationship seems unsatisfactory as there is too much scatter around that single line to adequately capture all the structure in the variability of the points. Colouring the points by bioregion reveals the hidden structure. The model could benefit from including an additional level of complexity: see how points in the same bioregion show less scatter compared to points in different bioregions.</span>
<span id="cb55-616"><a href="#cb55-616"></a></span>
<span id="cb55-617"><a href="#cb55-617"></a>Now look at the boxplots of the Sørensen dissimilarity index for each bioregional classification. It shows that the median values of the Sørensen dissimilarity index are different for each bioregion. Taken together, @fig-mod2_main_effects (A, B) provide a good indication that adding the bioregional classification might be an important predictor of the Sørensen dissimilarity index as a function of distance between pairs of sites along the coast.</span>
<span id="cb55-618"><a href="#cb55-618"></a></span>
<span id="cb55-619"><a href="#cb55-619"></a>Next, we will move ahead and fit the model inclusive of the distance along the coast and bioregion as per Equation \eqref{mod2}.</span>
<span id="cb55-620"><a href="#cb55-620"></a></span>
<span id="cb55-621"><a href="#cb55-621"></a><span class="fu">### Fit and Assess Nested Models</span></span>
<span id="cb55-622"><a href="#cb55-622"></a></span>
<span id="cb55-623"><a href="#cb55-623"></a>I have a suspicion that the full model (<span class="in">`mod2`</span>; see below) with the interaction terms will be a better fit than reduced models with only the effect due to distance (seen independently). How can we have greater certainty that we should indeed favour a slightly more complex model (with two predictors) over a simpler one with only (distance only)?</span>
<span id="cb55-624"><a href="#cb55-624"></a></span>
<span id="cb55-625"><a href="#cb55-625"></a>One way to do this is to use a nested model comparison. We will fit a reduced model (one slope for all bioregions) and compare this model to the full model (slopes are allowed to vary among bioregions).</span>
<span id="cb55-626"><a href="#cb55-626"></a></span>
<span id="cb55-629"><a href="#cb55-629"></a><span class="in">```{r}</span></span>
<span id="cb55-630"><a href="#cb55-630"></a><span class="co"># Fit the linear regression model with only distance</span></span>
<span id="cb55-631"><a href="#cb55-631"></a>mod2a <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> dist, <span class="at">data =</span> sw)</span>
<span id="cb55-632"><a href="#cb55-632"></a></span>
<span id="cb55-633"><a href="#cb55-633"></a><span class="co"># Fit the multiple linear regression model with interaction terms</span></span>
<span id="cb55-634"><a href="#cb55-634"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> dist <span class="sc">*</span> bio, <span class="at">data =</span> sw)</span>
<span id="cb55-635"><a href="#cb55-635"></a><span class="in">```</span></span>
<span id="cb55-636"><a href="#cb55-636"></a></span>
<span id="cb55-637"><a href="#cb55-637"></a>This is a nested model where <span class="in">`mod2a`</span> is nested within <span class="in">`mod2`</span>. 'Nested' means that the reduced model is a subset of the full model. Nested models can be used to test hypotheses about the significance of the predictors in the full model---does adding more predictors to the model improve the fit? Comparing a nested model with a full model can be done with a sequential ANOVA, which is what the <span class="in">`anova()`</span> function also does (in addition to its use in @sec-mod1-summary).</span>
<span id="cb55-638"><a href="#cb55-638"></a></span>
<span id="cb55-639"><a href="#cb55-639"></a>So, comparing <span class="in">`mod2a`</span> to <span class="in">`mod2`</span> with an *F*-test tests the significance of adding the <span class="in">`bio`</span> and using it together with <span class="in">`dist`</span>. The interaction is built into <span class="in">`mod2`</span> but we are not yet testing the significance of the interaction terms. We will do that later.</span>
<span id="cb55-640"><a href="#cb55-640"></a></span>
<span id="cb55-641"><a href="#cb55-641"></a></span>
<span id="cb55-644"><a href="#cb55-644"></a><span class="in">```{r}</span></span>
<span id="cb55-645"><a href="#cb55-645"></a><span class="fu">anova</span>(mod2a, mod2, <span class="at">test =</span> <span class="st">"F"</span>)</span>
<span id="cb55-646"><a href="#cb55-646"></a><span class="in">```</span></span>
<span id="cb55-647"><a href="#cb55-647"></a></span>
<span id="cb55-648"><a href="#cb55-648"></a>The sequential ANOVA shows that there is significant merit to consider an interaction term in the model. This model would then allow us to have a separate slope for the Sørensen index as function of distance for each bioregion. The residual sum of squares (<span class="in">`RSS`</span>) decreases from 7.7388 in Model 1 to 2.2507 in Model 2, which indicates that Model 2 explains a significantly larger proportion of the variance in the response variable. The *F*-test for comparing the two models yields an *F*-value of 390.95 with a highly significant *p*-value (&lt; 0.0001). The improvement in model fit due to the inclusion of the interaction term is therefore statistically significant.</span>
<span id="cb55-649"><a href="#cb55-649"></a></span>
<span id="cb55-650"><a href="#cb55-650"></a>The above analyses skirted around the questions stated in the beginning of @sec-mlr-interaction. I've provided statistical evidence that full model is a better fit than the reduced model (the sequential *F*-test tested this), so we should use both <span class="in">`dist`</span> and <span class="in">`bio`</span> in the model. I have not looked explicitly at the main effects of the predictors. However, we can easily address questions (1) and (2): </span>
<span id="cb55-651"><a href="#cb55-651"></a></span>
<span id="cb55-652"><a href="#cb55-652"></a><span class="ss">* </span>Question 1: looking at the summary of <span class="in">`mod2a`</span> tells us that the main effect of <span class="in">`dist`</span> is a significant (*p* &lt; 0.0001) predictor of the Sørensen dissimilarity index.</span>
<span id="cb55-653"><a href="#cb55-653"></a><span class="ss">* </span>Question 2: the main effect of <span class="in">`bio`</span> is also significant (*p* &lt; 0.0001), which is what we'd see if we fit the model <span class="in">`mod2b &lt;- lm(Y ~ bio, data = sw)`</span>.</span>
<span id="cb55-654"><a href="#cb55-654"></a></span>
<span id="cb55-655"><a href="#cb55-655"></a>Question 3 warrants deeper investigation. Next, we will look at the interaction terms in the full model <span class="in">`mod2`</span> to see if the effect of <span class="in">`dist`</span> on <span class="in">`Y`</span> is different for each level of <span class="in">`bio`</span>.</span>
<span id="cb55-656"><a href="#cb55-656"></a></span>
<span id="cb55-657"><a href="#cb55-657"></a><span class="fu">### Interpret the Full Model</span></span>
<span id="cb55-658"><a href="#cb55-658"></a></span>
<span id="cb55-659"><a href="#cb55-659"></a>**The model summary**</span>
<span id="cb55-660"><a href="#cb55-660"></a></span>
<span id="cb55-663"><a href="#cb55-663"></a><span class="in">```{r}</span></span>
<span id="cb55-664"><a href="#cb55-664"></a><span class="co"># Summary of the model</span></span>
<span id="cb55-665"><a href="#cb55-665"></a><span class="fu">summary</span>(mod2)</span>
<span id="cb55-666"><a href="#cb55-666"></a><span class="in">```</span></span>
<span id="cb55-667"><a href="#cb55-667"></a></span>
<span id="cb55-668"><a href="#cb55-668"></a>In the output returned by <span class="in">`summary(mod2)`</span>, we need to pay special attention to the use of dummy variable encoding for the categorical predictor. The <span class="in">`Coefficients`</span> section is similar to that of <span class="in">`mod1`</span> (see @sec-mod1-summary), but now it includes the categorical predictor <span class="in">`bio*`</span> and the interaction terms <span class="in">`dist:bio*`</span> (<span class="in">`*`</span> indicating the levels of the categorical variable). The <span class="in">`bio`</span> variable has four levels, <span class="in">`BMP`</span>, <span class="in">`B-ATZ`</span>, <span class="in">`AMP`</span>, and <span class="in">`ECTZ`</span>, and <span class="in">`AMP`</span> is selected as reference level. This decision to selected <span class="in">`AMP`</span> as reference is entirely arbitrary, and alphabetical sorting offers a convenient approach to selecting the reference. The coefficients for the other levels of <span class="in">`bio`</span> are interpreted as the sum of the response variable and the reference level. </span>
<span id="cb55-669"><a href="#cb55-669"></a></span>
<span id="cb55-670"><a href="#cb55-670"></a>The following are the key coefficients in the model summary: </span>
<span id="cb55-671"><a href="#cb55-671"></a></span>
<span id="cb55-672"><a href="#cb55-672"></a><span class="ss">* </span><span class="in">`(Intercept)`</span>: This is the estimated average value of <span class="in">`Y`</span> when <span class="in">`dist`</span> is zero and <span class="in">`bio`</span> is the reference category (<span class="in">`AMP`</span>). Its *p*-value (&gt; 0.05) suggests it's not significantly different from zero.</span>
<span id="cb55-673"><a href="#cb55-673"></a><span class="ss">* </span>Main Effects:</span>
<span id="cb55-674"><a href="#cb55-674"></a><span class="ss">    * </span><span class="in">`dist`</span>: This represents the estimated change in <span class="in">`Y`</span> for a one-unit increase in <span class="in">`dist`</span> when the bioregion is the reference category, <span class="in">`AMP`</span>. The highly significant *p*-value (&lt; 0.0001) indicates a strong effect of distance in the <span class="in">`AMP`</span>.</span>
<span id="cb55-675"><a href="#cb55-675"></a><span class="ss">    * </span><span class="in">`bioB-ATZ`</span>, <span class="in">`bioBMP`</span>, <span class="in">`bioECTZ`</span>: These are dummy variables representing different bioregions. Their coefficients indicate the difference in the average value of <span class="in">`Y`</span> between each of these bioregions and the reference bioregion when <span class="in">`dist`</span> is zero. Only <span class="in">`bioBMP`</span> and <span class="in">`bioECTZ`</span> are significantly different from the reference bioregion, <span class="in">`AMP`</span>.</span>
<span id="cb55-676"><a href="#cb55-676"></a></span>
<span id="cb55-677"><a href="#cb55-677"></a><span class="ss">* </span>Interaction Effects:</span>
<span id="cb55-678"><a href="#cb55-678"></a><span class="ss">    * </span><span class="in">`dist:bioB-ATZ`</span>, <span class="in">`dist:bioBMP`</span>, <span class="in">`dist:bioECTZ`</span>: These interaction terms capture how the effect of <span class="in">`dist`</span> on <span class="in">`Y`</span> varies across different bioregions. For instance, <span class="in">`dist:bioB-ATZ`</span> indicates the additional change in the effect of <span class="in">`dist`</span> in the <span class="in">`B-ATZ`</span> bioregion compared to the reference bioregion, <span class="in">`AMP`</span>. All interaction terms are highly significant, suggesting the effect of distance is different across bioregions.</span>
<span id="cb55-679"><a href="#cb55-679"></a></span>
<span id="cb55-680"><a href="#cb55-680"></a>Given this explanation, we can now interpret the coefficients of, for example, the <span class="in">`bioB-ATZ`</span> main effect and <span class="in">`dist:bioB-ATZ`</span> interaction. Since <span class="in">`AMP`</span> is the reference bioregion, its effect is absorbed into the intercept term. Therefore, the coefficient for <span class="in">`bioB-ATZ`</span> directly reflects the difference we are interested in. The coefficient for <span class="in">`bioB-ATZ`</span> is <span class="in">`r round(summary(mod2)$coefficients["bioB-ATZ", "Estimate"], 4)`</span> $\pm$ <span class="in">`r round(summary(mod2)$coefficients["bioB-ATZ", "Std. Error"], 4)`</span> lower than that of the reference, but the associated *p*-value (&gt; 0.05) indicates that the average value of <span class="in">`Y`</span> in the <span class="in">`B-ATZ`</span> bioregion is not significantly different from the reference bioregion, <span class="in">`AMP`</span>.</span>
<span id="cb55-681"><a href="#cb55-681"></a></span>
<span id="cb55-682"><a href="#cb55-682"></a>If we'd want to report the actual coefficient for <span class="in">`B-ATZ`</span>, we'd calculate the sum of the coefficients for <span class="in">`(Intercept)`</span> and <span class="in">`bioB-ATZ`</span>. This would give us the estimated average value of <span class="in">`Y`</span> in the <span class="in">`B-ATZ`</span> bioregion when <span class="in">`dist`</span> is zero. The associated SE is calculated as the square root of the sum of the squared SEs of the two coefficients. Therefore, the coefficient for <span class="in">`B-ATZ`</span> is <span class="in">`r round(summary(mod2)$coefficients["(Intercept)", "Estimate"] + summary(mod2)$coefficients["bioB-ATZ", "Estimate"], 4)`</span> $\pm$ <span class="in">`r round(sqrt(summary(mod2)$coefficients["(Intercept)", "Std. Error"]^2 + summary(mod2)$coefficients["bioB-ATZ", "Std. Error"]^2), 4)`</span>.</span>
<span id="cb55-683"><a href="#cb55-683"></a></span>
<span id="cb55-684"><a href="#cb55-684"></a>The coefficient of <span class="in">`r round(summary(mod2)$coefficients["dist:bioB-ATZ", "Estimate"], 4)`</span> for <span class="in">`dist:bioB-ATZ`</span> indicates that the effect of distance on <span class="in">`Y`</span> is <span class="in">`r round(summary(mod2)$coefficients["dist:bioB-ATZ", "Estimate"], 4)`</span> units greater in the <span class="in">`B-ATZ`</span> bioregion compared to the <span class="in">`AMP`</span> bioregion. The SE of <span class="in">`r round(summary(mod2)$coefficients["dist:bioB-ATZ", "Std. Error"], 4)`</span> suggests a high level of precision in this estimate, and the *p*-value (&lt; 0.0001) indicates that this difference is statistically significant.</span>
<span id="cb55-685"><a href="#cb55-685"></a></span>
<span id="cb55-686"><a href="#cb55-686"></a>As before, to calculate the actual coefficient for <span class="in">`dist`</span> in the <span class="in">`B-ATZ`</span> bioregion, we'd sum the coefficients for <span class="in">`dist`</span> and <span class="in">`dist:bioB-ATZ`</span>. The associated SE of this sum is calculated as the square root of the sum of the squared SEs of the two coefficients. Therefore, the coefficient for <span class="in">`dist`</span> in the <span class="in">`B-ATZ`</span> bioregion is <span class="in">`r round(summary(mod2)$coefficients["dist", "Estimate"] + summary(mod2)$coefficients["dist:bioB-ATZ", "Estimate"], 4)`</span> $\pm$ <span class="in">`r round(sqrt(summary(mod2)$coefficients["dist", "Std. Error"]^2 + summary(mod2)$coefficients["dist:bioB-ATZ", "Std. Error"]^2), 4)`</span>.</span>
<span id="cb55-687"><a href="#cb55-687"></a></span>
<span id="cb55-688"><a href="#cb55-688"></a>Concerning the overall hypothesis, the <span class="in">`Adjusted R-squared`</span> value of 0.8597 indicates that the model explains 85.97% of the variance in the response variable <span class="in">`Y`</span>. The <span class="in">`F-statistic`</span> and associated <span class="in">`p-value`</span> (&lt; 0.0001) indicate that the model as a whole is highly significant, meaning at least one of the predictors (including interactions) has a significant effect on <span class="in">`Y`</span>.</span>
<span id="cb55-689"><a href="#cb55-689"></a></span>
<span id="cb55-690"><a href="#cb55-690"></a>**The ANOVA table**</span>
<span id="cb55-691"><a href="#cb55-691"></a></span>
<span id="cb55-694"><a href="#cb55-694"></a><span class="in">```{r}</span></span>
<span id="cb55-695"><a href="#cb55-695"></a><span class="co"># The ANOVA table</span></span>
<span id="cb55-696"><a href="#cb55-696"></a><span class="fu">anova</span>(mod2)</span>
<span id="cb55-697"><a href="#cb55-697"></a><span class="in">```</span></span>
<span id="cb55-698"><a href="#cb55-698"></a></span>
<span id="cb55-699"><a href="#cb55-699"></a>The ANOVA table's interpretation is intuitive and simple: the <span class="in">`Pr(&gt;F)`</span> column shows the *p*-value for each predictor in the model. The <span class="in">`dist`</span> predictor has a highly significant effect on <span class="in">`Y`</span> (&lt; 0.0001), as do all the bioregions and their interactions with <span class="in">`dist`</span>. This confirms the results we obtained from the coefficients. We don't need to overthink this result.</span>
<span id="cb55-700"><a href="#cb55-700"></a></span>
<span id="cb55-701"><a href="#cb55-701"></a><span class="fu">## Example 3: The Final Model</span></span>
<span id="cb55-702"><a href="#cb55-702"></a></span>
<span id="cb55-703"><a href="#cb55-703"></a>I'll now expand <span class="in">`mod1`</span> to include <span class="in">`bio`</span> as a predictor alongside <span class="in">`augMean`</span>, <span class="in">`febSD`</span>, and <span class="in">`augSD`</span> (<span class="in">`mod1`</span> was applied only to data pertaining to <span class="in">`ECTZ`</span>, one of the four levels in <span class="in">`bio`</span>).</span>
<span id="cb55-704"><a href="#cb55-704"></a></span>
<span id="cb55-705"><a href="#cb55-705"></a><span class="in">```{=latex}</span></span>
<span id="cb55-706"><a href="#cb55-706"></a><span class="in">\begin{align}</span></span>
<span id="cb55-707"><a href="#cb55-707"></a><span class="in">Y &amp;= \alpha + \beta_1 \text{augMean} + \beta_2 \text{febSD} + \beta_3 \text{augSD} \nonumber \\</span></span>
<span id="cb55-708"><a href="#cb55-708"></a><span class="in">  &amp;\quad + \beta_4 \text{bio}_{\text{B-ATZ}} + \beta_5 \text{bio}_{\text{BMP}} + \beta_6 \text{bio}_{\text{ECTZ}} \nonumber \\</span></span>
<span id="cb55-709"><a href="#cb55-709"></a><span class="in">  &amp;\quad + \beta_7 (\text{augMean} \times \text{bio}_{\text{B-ATZ}}) + \beta_8 (\text{augMean} \times \text{bio}_{\text{BMP}}) \nonumber \\</span></span>
<span id="cb55-710"><a href="#cb55-710"></a><span class="in">  &amp;\quad + \beta_9 (\text{augMean} \times \text{bio}_{\text{ECTZ}}) + \beta_{10} (\text{febSD} \times \text{bio}_{\text{B-ATZ}}) \nonumber \\</span></span>
<span id="cb55-711"><a href="#cb55-711"></a><span class="in">  &amp;\quad + \beta_{11} (\text{febSD} \times \text{bio}_{\text{BMP}}) + \beta_{12} (\text{febSD} \times \text{bio}_{\text{ECTZ}}) \nonumber \\</span></span>
<span id="cb55-712"><a href="#cb55-712"></a><span class="in">  &amp;\quad + \beta_{13} (\text{augSD} \times \text{bio}_{\text{B-ATZ}}) + \beta_{14} (\text{augSD} \times \text{bio}_{\text{BMP}}) \nonumber \\</span></span>
<span id="cb55-713"><a href="#cb55-713"></a><span class="in">  &amp;\quad + \beta_{15} (\text{augSD} \times \text{bio}_{\text{ECTZ}}) + \epsilon \label{mod3}</span></span>
<span id="cb55-714"><a href="#cb55-714"></a><span class="in">\end{align}</span></span>
<span id="cb55-715"><a href="#cb55-715"></a><span class="in">```</span></span>
<span id="cb55-716"><a href="#cb55-716"></a>Where:</span>
<span id="cb55-717"><a href="#cb55-717"></a></span>
<span id="cb55-718"><a href="#cb55-718"></a><span class="ss">* </span>$Y$: The response variable (mean Sørensen dissimilarity).</span>
<span id="cb55-719"><a href="#cb55-719"></a><span class="ss">* </span>$\alpha$: The intercept term, representing the expected value of <span class="in">`Y`</span> when all predictors are zero and <span class="in">`bio`</span> is at the reference level <span class="in">`AMP`</span>).</span>
<span id="cb55-720"><a href="#cb55-720"></a><span class="ss">* </span>$\beta_1$: The coefficient for the main effect of <span class="in">`augMean.`</span></span>
<span id="cb55-721"><a href="#cb55-721"></a><span class="ss">* </span>$\beta_2$: The coefficient for the main effect of <span class="in">`febSD.`</span></span>
<span id="cb55-722"><a href="#cb55-722"></a><span class="ss">* </span>$\beta_3$: The coefficient for the main effect of <span class="in">`augSD.`</span></span>
<span id="cb55-723"><a href="#cb55-723"></a><span class="ss">* </span>$\beta_4, \beta_5, \beta_6$: The coefficients for the main effects of the categorical predictor <span class="in">`bio`</span> (for levels <span class="in">`B-ATZ`</span>, <span class="in">`BMP`</span>, and <span class="in">`ECTZ`</span> respectively, with <span class="in">`AMP`</span> as the reference category).</span>
<span id="cb55-724"><a href="#cb55-724"></a><span class="ss">* </span>$\beta_7, \beta_8, \beta_9$: The coefficients for the interaction effects between <span class="in">`augMean`</span> and <span class="in">`bio`</span> (for levels <span class="in">`B-ATZ`</span>, <span class="in">`BMP`</span>, and <span class="in">`ECTZ`</span> respectively).</span>
<span id="cb55-725"><a href="#cb55-725"></a><span class="ss">* </span>$\beta_{10}, \beta_{11}, \beta_{12}$: The coefficients for the interaction effects between <span class="in">`febSD`</span> and <span class="in">`bio`</span> (for levels <span class="in">`B-ATZ`</span>, <span class="in">`BMP`</span>, and <span class="in">`ECTZ`</span> respectively).</span>
<span id="cb55-726"><a href="#cb55-726"></a><span class="ss">* </span>$\beta_{13}, \beta_{14}, \beta_{15}$: The coefficients for the interaction effects between <span class="in">`augSD`</span> and <span class="in">`bio`</span> (for levels <span class="in">`B-ATZ`</span>, <span class="in">`BMP`</span>, and <span class="in">`ECTZ`</span> respectively).</span>
<span id="cb55-727"><a href="#cb55-727"></a><span class="ss">* </span>$\epsilon$: The error term, representing the unexplained variability in the response variable.</span>
<span id="cb55-728"><a href="#cb55-728"></a></span>
<span id="cb55-729"><a href="#cb55-729"></a>In this multiple regression model, we aim to understand the complex and interacting relationships between the response variables and the set of predictors. It allows us to investigate not only the individual effects of the continuous predictors on <span class="in">`Y`</span>, but also how these effects might vary across the different bioregions.</span>
<span id="cb55-730"><a href="#cb55-730"></a></span>
<span id="cb55-731"><a href="#cb55-731"></a>The model therefore incorporates interaction terms between each continuous predictor (<span class="in">`augMean`</span>, <span class="in">`febSD`</span>, and <span class="in">`augSD`</span>) and the categorical variable <span class="in">`bio`</span>. This allows us to assess whether the relationships between <span class="in">`augMean`</span>, <span class="in">`febSD`</span>, or <span class="in">`augSD`</span> and <span class="in">`Y`</span> change depending on the specific bioregion. Essentially, we are testing whether the slopes of these relationships are different in different bioregions. </span>
<span id="cb55-732"><a href="#cb55-732"></a></span>
<span id="cb55-733"><a href="#cb55-733"></a>Additionally, the model examines the main effects of the bioregions themselves on <span class="in">`Y`</span>. This means we're testing whether the average value of <span class="in">`Y`</span> differs significantly across bioregions, after accounting for the influence of the continuous predictors.  </span>
<span id="cb55-734"><a href="#cb55-734"></a></span>
<span id="cb55-735"><a href="#cb55-735"></a>This is how these different insights pertain to the model components:</span>
<span id="cb55-736"><a href="#cb55-736"></a></span>
<span id="cb55-737"><a href="#cb55-737"></a><span class="ss">* </span>Main Effects: The coefficients for the main effects of <span class="in">`augMean`</span>, <span class="in">`febSD`</span>, and <span class="in">`augSD`</span> represent the effect of each predictor when <span class="in">`bio`</span> is at its reference level.</span>
<span id="cb55-738"><a href="#cb55-738"></a><span class="ss">* </span>Coefficients for <span class="in">`bio`</span>: The coefficients for <span class="in">`bio`</span> (e.g., $\beta_4 \text{bio}_{\text{B-ATZ}}$) represent the difference in the intercept for the corresponding level of <span class="in">`bio`</span> compared to the reference level.</span>
<span id="cb55-739"><a href="#cb55-739"></a><span class="ss">* </span>Interaction Terms: The interaction terms allow the slopes of <span class="in">`augMean`</span>, <span class="in">`febSD`</span>, and <span class="in">`augSD`</span> to vary across the different levels of <span class="in">`bio`</span>. For example, $\beta_7 (\text{augMean} \times \text{bio}_{\text{B-ATZ}})$ represents how the effect of <span class="in">`augMean`</span> on <span class="in">`Y`</span> changes when <span class="in">`bio`</span> is <span class="in">`B-ATZ`</span> compared to <span class="in">`AMP`</span>.</span>
<span id="cb55-740"><a href="#cb55-740"></a></span>
<span id="cb55-741"><a href="#cb55-741"></a><span class="fu">### State the Hypotheses</span></span>
<span id="cb55-742"><a href="#cb55-742"></a></span>
<span id="cb55-743"><a href="#cb55-743"></a>**Overall hypothesis**</span>
<span id="cb55-744"><a href="#cb55-744"></a></span>
<span id="cb55-745"><a href="#cb55-745"></a>I'll only state the overall hypothesis for this model as the expansion of the individual hypotheses for each predictor and interactions (all the $\beta$-coefficients in Equation \ref{mod3}) is quite voluminous.</span>
<span id="cb55-746"><a href="#cb55-746"></a></span>
<span id="cb55-747"><a href="#cb55-747"></a>The null is that there is no relationship between the response variable <span class="in">`Y`</span> and the predictors (including their interactions):</span>
<span id="cb55-748"><a href="#cb55-748"></a></span>
<span id="cb55-749"><a href="#cb55-749"></a><span class="ss">- </span>$H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = \beta_7 = \beta_8 = \beta_9 = \beta_{10} = \beta_{11} = \beta_{12} = \beta_{13} = \beta_{14} = \beta_{15} = 0$</span>
<span id="cb55-750"><a href="#cb55-750"></a></span>
<span id="cb55-751"><a href="#cb55-751"></a>The alternative is that at least one predictor or interaction term has a significant relationship with the response variable <span class="in">`Y`</span>:</span>
<span id="cb55-752"><a href="#cb55-752"></a></span>
<span id="cb55-753"><a href="#cb55-753"></a><span class="ss">- </span>$H_A: \text{At least one } \beta_i \neq 0 \text{ for } i \in <span class="sc">\{</span>1, 2, ..., 15<span class="sc">\}</span>$</span>
<span id="cb55-754"><a href="#cb55-754"></a></span>
<span id="cb55-755"><a href="#cb55-755"></a><span class="fu">### Fit the Model</span></span>
<span id="cb55-756"><a href="#cb55-756"></a></span>
<span id="cb55-759"><a href="#cb55-759"></a><span class="in">```{r}</span></span>
<span id="cb55-760"><a href="#cb55-760"></a><span class="co">#| echo: false</span></span>
<span id="cb55-761"><a href="#cb55-761"></a><span class="co">#| eval: false</span></span>
<span id="cb55-762"><a href="#cb55-762"></a>sw_sub2 <span class="ot">&lt;-</span> sw <span class="sc">|&gt;</span></span>
<span id="cb55-763"><a href="#cb55-763"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(Y, augMean, febSD, augSD, bio)</span>
<span id="cb55-764"><a href="#cb55-764"></a></span>
<span id="cb55-765"><a href="#cb55-765"></a>mod3a <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> augMean <span class="sc">+</span> febSD <span class="sc">+</span> augSD, <span class="at">data =</span> sw_sub2)</span>
<span id="cb55-766"><a href="#cb55-766"></a></span>
<span id="cb55-767"><a href="#cb55-767"></a>initial_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="st">"Y ~ ."</span>)</span>
<span id="cb55-768"><a href="#cb55-768"></a></span>
<span id="cb55-769"><a href="#cb55-769"></a>threshold <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co"># Define a threshold for VIF values</span></span>
<span id="cb55-770"><a href="#cb55-770"></a></span>
<span id="cb55-771"><a href="#cb55-771"></a><span class="co"># Extract the names of the predictor variables</span></span>
<span id="cb55-772"><a href="#cb55-772"></a>predictors <span class="ot">&lt;-</span> <span class="fu">names</span>(<span class="fu">vif</span>(full_mod1))</span>
<span id="cb55-773"><a href="#cb55-773"></a></span>
<span id="cb55-774"><a href="#cb55-774"></a><span class="co"># Iteratively remove collinear variables</span></span>
<span id="cb55-775"><a href="#cb55-775"></a><span class="cf">while</span> (<span class="cn">TRUE</span>) {</span>
<span id="cb55-776"><a href="#cb55-776"></a>  <span class="co"># Calculate VIF values</span></span>
<span id="cb55-777"><a href="#cb55-777"></a>  vif_values <span class="ot">&lt;-</span> <span class="fu">vif</span>(full_mod1)</span>
<span id="cb55-778"><a href="#cb55-778"></a>  <span class="fu">print</span>(vif_values) <span class="co"># Print VIF values for debugging</span></span>
<span id="cb55-779"><a href="#cb55-779"></a>  max_vif <span class="ot">&lt;-</span> <span class="fu">max</span>(vif_values)</span>
<span id="cb55-780"><a href="#cb55-780"></a>  </span>
<span id="cb55-781"><a href="#cb55-781"></a>  <span class="co"># Check if the maximum VIF is above the threshold</span></span>
<span id="cb55-782"><a href="#cb55-782"></a>  <span class="cf">if</span> (max_vif <span class="sc">&gt;</span> threshold) {</span>
<span id="cb55-783"><a href="#cb55-783"></a>    <span class="co"># Find the variable with the highest VIF</span></span>
<span id="cb55-784"><a href="#cb55-784"></a>    high_vif_var <span class="ot">&lt;-</span> <span class="fu">names</span>(<span class="fu">which.max</span>(vif_values))</span>
<span id="cb55-785"><a href="#cb55-785"></a>    <span class="fu">cat</span>(<span class="st">"Removing variable:"</span>,</span>
<span id="cb55-786"><a href="#cb55-786"></a>        high_vif_var,</span>
<span id="cb55-787"><a href="#cb55-787"></a>        <span class="st">"with VIF:"</span>,</span>
<span id="cb55-788"><a href="#cb55-788"></a>        max_vif,</span>
<span id="cb55-789"><a href="#cb55-789"></a>        <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb55-790"><a href="#cb55-790"></a>    </span>
<span id="cb55-791"><a href="#cb55-791"></a>    <span class="co"># Update the formula to exclude the high VIF variable</span></span>
<span id="cb55-792"><a href="#cb55-792"></a>    updated_formula <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">"Y ~ . -"</span>, high_vif_var))</span>
<span id="cb55-793"><a href="#cb55-793"></a>    </span>
<span id="cb55-794"><a href="#cb55-794"></a>    <span class="co"># Refit the model without the high VIF variable</span></span>
<span id="cb55-795"><a href="#cb55-795"></a>    full_mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(updated_formula, <span class="at">data =</span> sw_sub2)</span>
<span id="cb55-796"><a href="#cb55-796"></a>    </span>
<span id="cb55-797"><a href="#cb55-797"></a>    <span class="co"># Update the environment data frame to reflect the removal</span></span>
<span id="cb55-798"><a href="#cb55-798"></a>    sw_sub2 <span class="ot">&lt;-</span> sw_sub2[, <span class="sc">!</span>(<span class="fu">names</span>(sw_sub2) <span class="sc">%in%</span> high_vif_var)]</span>
<span id="cb55-799"><a href="#cb55-799"></a>  } <span class="cf">else</span> {</span>
<span id="cb55-800"><a href="#cb55-800"></a>    <span class="cf">break</span></span>
<span id="cb55-801"><a href="#cb55-801"></a>  }</span>
<span id="cb55-802"><a href="#cb55-802"></a>}</span>
<span id="cb55-803"><a href="#cb55-803"></a><span class="in">```</span></span>
<span id="cb55-804"><a href="#cb55-804"></a></span>
<span id="cb55-805"><a href="#cb55-805"></a>In @sec-example1 I included the ECTZ seaweed flora in my analysis, but here I expand it to the full dataset. To assure myself that there is not a high degree of multicollinearity between the predictors, I have calculated the variance inflation factors (VIFs) for the full model (not shown). This allowed me to retain the same three predictors used in <span class="in">`mod1`</span>, i.e. <span class="in">`augMean`</span>, <span class="in">`febSD`</span>, and <span class="in">`augSD`</span>. This is the point of departure for <span class="in">`mod3`</span>.</span>
<span id="cb55-806"><a href="#cb55-806"></a></span>
<span id="cb55-807"><a href="#cb55-807"></a>Now I fit the model with those three continuous predictors and their interactions with the categorical variable <span class="in">`bio`</span>.</span>
<span id="cb55-808"><a href="#cb55-808"></a></span>
<span id="cb55-811"><a href="#cb55-811"></a><span class="in">```{r}</span></span>
<span id="cb55-812"><a href="#cb55-812"></a><span class="co"># Make a dataframe with only the relevant columns</span></span>
<span id="cb55-813"><a href="#cb55-813"></a>sw_sub2 <span class="ot">&lt;-</span> sw <span class="sc">|&gt;</span></span>
<span id="cb55-814"><a href="#cb55-814"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(Y, augMean, febSD, augSD, bio)</span>
<span id="cb55-815"><a href="#cb55-815"></a></span>
<span id="cb55-816"><a href="#cb55-816"></a><span class="co"># Fit the multiple linear regression model with interaction terms</span></span>
<span id="cb55-817"><a href="#cb55-817"></a>full_mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> (augMean <span class="sc">+</span> febSD <span class="sc">+</span> augSD) <span class="sc">*</span> bio, <span class="at">data =</span> sw_sub2)</span>
<span id="cb55-818"><a href="#cb55-818"></a>full_mod3a <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> augMean <span class="sc">+</span> febSD <span class="sc">+</span> augSD, <span class="at">data =</span> sw_sub2)</span>
<span id="cb55-819"><a href="#cb55-819"></a>null_mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> sw_sub2)</span>
<span id="cb55-820"><a href="#cb55-820"></a><span class="in">```</span></span>
<span id="cb55-821"><a href="#cb55-821"></a></span>
<span id="cb55-822"><a href="#cb55-822"></a>Model <span class="in">`full_mod3a`</span> is similar to <span class="in">`full_mod3`</span> but without the interaction terms. This will allow me to compare the two models and assess the importance of the interactions.</span>
<span id="cb55-823"><a href="#cb55-823"></a></span>
<span id="cb55-826"><a href="#cb55-826"></a><span class="in">```{r}</span></span>
<span id="cb55-827"><a href="#cb55-827"></a><span class="co"># Compare the models</span></span>
<span id="cb55-828"><a href="#cb55-828"></a><span class="fu">anova</span>(full_mod3, full_mod3a)</span>
<span id="cb55-829"><a href="#cb55-829"></a><span class="fu">AIC</span>(full_mod3, full_mod3a)</span>
<span id="cb55-830"><a href="#cb55-830"></a><span class="in">```</span></span>
<span id="cb55-831"><a href="#cb55-831"></a></span>
<span id="cb55-832"><a href="#cb55-832"></a>The AIC value for <span class="in">`full_mod3`</span> is lower than that of <span class="in">`full_mod3a`</span>, indicating that including the interaction with <span class="in">`bio`</span> is necessary. Likewise, the ANOVA test also shows that the full model (lower residual sum of squares) is significantly better than the reduced model.</span>
<span id="cb55-833"><a href="#cb55-833"></a></span>
<span id="cb55-834"><a href="#cb55-834"></a>I therefore use <span class="in">`full_mod3`</span> going forward. This is a complex model so I have used the stepwise selection function, <span class="in">`stepAIC()`</span>, to identify the most important predictors and interactions (code and output not shown). I hoped that this might have simplified the model somewhat, but the simplification I had hoped for did not materialise.</span>
<span id="cb55-835"><a href="#cb55-835"></a></span>
<span id="cb55-838"><a href="#cb55-838"></a><span class="in">```{r}</span></span>
<span id="cb55-839"><a href="#cb55-839"></a><span class="co">#| echo: false</span></span>
<span id="cb55-840"><a href="#cb55-840"></a><span class="co">#| include: false</span></span>
<span id="cb55-841"><a href="#cb55-841"></a></span>
<span id="cb55-842"><a href="#cb55-842"></a><span class="co"># Perform forward selection</span></span>
<span id="cb55-843"><a href="#cb55-843"></a>mod3 <span class="ot">&lt;-</span> <span class="fu">stepAIC</span>(null_mod3,</span>
<span id="cb55-844"><a href="#cb55-844"></a>                <span class="at">scope =</span> <span class="fu">list</span>(<span class="at">lower =</span> null_mod3, <span class="at">upper =</span> full_mod3),</span>
<span id="cb55-845"><a href="#cb55-845"></a>                <span class="at">direction =</span> <span class="st">"forward"</span>)</span>
<span id="cb55-846"><a href="#cb55-846"></a><span class="in">```</span></span>
<span id="cb55-847"><a href="#cb55-847"></a></span>
<span id="cb55-848"><a href="#cb55-848"></a><span class="fu">### Interpret the Model</span></span>
<span id="cb55-849"><a href="#cb55-849"></a></span>
<span id="cb55-850"><a href="#cb55-850"></a>**The model summary**</span>
<span id="cb55-851"><a href="#cb55-851"></a></span>
<span id="cb55-852"><a href="#cb55-852"></a>The model summary provides a detailed look at the individual predictors and their interactions in the model.</span>
<span id="cb55-853"><a href="#cb55-853"></a></span>
<span id="cb55-856"><a href="#cb55-856"></a><span class="in">```{r}</span></span>
<span id="cb55-857"><a href="#cb55-857"></a><span class="co"># Summary of the model</span></span>
<span id="cb55-858"><a href="#cb55-858"></a><span class="fu">summary</span>(mod3) <span class="co"># full_mod3 renamed to mod3 during stepAIC()</span></span>
<span id="cb55-859"><a href="#cb55-859"></a><span class="in">```</span></span>
<span id="cb55-860"><a href="#cb55-860"></a></span>
<span id="cb55-861"><a href="#cb55-861"></a>The first thing to notice is that the model function has been rewritten in the forward selection process (but none of the variables were deemed insignificant and removed):</span>
<span id="cb55-862"><a href="#cb55-862"></a></span>
<span id="cb55-863"><a href="#cb55-863"></a><span class="ss">* </span>Initial specification: <span class="in">`Y ~ (augMean + febSD + augSD) * bio`</span></span>
<span id="cb55-864"><a href="#cb55-864"></a><span class="ss">* </span>Specification after <span class="in">`stepAIC()`</span>: <span class="in">`Y ~ augMean + bio + augSD + febSD + augMean:bio + bio:augSD + bio:febSD`</span></span>
<span id="cb55-865"><a href="#cb55-865"></a></span>
<span id="cb55-866"><a href="#cb55-866"></a>Functionally, these two are identical, but the order in which the terms are presented differs. Although this has affected the order in which the coefficients are presented in the summary output, the coefficients are the same. The coefficients are:</span>
<span id="cb55-867"><a href="#cb55-867"></a></span>
<span id="cb55-868"><a href="#cb55-868"></a><span class="ss">* </span><span class="in">`(Intercept)`</span>: This is the estimated average value of <span class="in">`Y`</span> when all predictor variables are zero and the observation is in the reference bioregion (<span class="in">`AMP`</span>).</span>
<span id="cb55-869"><a href="#cb55-869"></a><span class="ss">* </span>Main Effects:</span>
<span id="cb55-870"><a href="#cb55-870"></a><span class="ss">    * </span><span class="in">`augMean`</span>:  For every one-unit increase in <span class="in">`augMean`</span>, <span class="in">`Y`</span> increases by 0.3441, on average, assuming all other predictors are held constant. This effect is highly significant.</span>
<span id="cb55-871"><a href="#cb55-871"></a><span class="ss">    * </span><span class="in">`augSD`</span> and <span class="in">`febSD`</span>: The main effects of these variables are not statistically significant, suggesting they might not have a direct impact on <span class="in">`Y`</span> when averaged across all bioregions.</span>
<span id="cb55-872"><a href="#cb55-872"></a><span class="ss">    * </span><span class="in">`bioB-ATZ`</span>, <span class="in">`bioBMP`</span>, <span class="in">`bioECTZ`</span>: These coefficients represent the average difference in <span class="in">`Y`</span> between each of these bioregions and the reference bioregion, when the continuous predictors are held at zero.</span>
<span id="cb55-873"><a href="#cb55-873"></a><span class="ss">* </span>Interaction Effects:</span>
<span id="cb55-874"><a href="#cb55-874"></a><span class="ss">    * </span><span class="in">`augMean`</span> interactions: The significant interactions of <span class="in">`augMean`</span> with bioregion indicate that the effect of <span class="in">`augMean`</span> on <span class="in">`Y`</span> varies across bioregions. Notably, the interaction with <span class="in">`bioBMP`</span> has a strong, significant negative effect, suggesting that the positive effect of <span class="in">`augMean`</span> is much weaker in this bioregion compared to the reference.</span>
<span id="cb55-875"><a href="#cb55-875"></a><span class="ss">    * </span><span class="in">`augSD`</span> and <span class="in">`febSD`</span> interactions: These interactions with bioregions are sometimes significant, providing good support for the alternative hypothesis that the effects of <span class="in">`augSD`</span> and <span class="in">`febSD`</span> on <span class="in">`Y`</span> depend on the specific bioregion.</span>
<span id="cb55-876"><a href="#cb55-876"></a></span>
<span id="cb55-877"><a href="#cb55-877"></a>Since dummy coding returns differences with respect to reference levels, how would we calculate the actual coefficients for, say, <span class="in">`augMean`</span>? Since there are significant interaction effects, we must consider the main effect of <span class="in">`augMean`</span> in conjunction with bioregion.</span>
<span id="cb55-878"><a href="#cb55-878"></a></span>
<span id="cb55-879"><a href="#cb55-879"></a>For <span class="in">`bio = B-ATZ`</span>:</span>
<span id="cb55-880"><a href="#cb55-880"></a></span>
<span id="cb55-881"><a href="#cb55-881"></a><span class="ss">* </span>$\beta_{\text{augMean}} + \beta_{\text{augMean:bioB-ATZ}} = 0.3441099 + (-0.0461775) = 0.2979324$</span>
<span id="cb55-882"><a href="#cb55-882"></a></span>
<span id="cb55-883"><a href="#cb55-883"></a>For <span class="in">`bio = BMP`</span>:</span>
<span id="cb55-884"><a href="#cb55-884"></a></span>
<span id="cb55-885"><a href="#cb55-885"></a><span class="ss">* </span>$\beta_{\text{augMean}} + \beta_{\text{augMean:bioBMP}} = 0.3441099 + (-0.2406297) = 0.1034802$</span>
<span id="cb55-886"><a href="#cb55-886"></a></span>
<span id="cb55-887"><a href="#cb55-887"></a>For <span class="in">`bio = ECTZ`</span>:</span>
<span id="cb55-888"><a href="#cb55-888"></a></span>
<span id="cb55-889"><a href="#cb55-889"></a>  $\beta_{\text{augMean}} + \beta_{\text{augMean:bioECTZ}} = 0.3441099 + (-0.0607745) = 0.2833354$</span>
<span id="cb55-890"><a href="#cb55-890"></a></span>
<span id="cb55-891"><a href="#cb55-891"></a>The respective SEs for these coefficients can be calculated using the formula for the standard error of the sum of two variables. For example:</span>
<span id="cb55-892"><a href="#cb55-892"></a></span>
<span id="cb55-893"><a href="#cb55-893"></a><span class="ss">* </span>$SE_{\text{augMean}} = \sqrt{SE_{\text{augMean}}^2 + SE_{\text{augMean:bio}}^2}$</span>
<span id="cb55-894"><a href="#cb55-894"></a></span>
<span id="cb55-895"><a href="#cb55-895"></a>**The ANOVA table**</span>
<span id="cb55-896"><a href="#cb55-896"></a></span>
<span id="cb55-897"><a href="#cb55-897"></a>The ANOVA table assesses the overall significance of groups of predictors or the sequential addition of predictors to the model.</span>
<span id="cb55-898"><a href="#cb55-898"></a></span>
<span id="cb55-901"><a href="#cb55-901"></a><span class="in">```{r}</span></span>
<span id="cb55-902"><a href="#cb55-902"></a><span class="fu">anova</span>(mod3)</span>
<span id="cb55-903"><a href="#cb55-903"></a><span class="in">```</span></span>
<span id="cb55-904"><a href="#cb55-904"></a></span>
<span id="cb55-905"><a href="#cb55-905"></a>The ANOVA table shows that the model is highly significant, with very low *p*-values throughout (&lt; 0.0001). This indicates that the model as a whole is a good fit for the data.</span>
<span id="cb55-906"><a href="#cb55-906"></a></span>
<span id="cb55-907"><a href="#cb55-907"></a><span class="fu">### Reporting</span></span>
<span id="cb55-908"><a href="#cb55-908"></a></span>
<span id="cb55-909"><a href="#cb55-909"></a>Here is what the reporting of the findings could look like in the Results section in your favourite journal.</span>
<span id="cb55-910"><a href="#cb55-910"></a></span>
<span id="cb55-911"><a href="#cb55-911"></a>**Results**</span>
<span id="cb55-912"><a href="#cb55-912"></a></span>
<span id="cb55-913"><a href="#cb55-913"></a>A multiple linear regression model examining the effects of the August climatological mean temperature (<span class="in">`augMean`</span>), the August and February climatological SD of temperature (<span class="in">`augSD`</span> and <span class="in">`febSD`</span>, respectively), and the bioregion classification (<span class="in">`bio`</span>) on the response variable, the Sørensen dissimilarity (<span class="in">`Y`</span>), including their interaction terms, revealed several significant findings (@tbl-results). This model allows a separate regression slope for each predictor within the bioregions (@fig-mlr6). The model explains a substantial portion of the variance in <span class="in">`Y`</span> ($R^2 = 0.780$, adjusted $R^2 = 0.776$), and the overall model fit is highly significant ($F(15, 954) = 225.1$, $p &lt; 0.0001$).</span>
<span id="cb55-914"><a href="#cb55-914"></a></span>
<span id="cb55-915"><a href="#cb55-915"></a>| Coefficient                 | Estimate | Std. Error | t value | P-value        |</span>
<span id="cb55-916"><a href="#cb55-916"></a>|-----------------------------|----------|------------|---------|----------------|</span>
<span id="cb55-917"><a href="#cb55-917"></a>| <span class="in">`(Intercept)`</span>               | 0.0299   | 0.0063     | 4.766   | &lt; 0.0001 ***   |</span>
<span id="cb55-918"><a href="#cb55-918"></a>| <span class="in">`augMean`</span>                   | 0.3441   | 0.0159     | 21.700  | &lt; 0.0001 ***   |</span>
<span id="cb55-919"><a href="#cb55-919"></a>| <span class="in">`bioB-ATZ`</span>                  | -0.0460  | 0.0243     | -1.895  | &gt; 0.05         |</span>
<span id="cb55-920"><a href="#cb55-920"></a>| <span class="in">`bioBMP`</span>                    | 0.0161   | 0.0101     | 1.596   | &gt; 0.05         |</span>
<span id="cb55-921"><a href="#cb55-921"></a>| <span class="in">`bioECTZ`</span>                   | -0.0015  | 0.0090     | -0.171  | &gt; 0.05         |</span>
<span id="cb55-922"><a href="#cb55-922"></a>| <span class="in">`augSD`</span>                     | -0.0059  | 0.0034     | -1.735  | &gt; 0.05         |</span>
<span id="cb55-923"><a href="#cb55-923"></a>| <span class="in">`febSD`</span>                     | -0.0006  | 0.0028     | -0.232  | &gt; 0.05         |</span>
<span id="cb55-924"><a href="#cb55-924"></a>| <span class="in">`augMean:bioB-ATZ`</span>          | -0.0462  | 0.0874     | -0.528  | &gt; 0.05         |</span>
<span id="cb55-925"><a href="#cb55-925"></a>| <span class="in">`augMean:bioBMP`</span>            | -0.2406  | 0.0211     | -11.382 | &lt; 0.0005 ***   |</span>
<span id="cb55-926"><a href="#cb55-926"></a>| <span class="in">`augMean:bioECTZ`</span>           | -0.0608  | 0.0189     | -3.215  | &lt; 0.005 **     |</span>
<span id="cb55-927"><a href="#cb55-927"></a>| <span class="in">`bioB-ATZ:augSD`</span>            | 0.0656   | 0.0371     | 1.768   | &gt; 0.05         |</span>
<span id="cb55-928"><a href="#cb55-928"></a>| <span class="in">`bioBMP:augSD`</span>              | 0.0410   | 0.0115     | 3.576   | &lt; 0.0005 ***   |</span>
<span id="cb55-929"><a href="#cb55-929"></a>| <span class="in">`bioECTZ:augSD`</span>             | 0.0281   | 0.0054     | 5.219   | &lt; 0.0005 ***   |</span>
<span id="cb55-930"><a href="#cb55-930"></a>| <span class="in">`bioB-ATZ:febSD`</span>            | 0.0409   | 0.0819     | 0.500   | &gt; 0.05         |</span>
<span id="cb55-931"><a href="#cb55-931"></a>| <span class="in">`bioBMP:febSD`</span>              | 0.0056   | 0.0150     | 0.376   | &gt; 0.05         |</span>
<span id="cb55-932"><a href="#cb55-932"></a>| <span class="in">`bioECTZ:febSD`</span>             | 0.0503   | 0.0082     | 6.113   | &lt; 0.0005 ***   |</span>
<span id="cb55-933"><a href="#cb55-933"></a></span>
<span id="cb55-934"><a href="#cb55-934"></a>: Summary of the multiple linear regression model examining the effects of <span class="in">`augMean`</span>, <span class="in">`augSD`</span>, <span class="in">`febSD`</span>, and <span class="in">`bio`</span> on <span class="in">`Y`</span>. {#tbl-results}</span>
<span id="cb55-935"><a href="#cb55-935"></a></span>
<span id="cb55-936"><a href="#cb55-936"></a>The main effect of <span class="in">`augMean`</span> was highly significant (Estimate = 0.3441, $p &lt; 0.0001$), indicating a strong positive relationship with <span class="in">`Y`</span>. The interaction term <span class="in">`augMean:bioBMP`</span> (Estimate = -0.2406, $p &lt; 0.0001$) and <span class="in">`augMean:bioECTZ`</span> (Estimate = -0.0608, $p &lt; 0.005$) were also significant, suggesting that the effect of <span class="in">`augMean`</span> on <span class="in">`Y`</span> varies significantly for <span class="in">`BMP`</span> and <span class="in">`ECTZ`</span> bioregions compared to the reference category (<span class="in">`AMP`</span>). The <span class="in">`bioBMP`</span> (Estimate = 0.0161, $p &gt; 0.05$) and <span class="in">`bioECTZ`</span> (Estimate = -0.0015, $p &gt; 0.05$) terms were not significant, indicating no significant difference from <span class="in">`AMP`</span>.</span>
<span id="cb55-937"><a href="#cb55-937"></a></span>
<span id="cb55-938"><a href="#cb55-938"></a>For <span class="in">`augSD`</span>, the main effect was not significant (Estimate = -0.0059, $p &gt; 0.05$). Significant interaction terms for <span class="in">`bioBMP:augSD`</span> (Estimate = 0.0410, $p &lt; 0.001$) and <span class="in">`bioECTZ:augSD`</span> (Estimate = 0.0281, $p &lt; 0.0001$) indicate that the effect of <span class="in">`augSD`</span> on <span class="in">`Y`</span> varies by bioregion.</span>
<span id="cb55-939"><a href="#cb55-939"></a></span>
<span id="cb55-940"><a href="#cb55-940"></a>The main effect of <span class="in">`febSD`</span> was not significant (Estimate = -0.0006, $p &gt; 0.05$), suggesting no direct relationship with <span class="in">`Y`</span>. However, the interaction term <span class="in">`bioECTZ:febSD`</span> (Estimate = 0.0503, $p = 0.0001$) was significant, indicating that the effect of <span class="in">`febSD`</span> on <span class="in">`Y`</span> differs for the <span class="in">`ECTZ`</span> bioregion.</span>
<span id="cb55-941"><a href="#cb55-941"></a></span>
<span id="cb55-942"><a href="#cb55-942"></a>The ANOVA further highlights the overall significance of each predictor. <span class="in">`augMean`</span> had a highly significant contribution to the model ($F = 2676.902$, $p &lt; 0.0001$), as did <span class="in">`bio`</span> ($F = 106.296$, $p &lt; 0.0001$), and their interactions (<span class="in">`augMean:bio`</span>, $F = 70.647$, $p &lt; 0.0001$; <span class="in">`bio:augSD`</span>, $F = 30.602$, $p &lt; 0.0001$; <span class="in">`bio:febSD`</span>, $F = 12.517$, $p = 4.953 \times 10^{-8}$). The main effect of <span class="in">`augSD`</span> was also significant ($F = 37.331$, $p = 1.451 \times 10^{-9}$), while <span class="in">`febSD`</span> did not significantly contribute to the model on its own ($F = 1.422$, $p = 0.2334$).</span>
<span id="cb55-943"><a href="#cb55-943"></a></span>
<span id="cb55-944"><a href="#cb55-944"></a>These findings suggest that the effects of <span class="in">`augMean`</span>, <span class="in">`augSD`</span>, and <span class="in">`febSD`</span> on <span class="in">`Y`</span> are influenced by the bioregional classification, with significant variations in the relationships depending on the specific bioregion.</span>
<span id="cb55-945"><a href="#cb55-945"></a></span>
<span id="cb55-946"><a href="#cb55-946"></a></span>
<span id="cb55-949"><a href="#cb55-949"></a><span class="in">```{r}</span></span>
<span id="cb55-950"><a href="#cb55-950"></a><span class="co">#| echo: false</span></span>
<span id="cb55-951"><a href="#cb55-951"></a><span class="co">#| fig-width: 5</span></span>
<span id="cb55-952"><a href="#cb55-952"></a><span class="co">#| fig-height: 11</span></span>
<span id="cb55-953"><a href="#cb55-953"></a><span class="co">#| fig.cap: "Individual linear regression fit to the variables `augMean`, `febSD`, and `augSD` for each bioregion as predictors of the seaweed species composition."</span></span>
<span id="cb55-954"><a href="#cb55-954"></a><span class="co">#| label: fig-mlr6</span></span>
<span id="cb55-955"><a href="#cb55-955"></a></span>
<span id="cb55-956"><a href="#cb55-956"></a><span class="co"># Create a function to generate interaction plots for each predictor</span></span>
<span id="cb55-957"><a href="#cb55-957"></a>plot_interaction <span class="ot">&lt;-</span> <span class="cf">function</span>(data, predictor, response, category) {</span>
<span id="cb55-958"><a href="#cb55-958"></a>  <span class="fu">ggplot</span>(data, <span class="fu">aes_string</span>(<span class="at">x =</span> predictor, <span class="at">y =</span> response,</span>
<span id="cb55-959"><a href="#cb55-959"></a>                          <span class="at">colour =</span> category)) <span class="sc">+</span></span>
<span id="cb55-960"><a href="#cb55-960"></a>    <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb55-961"><a href="#cb55-961"></a>    <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>,</span>
<span id="cb55-962"><a href="#cb55-962"></a>                <span class="at">formula =</span> y <span class="sc">~</span> x, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">linewidth =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb55-963"><a href="#cb55-963"></a>    <span class="fu">scale_colour_brewer</span>(<span class="at">palette =</span> <span class="st">"Set1"</span>) <span class="sc">+</span></span>
<span id="cb55-964"><a href="#cb55-964"></a>    <span class="fu">facet_wrap</span>(<span class="sc">~</span>bio, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb55-965"><a href="#cb55-965"></a>    <span class="fu">labs</span>(<span class="at">x =</span> predictor,</span>
<span id="cb55-966"><a href="#cb55-966"></a>         <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(beta[sim]))) <span class="sc">+</span></span>
<span id="cb55-967"><a href="#cb55-967"></a>    <span class="fu">theme_bw</span>()</span>
<span id="cb55-968"><a href="#cb55-968"></a>}</span>
<span id="cb55-969"><a href="#cb55-969"></a></span>
<span id="cb55-970"><a href="#cb55-970"></a><span class="co"># List of continuous predictors</span></span>
<span id="cb55-971"><a href="#cb55-971"></a>predictors <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"augMean"</span>, <span class="st">"febSD"</span>, <span class="st">"augSD"</span>)</span>
<span id="cb55-972"><a href="#cb55-972"></a></span>
<span id="cb55-973"><a href="#cb55-973"></a><span class="co"># Generate and print interaction plots for each predictor</span></span>
<span id="cb55-974"><a href="#cb55-974"></a>plts5 <span class="ot">&lt;-</span> <span class="fu">map</span>(predictors, plot_interaction, <span class="at">data =</span> sw,</span>
<span id="cb55-975"><a href="#cb55-975"></a>             <span class="at">response =</span> <span class="st">"Y"</span>, <span class="at">category =</span> <span class="st">"bio"</span>)</span>
<span id="cb55-976"><a href="#cb55-976"></a></span>
<span id="cb55-977"><a href="#cb55-977"></a><span class="co"># Arrange the interaction plots in a grid</span></span>
<span id="cb55-978"><a href="#cb55-978"></a><span class="fu">ggarrange</span>(<span class="at">plotlist =</span> plts5, <span class="at">ncol =</span> <span class="dv">1</span>, <span class="at">labels =</span> <span class="st">"AUTO"</span>,</span>
<span id="cb55-979"><a href="#cb55-979"></a>          <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">common.legend =</span> <span class="cn">TRUE</span>)</span>
<span id="cb55-980"><a href="#cb55-980"></a><span class="in">```</span></span>
<span id="cb55-981"><a href="#cb55-981"></a></span>
<span id="cb55-982"><a href="#cb55-982"></a><span class="fu">## Alternative Categorical Variable Coding Schemes (Contrasts) {#sec-contrasts}</span></span>
<span id="cb55-983"><a href="#cb55-983"></a></span>
<span id="cb55-984"><a href="#cb55-984"></a>Throughout the book, we have used dummy variable coding the specify the categorical variables in the multiple linear regression models. But, should dummy variable coding not be to your liking, there are other coding schemes that can be used to represent the categorical variables. These alternative coding schemes are known as contrasts. The choice of contrast coding can affect the interpretation of the regression coefficients. </span>
<span id="cb55-985"><a href="#cb55-985"></a></span>
<span id="cb55-986"><a href="#cb55-986"></a>I'll provide some synthetic data to illustrate a few different contrasts. The data consist of a continuous variable <span class="in">`x`</span>, a categorical variable <span class="in">`cat_var`</span> with four levels, and a response variable <span class="in">`y`</span> that has some relationship with <span class="in">`x`</span> and <span class="in">`cat_var`</span>. I'll use dummy variable coding as the reference (haha!).</span>
<span id="cb55-987"><a href="#cb55-987"></a></span>
<span id="cb55-990"><a href="#cb55-990"></a><span class="in">```{r}</span></span>
<span id="cb55-991"><a href="#cb55-991"></a><span class="co">#| echo: false</span></span>
<span id="cb55-992"><a href="#cb55-992"></a></span>
<span id="cb55-993"><a href="#cb55-993"></a><span class="co"># Generate artificial data</span></span>
<span id="cb55-994"><a href="#cb55-994"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)  <span class="co"># For reproducibility</span></span>
<span id="cb55-995"><a href="#cb55-995"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># Sample size</span></span>
<span id="cb55-996"><a href="#cb55-996"></a></span>
<span id="cb55-997"><a href="#cb55-997"></a><span class="co"># Generate continuous variable</span></span>
<span id="cb55-998"><a href="#cb55-998"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb55-999"><a href="#cb55-999"></a></span>
<span id="cb55-1000"><a href="#cb55-1000"></a><span class="co"># Generate categorical variable with 4 levels</span></span>
<span id="cb55-1001"><a href="#cb55-1001"></a>cat_var <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">"A"</span>, <span class="st">"B"</span>, <span class="st">"C"</span>, <span class="st">"D"</span>), n, <span class="at">replace =</span> <span class="cn">TRUE</span>))</span>
<span id="cb55-1002"><a href="#cb55-1002"></a></span>
<span id="cb55-1003"><a href="#cb55-1003"></a><span class="co"># Generate response variable with some relationship to cat_var and x</span></span>
<span id="cb55-1004"><a href="#cb55-1004"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">ifelse</span>(cat_var <span class="sc">==</span> <span class="st">"A"</span>, <span class="dv">3</span>, <span class="fu">ifelse</span>(cat_var <span class="sc">==</span> <span class="st">"B"</span>, <span class="dv">1</span>, <span class="fu">ifelse</span>(cat_var <span class="sc">==</span> <span class="st">"C"</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">3</span>))) <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb55-1005"><a href="#cb55-1005"></a></span>
<span id="cb55-1006"><a href="#cb55-1006"></a><span class="co"># Combine into a data frame</span></span>
<span id="cb55-1007"><a href="#cb55-1007"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x, cat_var)</span>
<span id="cb55-1008"><a href="#cb55-1008"></a><span class="in">```</span></span>
<span id="cb55-1009"><a href="#cb55-1009"></a></span>
<span id="cb55-1012"><a href="#cb55-1012"></a><span class="in">```{r}</span></span>
<span id="cb55-1013"><a href="#cb55-1013"></a><span class="fu">head</span>(data)</span>
<span id="cb55-1014"><a href="#cb55-1014"></a><span class="in">```</span></span>
<span id="cb55-1015"><a href="#cb55-1015"></a></span>
<span id="cb55-1016"><a href="#cb55-1016"></a>Categorical variable coding (any scheme) only affects the interpretation of the categorical variable main effects and their interactions, so I'll not discuss the coefficient associated with the continuous variable <span class="in">`x`</span> (the slope) in the model throughout the explanations offered below. </span>
<span id="cb55-1017"><a href="#cb55-1017"></a></span>
<span id="cb55-1018"><a href="#cb55-1018"></a>**Dummy Variable Coding (Treatment Contrasts)**</span>
<span id="cb55-1019"><a href="#cb55-1019"></a></span>
<span id="cb55-1020"><a href="#cb55-1020"></a>This is the most commonly used coding scheme, and <span class="in">`lm()`</span>'s default. One level is the reference category (<span class="in">`A`</span>) and the other levels are compared against it. Contrast matrices can be assigned and/or inspected using the <span class="in">`contrasts()`</span> function. For the dummy coding, the reference level <span class="in">`A`</span> will remain 0 and the other levels will be independently coded as 1 in three columns. You'll now understand why, when we have four levels within a categorical variable, we only need three dummy variables to represent them.</span>
<span id="cb55-1021"><a href="#cb55-1021"></a></span>
<span id="cb55-1024"><a href="#cb55-1024"></a><span class="in">```{r}</span></span>
<span id="cb55-1025"><a href="#cb55-1025"></a><span class="co"># Dummy coding (treatment coding) ... default</span></span>
<span id="cb55-1026"><a href="#cb55-1026"></a><span class="fu">contrasts</span>(data<span class="sc">$</span>cat_var)</span>
<span id="cb55-1027"><a href="#cb55-1027"></a><span class="in">```</span></span>
<span id="cb55-1028"><a href="#cb55-1028"></a>When we have four levels in a categorical variable, there are three dummy variable columns in the contrast matrix. The first row, consisting of all zeros (0, 0, 0), represents the reference level, which in this case is <span class="in">`A`</span>. The other rows represent the different levels of the categorical variable, with a 1 in the respective column indicating that level. For example, level <span class="in">`A`</span> is represented by (0, 0, 0), <span class="in">`B`</span> by (1, 0, 0), <span class="in">`C`</span> by (0, 1, 0), and <span class="in">`D`</span> by (0, 0, 1). In the regression model, these contrasts are used to estimate the differences between each level and the reference level. Specifically, the first contrast column indicates that the coefficient for this column will represent the difference between the mean of the response variable for level <span class="in">`B`</span> and the mean for the reference level <span class="in">`A`</span>, holding all other variables constant. Similarly, the second and third columns represent the differences between levels <span class="in">`C`</span> and <span class="in">`A`</span>, and <span class="in">`D`</span> and <span class="in">`A`</span>, respectively. This coding allows for a straightforward interpretation of how each level of the categorical variable affects the response variable relative to the reference level.</span>
<span id="cb55-1029"><a href="#cb55-1029"></a></span>
<span id="cb55-1032"><a href="#cb55-1032"></a><span class="in">```{r}</span></span>
<span id="cb55-1033"><a href="#cb55-1033"></a>model_dummy <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> cat_var, <span class="at">data =</span> data)</span>
<span id="cb55-1034"><a href="#cb55-1034"></a><span class="fu">summary</span>(model_dummy)</span>
<span id="cb55-1035"><a href="#cb55-1035"></a><span class="in">```</span></span>
<span id="cb55-1036"><a href="#cb55-1036"></a></span>
<span id="cb55-1037"><a href="#cb55-1037"></a>The model summary shows that the coefficients for <span class="in">`cat_varB`</span>, <span class="in">`cat_varC`</span>, and <span class="in">`cat_varD`</span> represent the differences in the mean of the response variable <span class="in">`y`</span> between the reference category <span class="in">`A`</span> and categories <span class="in">`B`</span>, <span class="in">`C`</span>, and <span class="in">`D`</span>, respectively, while controlling for the effect of the continuous variable <span class="in">`x`</span>.</span>
<span id="cb55-1038"><a href="#cb55-1038"></a></span>
<span id="cb55-1039"><a href="#cb55-1039"></a>Interpretation:</span>
<span id="cb55-1040"><a href="#cb55-1040"></a></span>
<span id="cb55-1041"><a href="#cb55-1041"></a><span class="ss">*   </span><span class="in">`(Intercept)`</span> (2.8176): The intercept represents the estimated mean value of the response (<span class="in">`y`</span>) when <span class="in">`x`</span> is zero and the categorical variable is at the reference level <span class="in">`A`</span>. This is the baseline from which other categories are compared.</span>
<span id="cb55-1042"><a href="#cb55-1042"></a><span class="ss">*   </span><span class="in">`x`</span> (1.8274): For each one-unit increase in <span class="in">`x`</span>, <span class="in">`y`</span> is expected to increase by 1.8274 units, holding the categorical variable constant. This effect is consistent across all levels of the categorical variable because the model does not have an interaction effect present.</span>
<span id="cb55-1043"><a href="#cb55-1043"></a><span class="ss">*   </span><span class="in">`cat_varB`</span> (-1.7201): On average, the value of <span class="in">`y`</span> for level <span class="in">`B`</span> is 1.7201 units lower than that for the reference level <span class="in">`A`</span>, when <span class="in">`x`</span> is held constant. This corresponds to the (1, 0, 0) row in the contrast matrix.</span>
<span id="cb55-1044"><a href="#cb55-1044"></a><span class="ss">*   </span><span class="in">`cat_varC`</span> (-3.9056): Similarly, on average, the value of <span class="in">`y`</span> for level <span class="in">`C`</span> is 3.9056 units lower than that for the reference level, when <span class="in">`x`</span> is held constant. This corresponds to the (0, 1, 0) row in the contrast matrix.</span>
<span id="cb55-1045"><a href="#cb55-1045"></a><span class="ss">*   </span><span class="in">`cat_varD`</span> (-5.4880): Lastly, on average, the value of <span class="in">`y`</span> for level <span class="in">`D`</span> is 5.4880 units lower compared to the reference , when <span class="in">`x`</span> is held constant. This is row (0, 0, 1) row in the contrast matrix.</span>
<span id="cb55-1046"><a href="#cb55-1046"></a></span>
<span id="cb55-1047"><a href="#cb55-1047"></a>All these coefficients are highly significant (*p* &lt; 0.0001), indicating strong evidence for differences between each category and the reference category <span class="in">`A`</span>.</span>
<span id="cb55-1048"><a href="#cb55-1048"></a></span>
<span id="cb55-1049"><a href="#cb55-1049"></a>The model explains a large proportion of the variance in <span class="in">`y`</span> (Adjusted *R*-squared: 0.8822), suggesting a good fit. The *F*-statistic (186.4) with a very low *p*-value (&lt; 0.0001) indicates that the model as a whole is statistically significant.</span>
<span id="cb55-1050"><a href="#cb55-1050"></a></span>
<span id="cb55-1051"><a href="#cb55-1051"></a>If you want to change the reference level, you can use the <span class="in">`relevel()`</span> function. For example, to change the reference level of <span class="in">`cat_var`</span> variable to <span class="in">`C_2`</span>, you can use:</span>
<span id="cb55-1052"><a href="#cb55-1052"></a></span>
<span id="cb55-1055"><a href="#cb55-1055"></a><span class="in">```{r}</span></span>
<span id="cb55-1056"><a href="#cb55-1056"></a><span class="co"># Set "C" as the reference level for cat_var</span></span>
<span id="cb55-1057"><a href="#cb55-1057"></a>data<span class="sc">$</span>cat_var <span class="ot">&lt;-</span> <span class="fu">relevel</span>(data<span class="sc">$</span>cat_var, <span class="at">ref =</span> <span class="st">"C"</span>)</span>
<span id="cb55-1058"><a href="#cb55-1058"></a><span class="fu">contrasts</span>(data<span class="sc">$</span>cat_var)</span>
<span id="cb55-1059"><a href="#cb55-1059"></a><span class="in">```</span></span>
<span id="cb55-1060"><a href="#cb55-1060"></a></span>
<span id="cb55-1061"><a href="#cb55-1061"></a>This may be useful when you want to compare the other levels to a different reference level.</span>
<span id="cb55-1062"><a href="#cb55-1062"></a></span>
<span id="cb55-1063"><a href="#cb55-1063"></a>**Effect Coding (Sum Contrasts)**</span>
<span id="cb55-1064"><a href="#cb55-1064"></a></span>
<span id="cb55-1065"><a href="#cb55-1065"></a>This coding method compares the levels of a categorical variable to the overall mean of the dependent variable. The coefficients represent the difference between each level and the grand mean. Instead of using 0 and 1 as we did with dummy variable coding, effect coding uses -1, 0, and 1 to represent the different levels of the categorical variable.</span>
<span id="cb55-1066"><a href="#cb55-1066"></a></span>
<span id="cb55-1069"><a href="#cb55-1069"></a><span class="in">```{r}</span></span>
<span id="cb55-1070"><a href="#cb55-1070"></a><span class="co"># Reset the reference level to "A"</span></span>
<span id="cb55-1071"><a href="#cb55-1071"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x, cat_var)</span>
<span id="cb55-1072"><a href="#cb55-1072"></a></span>
<span id="cb55-1073"><a href="#cb55-1073"></a><span class="co"># Effect coding</span></span>
<span id="cb55-1074"><a href="#cb55-1074"></a><span class="fu">contrasts</span>(data<span class="sc">$</span>cat_var) <span class="ot">&lt;-</span> <span class="fu">contr.sum</span>(<span class="dv">4</span>)</span>
<span id="cb55-1075"><a href="#cb55-1075"></a><span class="fu">contrasts</span>(data<span class="sc">$</span>cat_var)</span>
<span id="cb55-1076"><a href="#cb55-1076"></a><span class="in">```</span></span>
<span id="cb55-1077"><a href="#cb55-1077"></a></span>
<span id="cb55-1078"><a href="#cb55-1078"></a>In effect coding (sum contrasts), each level of the categorical variable is compared to the overall mean rather than a specific reference category. This contrast matrix with four levels (A, B, C, D) and three columns can be interpreted as follows:</span>
<span id="cb55-1079"><a href="#cb55-1079"></a></span>
<span id="cb55-1080"><a href="#cb55-1080"></a><span class="ss">*   </span>Level <span class="in">`A`</span> (1, 0, 0): The first row indicates that level <span class="in">`A`</span> is included in the first contrast (<span class="in">`cat_var1`</span>), which means the mean of level <span class="in">`A`</span> is being compared to the overall mean. Since the other columns are zero, level <span class="in">`A`</span> does not contribute to the other contrasts.</span>
<span id="cb55-1081"><a href="#cb55-1081"></a><span class="ss">*   </span>Level <span class="in">`B`</span> (0, 1, 0): The second row indicates that level <span class="in">`B`</span> is included in the second contrast (<span class="in">`cat_var2`</span>). The mean of level <span class="in">`B`</span> is being compared to the overall mean, and it does not contribute to the other contrasts.</span>
<span id="cb55-1082"><a href="#cb55-1082"></a><span class="ss">*   </span>Level <span class="in">`C`</span> (0, 0, 1): The third row indicates that level <span class="in">`C`</span> is included in the third contrast (<span class="in">`cat_var3`</span>). The mean of level C is being compared to the overall mean, and it does not contribute to the other contrasts.</span>
<span id="cb55-1083"><a href="#cb55-1083"></a><span class="ss">*   </span>Level <span class="in">`D`</span> (-1, -1, -1): The fourth row is a balancing row, ensuring that the sum of the contrasts for each level equals zero. This indicates that level D is being compared to the overall mean indirectly by balancing the contributions of levels <span class="in">`A`</span>, <span class="in">`B`</span>, and <span class="in">`C`</span>.</span>
<span id="cb55-1084"><a href="#cb55-1084"></a></span>
<span id="cb55-1087"><a href="#cb55-1087"></a><span class="in">```{r}</span></span>
<span id="cb55-1088"><a href="#cb55-1088"></a>model_effect <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> cat_var, <span class="at">data =</span> data)</span>
<span id="cb55-1089"><a href="#cb55-1089"></a><span class="fu">summary</span>(model_effect)</span>
<span id="cb55-1090"><a href="#cb55-1090"></a><span class="in">```</span></span>
<span id="cb55-1091"><a href="#cb55-1091"></a></span>
<span id="cb55-1092"><a href="#cb55-1092"></a>Interpretation:</span>
<span id="cb55-1093"><a href="#cb55-1093"></a></span>
<span id="cb55-1094"><a href="#cb55-1094"></a><span class="ss">*   </span><span class="in">`(Intercept)`</span> 0.03921: The intercept represents the grand mean of the response variable (<span class="in">`y`</span>). Since the intercept is not statistically significant (*p* &gt; 0.05), it indicates that the overall mean is not significantly different from zero when considering the average effect of all levels of the categorical variable.</span>
<span id="cb55-1095"><a href="#cb55-1095"></a><span class="ss">*   </span><span class="in">`x`</span> (1.82741): For each one-unit increase in (<span class="in">`x`</span>), the response (<span class="in">`y`</span>) increases by approximately 1.82741 units. This effect is highly significant (*p* &lt; 0.0001).</span>
<span id="cb55-1096"><a href="#cb55-1096"></a><span class="ss">*   </span><span class="in">`cat_var1`</span> (2.77844): Level <span class="in">`A`</span> has a mean (<span class="in">`y`</span>) that is 2.77844 units higher than the grand mean. This effect is highly significant (*p* &lt; 0.0001).</span>
<span id="cb55-1097"><a href="#cb55-1097"></a><span class="ss">*   </span><span class="in">`cat_var2`</span> (1.05832): Level <span class="in">`B`</span> has a mean (<span class="in">`y`</span>) that is 1.05832 units higher than the grand mean. This effect is also highly significant (*p* &lt; 0.0001).</span>
<span id="cb55-1098"><a href="#cb55-1098"></a><span class="ss">*   </span><span class="in">`cat_var3`</span> (-1.12720): Level <span class="in">`C`</span> has a mean (<span class="in">`y`</span>) that is 1.12720 units lower than the grand mean. This effect is highly significant (p &lt; 0.0001).</span>
<span id="cb55-1099"><a href="#cb55-1099"></a></span>
<span id="cb55-1100"><a href="#cb55-1100"></a>All these coefficients are highly significant (*p* &lt; 0.0001), indicating strong evidence for differences between each category and the overall mean of all levels.</span>
<span id="cb55-1101"><a href="#cb55-1101"></a></span>
<span id="cb55-1102"><a href="#cb55-1102"></a>The model explains a large proportion of the variance in <span class="in">`y`</span> (Adjusted *R*-squared: 0.8822), suggesting a good fit. The *F*-statistic (186.4) with a very low *p*-value (&lt; 0.0001) indicates that the model as a whole is statistically significant.</span>
<span id="cb55-1103"><a href="#cb55-1103"></a></span>
<span id="cb55-1104"><a href="#cb55-1104"></a>**Helmert Coding**</span>
<span id="cb55-1105"><a href="#cb55-1105"></a></span>
<span id="cb55-1106"><a href="#cb55-1106"></a>Helmert coding compares each level of a categorical variable to the mean of the subsequent levels. It is useful for testing ordered differences. </span>
<span id="cb55-1107"><a href="#cb55-1107"></a></span>
<span id="cb55-1110"><a href="#cb55-1110"></a><span class="in">```{r}</span></span>
<span id="cb55-1111"><a href="#cb55-1111"></a><span class="co"># Helmert coding</span></span>
<span id="cb55-1112"><a href="#cb55-1112"></a><span class="fu">contrasts</span>(data<span class="sc">$</span>cat_var) <span class="ot">&lt;-</span> <span class="fu">contr.helmert</span>(<span class="dv">4</span>)</span>
<span id="cb55-1113"><a href="#cb55-1113"></a><span class="fu">contrasts</span>(data<span class="sc">$</span>cat_var)</span>
<span id="cb55-1114"><a href="#cb55-1114"></a><span class="in">```</span></span>
<span id="cb55-1115"><a href="#cb55-1115"></a></span>
<span id="cb55-1116"><a href="#cb55-1116"></a>The contrast matrix for a categorical variable with four levels (<span class="in">`A`</span>, <span class="in">`B`</span>, <span class="in">`C`</span>, <span class="in">`D`</span>) and three columns can be interpreted as follows:</span>
<span id="cb55-1117"><a href="#cb55-1117"></a></span>
<span id="cb55-1118"><a href="#cb55-1118"></a><span class="ss">*   </span>Level <span class="in">`A`</span> (-1, -1, -1): Level <span class="in">`A`</span> is compared to the mean of levels <span class="in">`B`</span>, <span class="in">`C`</span>, and <span class="in">`D`</span>. The negative values indicate that level A is being subtracted in these comparisons.</span>
<span id="cb55-1119"><a href="#cb55-1119"></a><span class="ss">*   </span>Level <span class="in">`B`</span> (1, -1, -1): Level <span class="in">`B`</span> is compared to the mean of levels <span class="in">`C`</span> and <span class="in">`D`</span>. The positive value in the first column indicates that level <span class="in">`B`</span> is being added in this comparison.</span>
<span id="cb55-1120"><a href="#cb55-1120"></a><span class="ss">*   </span>Level <span class="in">`C`</span> (0, 2, -1): Level <span class="in">`C`</span> is compared to the mean of level <span class="in">`D`</span>. The positive value in the second column indicates that level <span class="in">`C`</span> is being added in this comparison, while the negative value in the third column is part of the comparison for subsequent levels.</span>
<span id="cb55-1121"><a href="#cb55-1121"></a><span class="ss">*   </span>Level <span class="in">`D`</span> (0, 0, 3): Level <span class="in">`D`</span> is compared on its own in the final contrast. The positive value in the third column indicates that level <span class="in">`D`</span> is being added in this comparison.</span>
<span id="cb55-1122"><a href="#cb55-1122"></a></span>
<span id="cb55-1125"><a href="#cb55-1125"></a><span class="in">```{r}</span></span>
<span id="cb55-1126"><a href="#cb55-1126"></a>model_helmert <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> cat_var, <span class="at">data =</span> data)</span>
<span id="cb55-1127"><a href="#cb55-1127"></a><span class="fu">summary</span>(model_helmert)</span>
<span id="cb55-1128"><a href="#cb55-1128"></a><span class="in">```</span></span>
<span id="cb55-1129"><a href="#cb55-1129"></a></span>
<span id="cb55-1130"><a href="#cb55-1130"></a>Interpretation:</span>
<span id="cb55-1131"><a href="#cb55-1131"></a></span>
<span id="cb55-1132"><a href="#cb55-1132"></a><span class="ss">*   </span><span class="in">`(Intercept)`</span> (0.03921): The grand mean of  <span class="in">`y`</span>  when  <span class="in">`x`</span>  is zero.</span>
<span id="cb55-1133"><a href="#cb55-1133"></a><span class="ss">*   </span><span class="in">`x`</span> (1.82741): For each unit increase in  x ,  y  increases by 1.82741 units.</span>
<span id="cb55-1134"><a href="#cb55-1134"></a><span class="ss">*   </span><span class="in">`cat_var1`</span> (-0.86006): The mean of level <span class="in">`A`</span> is 0.86006 units lower than the combined mean of levels <span class="in">`B`</span>, <span class="in">`C`</span>, and <span class="in">`D`</span>.</span>
<span id="cb55-1135"><a href="#cb55-1135"></a><span class="ss">*   </span><span class="in">`cat_var2`</span> (-1.01519): The mean of level <span class="in">`B`</span> is 1.01519 units lower than the combined mean of levels <span class="in">`C`</span> and <span class="in">`D`</span>.</span>
<span id="cb55-1136"><a href="#cb55-1136"></a><span class="ss">*   </span><span class="in">`cat_var3`</span> (-0.90319): The mean of level <span class="in">`C`</span> is 0.90319 units lower than the mean of level <span class="in">`D`</span>.</span>
<span id="cb55-1137"><a href="#cb55-1137"></a></span>
<span id="cb55-1138"><a href="#cb55-1138"></a>The interpretation of the overall model remains more-or-less similar to before:</span>
<span id="cb55-1139"><a href="#cb55-1139"></a></span>
<span id="cb55-1140"><a href="#cb55-1140"></a>All these coefficients are highly significant (*p* &lt; 0.0001), indicating strong evidence for differences between each level and the overall mean of all subsequent levels.</span>
<span id="cb55-1141"><a href="#cb55-1141"></a></span>
<span id="cb55-1142"><a href="#cb55-1142"></a>The model explains a large proportion of the variance in <span class="in">`y`</span> (Adjusted *R*-squared: 0.8822), suggesting a good fit. The *F*-statistic (186.4) with a very low *p*-value (&lt; 0.0001) indicates that the model as a whole is statistically significant.</span>
<span id="cb55-1143"><a href="#cb55-1143"></a></span>
<span id="cb55-1144"><a href="#cb55-1144"></a><span class="fu">## Exercises</span></span>
<span id="cb55-1145"><a href="#cb55-1145"></a></span>
<span id="cb55-1146"><a href="#cb55-1146"></a>::: callout-important</span>
<span id="cb55-1147"><a href="#cb55-1147"></a><span class="fu">## Task G</span></span>
<span id="cb55-1148"><a href="#cb55-1148"></a>Use the data loaded at the start of this chapter for this task.</span>
<span id="cb55-1149"><a href="#cb55-1149"></a></span>
<span id="cb55-1150"><a href="#cb55-1150"></a>In this task you will develop data analysis, undertake model building, and provide an interpretation of the findings. Your goal is to explore the species composition and assembly processes of the seaweed flora around the coast of South Africa. See @smit2017seaweeds for more information about the data and the analysis.</span>
<span id="cb55-1151"><a href="#cb55-1151"></a></span>
<span id="cb55-1152"><a href="#cb55-1152"></a>a. **Analysis:** Please develop multiple linear regression models for the seaweed species composition ($\beta_\text{sim}$ and $\beta_\text{sne}$, i.e. columns called <span class="in">`Y1`</span> and <span class="in">`Y2`</span>, respectively) using the all the predictors in this dataset. At the end, the final model(s) that best describe(s) the species assembly processes operating along the South African coast should be presented. The final model may/may not contain all the predictors in the dataset, and it is your goal to justify the variable and model selection.</span>
<span id="cb55-1153"><a href="#cb55-1153"></a></span>
<span id="cb55-1154"><a href="#cb55-1154"></a><span class="in">    - Accomplishing a) will require that you work through the whole model-building process as outlined in the chapter. This includes the following steps:</span></span>
<span id="cb55-1155"><a href="#cb55-1155"></a><span class="in">        - Data exploration and visualisation (EDA)</span></span>
<span id="cb55-1156"><a href="#cb55-1156"></a><span class="in">        - Model building (providing hypothesis statements, variable selection using VIF and forward selection, comparisons of nested models, justifications for model selection)</span></span>
<span id="cb55-1157"><a href="#cb55-1157"></a><span class="in">        - Model diagnostics</span></span>
<span id="cb55-1158"><a href="#cb55-1158"></a><span class="in">        - Explanation of `summary()` and `anova()` outputs</span></span>
<span id="cb55-1159"><a href="#cb55-1159"></a><span class="in">        - Producing the Results section</span></span>
<span id="cb55-1160"><a href="#cb55-1160"></a><span class="in">        - **[60%]**</span></span>
<span id="cb55-1161"><a href="#cb55-1161"></a></span>
<span id="cb55-1162"><a href="#cb55-1162"></a>b. **Interpretation:** Once you have arrived at the best model, discuss your findings in the light of the appropriate ecological hypotheses that explain the relationships between the predictors and the seaweed species composition. Include insights drawn from the analysis of $\beta_\text{sør}$ that I developed in this chapter, and also rely on the theory you have developed for the lecture material the class presented in Task A2.</span>
<span id="cb55-1163"><a href="#cb55-1163"></a></span>
<span id="cb55-1164"><a href="#cb55-1164"></a><span class="in">    - Accomplishing b) is thus all about model interpretation and discussing the ecological relevance of the results.</span></span>
<span id="cb55-1165"><a href="#cb55-1165"></a><span class="in">    - **[40%]**</span></span>
<span id="cb55-1166"><a href="#cb55-1166"></a><span class="in">    </span></span>
<span id="cb55-1167"><a href="#cb55-1167"></a>The format of this task is a Quarto file that will be converted to an HTML file. The HTML file will contain the graphs, all calculations, and the text sections. The task should be written up as a publication (i.e. use appropriate headings) using a journal style of your choice. Aside from this, there are no limitations.</span>
<span id="cb55-1168"><a href="#cb55-1168"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ajsmit/BCB_Stats/edit/main/multiple_linear_regression.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer><script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>