<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.4">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Albertus J. Smit">
<title>
&lt;span id="sec-regularisation" class="quarto-section-identifier"&gt;&lt;span class="chapter-number"&gt;8&lt;/span&gt;&nbsp; &lt;span class="chapter-title"&gt;Regularisation Techniques&lt;/span&gt;&lt;/span&gt; | Albertus J. Smit – The Biostatistics Book</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./part_B.html" rel="next">
<link href="./non-linear_regression.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script><script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script><script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script><script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./logo.png" alt="" class="navbar-logo"></a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">The Biostatistics Book</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/ajsmit/BCB_Stats" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item quarto-navbar-tools-item" href="./The-Biostatistics-Book.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="./The-Biostatistics-Book.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
</div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-1">
<li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
</div>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part_A.html">Parametric Methods</a></li><li class="breadcrumb-item"><a href="./regularisation.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regularisation Techniques</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part_A.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parametric Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Correlation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./simple_linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./polynomial_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Polynomial Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multiple_linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multiple Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./generalised_linear_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Generalised Linear Models (GLM)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./non-linear_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonlinear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regularisation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regularisation Techniques</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part_B.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Non-Parametric Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./assumption_tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Testing Assumptions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quantile_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Quantile Regression</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part_C.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Semi-Parametric Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./generalised_additive_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Generalised Additive Models</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Appendix A</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
<li><a href="#ridge-regression-l2-regularisation" id="toc-ridge-regression-l2-regularisation" class="nav-link active" data-scroll-target="#ridge-regression-l2-regularisation"><span class="header-section-number">8.1</span> Ridge Regression (L2 Regularisation)</a></li>
  <li><a href="#lasso-regression-l1-regularisation" id="toc-lasso-regression-l1-regularisation" class="nav-link" data-scroll-target="#lasso-regression-l1-regularisation"><span class="header-section-number">8.2</span> Lasso Regression (L1 Regularisation)</a></li>
  <li><a href="#elastic-net-regression" id="toc-elastic-net-regression" class="nav-link" data-scroll-target="#elastic-net-regression"><span class="header-section-number">8.3</span> Elastic Net Regression</a></li>
  <li><a href="#sec-cross-validation" id="toc-sec-cross-validation" class="nav-link" data-scroll-target="#sec-cross-validation"><span class="header-section-number">8.4</span> Cross-Validation</a></li>
  <li><a href="#sec-r-function" id="toc-sec-r-function" class="nav-link" data-scroll-target="#sec-r-function"><span class="header-section-number">8.5</span> R Function</a></li>
  <li><a href="#example-1-ridge-regression" id="toc-example-1-ridge-regression" class="nav-link" data-scroll-target="#example-1-ridge-regression"><span class="header-section-number">8.6</span> Example 1: Ridge Regression</a></li>
  <li><a href="#example-2-lasso-regression" id="toc-example-2-lasso-regression" class="nav-link" data-scroll-target="#example-2-lasso-regression"><span class="header-section-number">8.7</span> Example 2: Lasso Regression</a></li>
  <li><a href="#example-3-elastic-net-regression" id="toc-example-3-elastic-net-regression" class="nav-link" data-scroll-target="#example-3-elastic-net-regression"><span class="header-section-number">8.8</span> Example 3: Elastic Net Regression</a></li>
  <li><a href="#theory-driven-and-data-driven-variable-selection" id="toc-theory-driven-and-data-driven-variable-selection" class="nav-link" data-scroll-target="#theory-driven-and-data-driven-variable-selection"><span class="header-section-number">8.9</span> Theory-Driven and Data-Driven Variable Selection</a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ajsmit/BCB_Stats/edit/main/regularisation.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./part_A.html">Parametric Methods</a></li><li class="breadcrumb-item"><a href="./regularisation.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regularisation Techniques</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-regularisation" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Regularisation Techniques</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://tangledbank.netlify.app">Albertus J. Smit</a> <a href="https://orcid.org/0000-0002-3799-6126" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://uwc.ac.za">
            University of the Western Cape
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header><p>Regularisation techniques are invaluable when dealing with complex datasets or situations where traditional methods may fall short. They are used to enhance model stability, improve predictive performance, and increase interpretability, especially when working with multi-dimensional data in multiple linear regression models and multivariate analyses. Regularisation addresses several common challenges in statistical modelling: i) multicollinearity, ii) variable selection, iii) overfitting, and iv) model simplification.</p>
<p>Environmental datasets often contain many independent variables, and it is likely that only some of them are necessary to explain the phenomenon of interest. <strong>Variable selection</strong> is the process of identifying the most important predictors to include in a model. This can be achieved through the application of specialist, domain-specific knowledge, or through statistical or data-driven approaches. Regularisation is an example of the latter, as it can automatically identify the most relevant predictors on statistical grounds, serving as an alternative to traditional variable selection methods such as Variance Inflation Factor (VIF) and stepwise selection (see <a href="multiple_linear_regression.html#sec-multicollinearity" class="quarto-xref"><span>Section 5.6.4</span></a> and <a href="multiple_linear_regression.html#sec-forward-selection" class="quarto-xref"><span>Section 5.6.5</span></a>).</p>
<p><strong>Overfitting</strong> occurs when a model ‘explains’ the noise in data together with the underlying pattern, which might happen when the model has too many predictors relative to the number of observations. This may also result when variable selection has not been sufficiently addressed. An overfit model performs exceptionally well on training data but fails to generalise to new, unseen data. Additionally, having too many predictors can lead to <strong>multicollinearity</strong> (see <a href="multiple_linear_regression.html#sec-multicollinearity" class="quarto-xref"><span>Section 5.6.4</span></a>). This is a common issue in multiple linear regression when some of the many predictors included in the model are correlated. Multicollinearity can lead to inflated standard errors, unstable coefficients, and difficulty interpreting the model. Regularisation help manage multicollinearity by shrinking coefficient estimates or setting some to zero.</p>
<p>Effectiveness in variable selection, reducing multicollinearity, and mitigating overfitting all contribute to <strong>model simplification</strong>. Regularisation achieves similar outcomes by shrinking coefficient estimates or setting some to zero, making the model easier to understand, explain, and interpret.</p>
<p>In this chapter, we will discuss three common regularisation techniques: Lasso, Ridge, and Elastic Net Regression.</p>
<section id="ridge-regression-l2-regularisation" class="level2 page-columns page-full" data-number="8.1"><h2 data-number="8.1" class="anchored" data-anchor-id="ridge-regression-l2-regularisation">
<span class="header-section-number">8.1</span> Ridge Regression (L2 Regularisation)</h2>
<p>Ridge regression mathematically ‘tames’ the wildness of linear regression when faced with multicollinearity. It achieves this by adding a penalty term to the linear regression loss function—a term proportional to the square of the coefficients (the L2 norm). This penalty nudges the coefficients towards zero, effectively shrinking them without forcing them to be exactly zero.</p>
<p>In linear regression, the loss function is typically the Mean Squared Error (MSE), which is the average of the squared residuals (also known as the residual sum of squares, RSS). The optimisation objective is to minimise this loss function. In other words, the linear regression model aims to find the coefficients that minimise the average squared difference between the observed values and the predicted values. The RSS is expressed in <a href="#eq-rss" class="quarto-xref">Equation&nbsp;<span>8.1</span></a>:</p>
<p><span id="eq-rss"><span class="math display">RSS(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 \tag{8.1}</span></span></p>
<p>And the MSE, which is the loss function to be minimised, is in <a href="#eq-mse" class="quarto-xref">Equation&nbsp;<span>8.2</span></a>:</p>
<p><span id="eq-mse"><span class="math display">MSE(\beta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 \tag{8.2}</span></span></p>
<p>Where:</p>
<ul>
<li>
<span class="math inline">y_i</span> is the observed value for the <span class="math inline">i</span>-th observation.</li>
<li>
<span class="math inline">\beta_0</span> is the intercept.</li>
<li>
<span class="math inline">\beta_j</span> are the coefficients for the predictors.</li>
<li>
<span class="math inline">x_{ij}</span> is the value of the <span class="math inline">j</span>-th predictor variable for the <span class="math inline">i</span>-th observation.</li>
<li>
<span class="math inline">n</span> is the number of observations.</li>
<li>
<span class="math inline">p</span> is the number of predictors.</li>
</ul>
<p>The notation <span class="math inline">RSS(\beta)</span> and <span class="math inline">MSE(\beta)</span> indicates that these are functions of the coefficients <span class="math inline">\beta</span>. The optimisation objective for linear regression is to find the coefficients <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span> to <span class="math inline">\beta_p</span> that minimise the MSE. This can be expressed in Equation <a href="#eq-linear-minimisation" class="quarto-xref">Equation&nbsp;<span>8.3</span></a>:</p>
<p><span id="eq-linear-minimisation"><span class="math display">\min_{\beta} \left\{ \frac{1}{n} \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 \right\} \tag{8.3}</span></span></p>
<p>Ridge regression extends the optimisation of the least squares regression by introducing a penalty term to the loss function. This penalty term is proportional to the square of the L2 norm of the coefficient vector, penalising large coefficient values. Ridge regression is specifically designed to handle multicollinearity and mitigate issues caused by correlated predictors. It also helps prevent overfitting when there are many predictors relative to the sample size, providing a more stable estimation process.</p>
<p>The penalty term is controlled by a hyperparameter<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> called lambda (<span class="math inline">\lambda</span>) that determines the strength of the penalty. Larger values of <span class="math inline">\lambda</span> lead to more shrinkage of the coefficients. When <span class="math inline">\lambda</span> = 0, Ridge Regression is equivalent to ordinary least squares regression. As <span class="math inline">\lambda</span> approaches infinity, all coefficients (except the intercept) approach zero. To find the optimal <span class="math inline">\lambda</span>, you might have to use techniques like cross-validation. Cross-validation will be discussed later in Section <a href="#sec-cross-validation" class="quarto-xref"><span>Section 8.4</span></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Hyperparameters are configuration settings that are external to your model and not learned from the data itself.</p></div></div><p>The loss function in Ridge Regression is given by <a href="#eq-ridge-loss-function" class="quarto-xref">Equation&nbsp;<span>8.4</span></a>:</p>
<p><span id="eq-ridge-loss-function"><span class="math display">L_{ridge}(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \tag{8.4}</span></span></p>
<p>Where <span class="math inline">\lambda</span> is the regularisation parameter controlling the penalty’s strength. Note that typically, the intercept <span class="math inline">\beta_0</span> is not included in the penalty term.</p>
<p>In Equation <a href="#eq-ridge-loss-function" class="quarto-xref">Equation&nbsp;<span>8.4</span></a>, <span class="math inline">L_{ridge}(\beta)</span> is the Ridge Regression loss function. This loss function includes the residual sum of squares (RSS) plus a penalty term <span class="math inline">\lambda \sum_{j=1}^p \beta_j^2</span>. The optimisation objective in Ridge Regression is to find the values of the coefficients <span class="math inline">\beta_1</span> through <span class="math inline">\beta_p</span> that minimise this penalised loss function, while also finding the optimal value for the intercept <span class="math inline">\beta_0</span>.</p>
<p>Ridge regression introduces a bias-variance trade-off. By shrinking the coefficients, it introduces a slight bias, as the model’s predictions may not perfectly match the training data. However, this bias is often offset by a significant reduction in variance. The reduced variance means the model’s predictions are more stable and less sensitive to small changes in the input data. This trade-off often results in improved overall predictive performance, especially on new, unseen data.</p>
<p>So, Ridge Regression sacrifices a bit of bias (accuracy on the sample data) to gain a lot in terms of reduced variance (generalisation to new data). This is a typical example of the bias-variance trade-off in statistical modelling and machine learning, where we often find that a bit of bias can lead to a much more robust and reliable model.</p>
<p>Unlike some other regularisation methods, such as principal component regression, Ridge Regression maintains the interpretability of the coefficients in terms of their relationship with the outcome. It is also versatile and can be applied to various types of regression models, including linear and logistic regression.</p>
</section><section id="lasso-regression-l1-regularisation" class="level2" data-number="8.2"><h2 data-number="8.2" class="anchored" data-anchor-id="lasso-regression-l1-regularisation">
<span class="header-section-number">8.2</span> Lasso Regression (L1 Regularisation)</h2>
<p>Lasso (Least Absolute Shrinkage and Selection Operator) regression employs a different penalty term compared to Ridge Regression. Instead of squaring the coefficients, Lasso Regression takes their absolute values. The cost function in Lasso Regression is given in Equation <a href="#eq-lasso-loss-function" class="quarto-xref">Equation&nbsp;<span>8.5</span></a>:</p>
<p><span id="eq-lasso-loss-function"><span class="math display">L_{lasso}(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| \tag{8.5}</span></span></p>
<p>In Equation <a href="#eq-lasso-loss-function" class="quarto-xref">Equation&nbsp;<span>8.5</span></a>, <span class="math inline">L_{lasso}(\beta)</span> is the Lasso Regression loss function. It includes the residual sum of squares (RSS) plus a penalty term <span class="math inline">\lambda \sum_{j=1}^{p} |\beta_j|</span> (L1 norm). This penalty term is the sum of the absolute values of the coefficients, scaled by the regularisation parameter <span class="math inline">\lambda</span> (similar to Ridge Regression). Lasso regression seeks the values of <span class="math inline">\beta_0</span> through <span class="math inline">\beta_p</span> that minimise <span class="math inline">L_{lasso}(\beta)</span>. As with Ridge Regression, the intercept <span class="math inline">\beta_0</span> is typically not included in the penalty term.</p>
<p>The strength of Lasso Regression lies in its ability to shrink some coefficients all the way to zero, effectively eliminating those variables from the model. This automatic variable selection makes Lasso Regression well-suited for creating sparse models where only the most influential variables are retained. This simplification aids in interpretation and can enhance model performance by reducing noise and overfitting.</p>
<p>Lasso Regression still applies a degree of shrinkage for the coefficients that are not shrunk to zero. Shrinkage reduces their variance and provide more stable models that are less sensitive to fluctuations in the data. Similar to Ridge Regression, Lasso involves a trade-off between bias and variance. The shrinkage introduces a small bias but can greatly reduce variance and result in better overall predictions.</p>
<p>Lasso regression is useful when dealing with datasets that have a large number of potential predictor variables. It helps identify the most relevant predictors. The end results is a simpler and more interpretable model. If you suspect redundancy among your predictor variables, Lasso can prune them and retain only those that provide the best predictive value. As always, the optimal value for <span class="math inline">\lambda</span> should be determined through techniques like cross-validation.</p>
</section><section id="elastic-net-regression" class="level2" data-number="8.3"><h2 data-number="8.3" class="anchored" data-anchor-id="elastic-net-regression">
<span class="header-section-number">8.3</span> Elastic Net Regression</h2>
<p>Elastic net regression is a hybrid regularisation technique that combines the penalties of Ridge and Lasso Regression. It tries to provide the advantages of both methods and mitigate their drawbacks.</p>
<p>Here, the penalty term is the weighted average of the L1 (Lasso) and L2 (Ridge) penalties. A mixing parameter called alpha (<span class="math inline">\alpha</span>) controls the weighting between the two penalties. When <span class="math inline">\alpha</span> = 0, Elastic Net is equivalent to Ridge Regression and when <span class="math inline">\alpha</span> = 1 it is equivalent to Lasso Regression. For values of <span class="math inline">\alpha</span> between 0 and 1, Elastic Net blends the properties of both methods and provides some flexibility to regularisation.</p>
<p>The cost function in Elastic Net Regression is given in <a href="#eq-elastic-net-loss-function" class="quarto-xref">Equation&nbsp;<span>8.6</span></a>:</p>
<p><span id="eq-elastic-net-loss-function"><span class="math display">L_{e_net}(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \left( \alpha \sum_{j=1}^{p} |\beta_j| + (1 - \alpha) \sum_{j=1}^{p} \beta_j^2 \right) \tag{8.6}</span></span></p>
<p>Where <span class="math inline">\alpha</span> is the mixing parameter, with <span class="math inline">0 \leq \alpha \leq 1</span>.</p>
<p>In <a href="#eq-elastic-net-loss-function" class="quarto-xref">Equation&nbsp;<span>8.6</span></a> there is the familiar RSS plus the combined penalty term that is a weighted sum of the L1 and L2 norms. The objective of Elastic Net Regression is again to minimise <span class="math inline">L_{e_net}(\beta)</span> by seeking optimal values of <span class="math inline">\beta_1</span> through <span class="math inline">\beta_p</span>.</p>
<p>Like the other regularisation techniques, Elastic Net is also used when you have highly correlated predictors. While Lasso Regression might arbitrarily select one variable from a group and ignore the rest, Elastic Net tends to select groups of correlated features together and so provide a more comprehensive understanding of variable importance. The flexibility of adjusting the <span class="math inline">\alpha</span> parameter allows you to fine-tune the regularisation to best suit your specific dataset and modelling goals. It balances variable selection (Lasso) and shrinkage (Ridge). Also, Elastic Net can outperform Lasso and Ridge Regression in terms of prediction accuracy when dealing with high-dimensional datasets where the number of predictors exceeds the number of observations.</p>
<p>Elastic net is a good option if you have a dataset with many potential predictor variables and suspect strong correlations among them. Use it when you are uncertain whether pure variable selection (Lasso) or pure shrinkage (Ridge) is the best approach. The challenge is that now we also have to tune the <span class="math inline">\alpha</span> parameter in addition to the regularisation parameter <span class="math inline">\lambda</span>. A caveat is that Elastic Net retains the interpretability of individual coefficients but the interpretation becomes slightly more nuanced due to the mixed penalty term. This requires a thoughtful approach to understanding the model outputs.</p>
</section><section id="sec-cross-validation" class="level2 page-columns page-full" data-number="8.4"><h2 data-number="8.4" class="anchored" data-anchor-id="sec-cross-validation">
<span class="header-section-number">8.4</span> Cross-Validation</h2>
<p>The values of the hyperparameters (<span class="math inline">\lambda</span> or <span class="math inline">\alpha</span>) significantly affect the model’s performance and generalisation ability and so it necessitates careful optimisation. The <code>cv.glmnet()</code> function (see <a href="#sec-r-function" class="quarto-xref"><span>Section 8.5</span></a>) automates this process by performing both hyperparameter tuning<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> and cross-validation. It systematically evaluates different combinations of <span class="math inline">\lambda</span> or <span class="math inline">\alpha</span> values across multiple subsets of our data, using cross-validation to estimate their out-of-sample performance. This allows for the selection of the hyperparameter combination that yields the best performance and thus avoids the risk of overfitting and improves model generalisation.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;The goal of hyperparameter tuning is to find the optimal combination of hyperparameters that leads to the best model performance on your specific dataset. This is done by systematically evaluating different hyperparameter values and selecting the combination that yields the best results.</p></div></div><p>The most widely used cross-validation method is <span class="math inline">\text{k}</span>-fold cross-validation. The dataset is divided into <span class="math inline">\text{k}</span> equally sized subsets (specified by the user). The subsets are called ‘folds’. The model is then trained <span class="math inline">\text{k}</span> times, each time using <span class="math inline">\text{k}-1</span> folds for training and the remaining fold for validation. It provides a robust estimate of model performance by utilising all data points for both training and validation. It balances computational cost and bias reduction. But, the choice of k can influence results, and there’s a trade-off between bias and variance: lower <span class="math inline">\text{k}</span> values may lead to higher bias but lower variance, whilst higher <span class="math inline">\text{k}</span> values do the opposite.</p>
<p>The general approach taken in <span class="math inline">\text{k}</span>-fold cross validation is that, for each combination of hyperparameter values, we:</p>
<ol type="1">
<li>Perform <span class="math inline">\text{k}</span>-fold cross-validation on the training data.</li>
<li>Calculate the average performance metric (e.g., mean squared error) across all folds.</li>
<li>Select the hyperparameter values that produced the best average performance.</li>
</ol>
<p>This ensures that the hyperparameters we select are robust and generalissable to unseen data, rather than being overly influenced by the peculiarities of a single training set.</p>
<p><em>K</em>-fold cross-validation is the most frequently-used form of cross-validation, but several other types exist. Some of them are:</p>
<p><strong>Leave-one-out cross-validation (LOOCV)</strong> is an extreme case of <span class="math inline">\text{k}</span>-fold cross-validation where <span class="math inline">\text{k}</span> equals the number of data points. This method trains the model on all but one data point and validates on the left-out point, repeating this process for each data point. LOOCV provides an nearly unbiased estimate of model performance but can be computationally expensive for large datasets. It’s most often used for small datasets where maximising training data is important. The downside is that LOOCV can suffer from high variance, especially for noisy datasets.</p>
<p><strong>Stratified cross-validation</strong> ensures each fold maintains the same proportion of samples for each class as in the complete dataset. It useful for imbalanced datasets or when dealing with categorical outcomes. By preserving the class distribution in each fold, stratified cross-validation provides a more representative evaluation of model performance across all classes. Implementing stratification can be complex for multi-class problems or continuous outcomes.</p>
<p><strong>Holdout validation</strong> is the simplest form of cross-validation. The dataset is split into a training set and a test set. Typically, about 70-80% of the data is used for training and the balance is reserved for testing. The model is trained on the training set and then evaluated on the held-out test set. It is computationally efficient and provides a quick estimate of model performance but it has several limitations. Firstly, because it doesn’t make full use of the available data for training, it can be an issue for smaller datasets. Secondly, the results can be highly dependent on the particular split chosen, leading to high variance in performance estimates. This is especially true for smaller datasets or when the split doesn’t represent the overall data distribution well. But holdout validation remains useful for large datasets or as a quick initial assessment before applying more complex cross-validation techniques.</p>
<p>The examples will show <span class="math inline">\text{k}</span>-fold cross validation, but you can easily adapt the code to use other cross-validation methods.</p>
</section><section id="sec-r-function" class="level2" data-number="8.5"><h2 data-number="8.5" class="anchored" data-anchor-id="sec-r-function">
<span class="header-section-number">8.5</span> R Function</h2>
<p>In R, the <strong>glmnet</strong> package provides functions for fitting regularised linear models. The <code>cv.glmnet()</code> function performs cross-validated regularisation path selection for the Elastic Net, Lasso, and Ridge Regression models.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> <span class="cn">NULL</span>, <span class="at">nfolds =</span> <span class="dv">10</span>,</span>
<span id="cb1-2"><a href="#cb1-2"></a>          <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The function takes the following arguments:</p>
<ul>
<li>
<code>x</code>: A matrix of predictors.</li>
<li>
<code>y</code>: A matrix of response variables (but read the help file as this varies depending on the data type).</li>
<li>
<code>alpha</code>: The mixing parameter for the Elastic Net penalty. When <code>alpha = 0</code>, the model is a Ridge Regression. When <code>alpha = 1</code>, the model is a Lasso Regression. The default value is <code>alpha = 1</code>.</li>
<li>
<code>lambda</code>: A vector of regularisation parameters. The function fits a model for each value of <code>lambda</code> and selects the best one based on cross-validation. The default is <code>lambda = NULL</code>, which means the function will generate a sequence of <code>100</code> values between <code>10^-2</code> and <code>10^2</code>.</li>
<li>
<code>nfolds</code>: The number of folds in the cross-validation. The default is <code>nfolds = 10</code>.</li>
<li>
<code>standardize</code>: A logical value indicating whether the predictors should be standardised. The default is <code>standardize = TRUE</code>.</li>
</ul>
<p>It is not clearly documented in the function’s help file, but the ‘glm’ in the function name indicates that the function fits a generalised linear model. This implies ‘gaussian,’ ‘binomial,’ ‘poisson,’ ‘multinomial,’ ‘cox,’ and ‘mgaussian’ families are supported, which can be supplied via the <code>family</code> argument to the function. The ‘net’ part of the name indicates that the function fits an Elastic Net, thus allowing choose between Lasso and Ridge by setting <code>alpha</code> to 1 or 0 (or something in-between). The ‘cv’ part of the name indicates that the function performs cross-validation.</p>
</section><section id="example-1-ridge-regression" class="level2 page-columns page-full" data-number="8.6"><h2 data-number="8.6" class="anchored" data-anchor-id="example-1-ridge-regression">
<span class="header-section-number">8.6</span> Example 1: Ridge Regression</h2>
<p>The data I use here should be well-known by now. They are the same seaweed dataset used throughout <a href="multiple_linear_regression.html" class="quarto-xref"><span>Chapter 5</span></a>. I will use Ridge Regression to predict the response variable <code>Y</code> using the predictors <code>annMean</code>, <code>augMean</code>, <code>augSD</code>, <code>febSD</code>, and <code>febRange</code>.</p>
<p>First, I will read in the data and prepare them in the format required by <code>cv.glmnet()</code>. This involves standardising the response variable and predictors and converting them to matrices. I specify the range of <span class="math inline">\lambda</span> values to try and set up 10-fold cross-validation. I then fit the model and plot the results of the cross-validation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># Ridge Regression with Cross-Validation</span></span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb2-5"><a href="#cb2-5"></a></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb2-9"><a href="#cb2-9"></a></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="co"># Read the data</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>sw <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"data/spp_df2.csv"</span>)</span>
<span id="cb2-12"><a href="#cb2-12"></a></span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="co"># Standardise the response variable and present as a matrix</span></span>
<span id="cb2-14"><a href="#cb2-14"></a>y <span class="ot">&lt;-</span> sw <span class="sc">|&gt;</span></span>
<span id="cb2-15"><a href="#cb2-15"></a>  <span class="fu">select</span>(Y) <span class="sc">|&gt;</span></span>
<span id="cb2-16"><a href="#cb2-16"></a>  <span class="fu">scale</span>(<span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">FALSE</span>) <span class="sc">|&gt;</span></span>
<span id="cb2-17"><a href="#cb2-17"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb2-18"><a href="#cb2-18"></a></span>
<span id="cb2-19"><a href="#cb2-19"></a><span class="co"># Provide the predictors as a matrix</span></span>
<span id="cb2-20"><a href="#cb2-20"></a>X <span class="ot">&lt;-</span> sw <span class="sc">|&gt;</span></span>
<span id="cb2-21"><a href="#cb2-21"></a>  <span class="fu">select</span>(<span class="sc">-</span>X, <span class="sc">-</span>dist, <span class="sc">-</span>bio, <span class="sc">-</span>Y, <span class="sc">-</span>Y1, <span class="sc">-</span>Y2) <span class="sc">|&gt;</span></span>
<span id="cb2-22"><a href="#cb2-22"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb2-23"><a href="#cb2-23"></a></span>
<span id="cb2-24"><a href="#cb2-24"></a><span class="co"># Set up lambda sequence</span></span>
<span id="cb2-25"><a href="#cb2-25"></a>lambdas_to_try <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="sc">^</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb2-26"><a href="#cb2-26"></a></span>
<span id="cb2-27"><a href="#cb2-27"></a><span class="co"># Perform 10-fold cross-validation</span></span>
<span id="cb2-28"><a href="#cb2-28"></a>ridge_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambdas_to_try,</span>
<span id="cb2-29"><a href="#cb2-29"></a>  <span class="at">standardize =</span> <span class="cn">TRUE</span>, <span class="at">nfolds =</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Plot cross-validation results (ggplot shown)</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="fu">plot</span>(ridge_cv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-ridge-regression" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-ridge-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regularisation_files/figure-html/fig-ridge-regression-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-ridge-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.1: Cross-validation statistics for the Ridge Regression approach applied to the seaweed data.
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-ridge-regression" class="quarto-xref">Figure&nbsp;<span>8.1</span></a>, generated from the <code>cv.glmnet()</code> object, illustrates the relationship between the regularisation parameter <span class="math inline">\lambda</span> and the model’s cross-validation performance. The <em>y</em>-axis represents the mean squared error (MSE) from cross-validation, whilst the <em>x</em>-axis shows the <span class="math inline">log(\lambda)</span> values tested. Red dots indicate the mean MSE for each <span class="math inline">\lambda</span>, with error bars showing ±1 standard error. Two vertical dashed lines highlight important <span class="math inline">\lambda</span> values: <span class="math inline">\lambda_{min}</span>, which minimises the mean MSE, and <span class="math inline">\lambda_{1se}</span>, the largest <span class="math inline">\lambda</span> within one standard error of the minimum MSE. One may select the optimal <span class="math inline">\lambda</span> using either the <span class="math inline">\lambda_{min}</span> or the <span class="math inline">\lambda_{1se}</span> rule, accessible via <code>cv.glmnet_object$lambda.min</code> and <code>cv.glmnet_object$lambda.1se</code>, respectively. To utilise the chosen <span class="math inline">\lambda</span>, one refits the model using <code>glmnet()</code> and extract the coefficients.</p>
<p>For performance evaluation, one can calculate the sum of squared residuals (SSR) as the sum of squared differences between observed and predicted values, and the R-squared value as the square of the correlation between observed and predicted values, representing the proportion of variance in the dependent variable that is predictable from the independent variable(s).</p>
<p>The results show that the model explains 67.07% of the variance in the response variable:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># Fit models and calculate performance metrics</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>fit_model_and_calculate_metrics <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, lambda) {</span>
<span id="cb4-3"><a href="#cb4-3"></a>  model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda,</span>
<span id="cb4-4"><a href="#cb4-4"></a>                  <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb4-5"><a href="#cb4-5"></a>  y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, X)</span>
<span id="cb4-6"><a href="#cb4-6"></a>  ssr <span class="ot">&lt;-</span> <span class="fu">sum</span>((y <span class="sc">-</span> y_hat) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb4-7"><a href="#cb4-7"></a>  rsq <span class="ot">&lt;-</span> <span class="fu">cor</span>(y, y_hat) <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>  <span class="fu">list</span>(<span class="at">model =</span> model, <span class="at">ssr =</span> ssr, <span class="at">rsq =</span> rsq)</span>
<span id="cb4-9"><a href="#cb4-9"></a>}</span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co"># Best cross-validated lambda</span></span>
<span id="cb4-12"><a href="#cb4-12"></a>lambda_cv <span class="ot">&lt;-</span> ridge_cv<span class="sc">$</span>lambda.min</span>
<span id="cb4-13"><a href="#cb4-13"></a>mod_cv <span class="ot">&lt;-</span> <span class="fu">fit_model_and_calculate_metrics</span>(X, y, lambda_cv)</span>
<span id="cb4-14"><a href="#cb4-14"></a></span>
<span id="cb4-15"><a href="#cb4-15"></a><span class="co"># Print results</span></span>
<span id="cb4-16"><a href="#cb4-16"></a>mod_cv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$model

Call:  glmnet(x = X, y = y, alpha = 0, lambda = lambda, standardize = TRUE) 

  Df  %Dev Lambda
1  5 67.06  0.001

$ssr
[1] 5.321994

$rsq
         s0
Y 0.6706681</code></pre>
</div>
</div>
<p>As already indicated, an alternative to using <code>lambda.min</code> for selecting the optimal <span class="math inline">\lambda</span> value is to use the 1 SE rule, which is contained in the attribute <code>lambda.1se</code>. This reduces the risk of overfitting as it tends to select a simpler model. We can use this value to refit the model and extract the coefficients, as before.</p>
<p>AIC and BIC can also be used to select suitable models. These information criteria penalise the model for the number of parameters used, providing a balance between model complexity and goodness of fit. The <code>calculate_ic()</code> function below calculates the AIC and BIC for a given model and returns the results in a list. We can then use this function to calculate the AIC and BIC for each model fit with each <span class="math inline">\lambda</span> in <code>lambdas_to_try</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Calculate AIC and BIC</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>calculate_ic <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, lambda) {</span>
<span id="cb6-3"><a href="#cb6-3"></a>  model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda,</span>
<span id="cb6-4"><a href="#cb6-4"></a>                  <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb6-5"><a href="#cb6-5"></a>  betas <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(<span class="fu">coef</span>(model)[<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb6-6"><a href="#cb6-6"></a>  resid <span class="ot">&lt;-</span> y <span class="sc">-</span> (<span class="fu">scale</span>(X) <span class="sc">%*%</span> betas)</span>
<span id="cb6-7"><a href="#cb6-7"></a>  H <span class="ot">&lt;-</span> <span class="fu">scale</span>(X) <span class="sc">%*%</span></span>
<span id="cb6-8"><a href="#cb6-8"></a>    <span class="fu">solve</span>(<span class="fu">t</span>(<span class="fu">scale</span>(X)) <span class="sc">%*%</span> <span class="fu">scale</span>(X) <span class="sc">+</span> lambda <span class="sc">*</span></span>
<span id="cb6-9"><a href="#cb6-9"></a>            <span class="fu">diag</span>(<span class="fu">ncol</span>(X))) <span class="sc">%*%</span> <span class="fu">t</span>(<span class="fu">scale</span>(X))</span>
<span id="cb6-10"><a href="#cb6-10"></a>  df <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(H))</span>
<span id="cb6-11"><a href="#cb6-11"></a>  log_resid_ss <span class="ot">&lt;-</span> <span class="fu">log</span>(<span class="fu">sum</span>(resid <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb6-12"><a href="#cb6-12"></a>  aic <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X) <span class="sc">*</span> log_resid_ss <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> df</span>
<span id="cb6-13"><a href="#cb6-13"></a>  bic <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X) <span class="sc">*</span> log_resid_ss <span class="sc">+</span> <span class="fu">log</span>(<span class="fu">nrow</span>(X)) <span class="sc">*</span> df</span>
<span id="cb6-14"><a href="#cb6-14"></a>  <span class="fu">list</span>(<span class="at">aic =</span> aic, <span class="at">bic =</span> bic)</span>
<span id="cb6-15"><a href="#cb6-15"></a>}</span>
<span id="cb6-16"><a href="#cb6-16"></a></span>
<span id="cb6-17"><a href="#cb6-17"></a>ic_results <span class="ot">&lt;-</span> <span class="fu">map</span>(lambdas_to_try, <span class="sc">~</span> <span class="fu">calculate_ic</span>(X, y, .x)) <span class="sc">|&gt;</span></span>
<span id="cb6-18"><a href="#cb6-18"></a>  <span class="fu">transpose</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A plot of the change in the information criteria with <span class="math inline">log(\lambda)</span> is shown in <a href="#fig-aic-bic" class="quarto-xref">Figure&nbsp;<span>8.2</span></a>. The optimal <span class="math inline">\lambda</span> values according to both AIC and BIC can then be used to refit the model and arrive at the coefficients of interest.</p>
<div class="cell page-columns page-full">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># Plot information criteria</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>plot_ic <span class="ot">&lt;-</span> <span class="cf">function</span>(lambdas, ic_results) {</span>
<span id="cb7-3"><a href="#cb7-3"></a>  df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">lambda =</span> <span class="fu">log</span>(lambdas),</span>
<span id="cb7-4"><a href="#cb7-4"></a>                   <span class="at">aic =</span> <span class="fu">unlist</span>(ic_results<span class="sc">$</span>aic),</span>
<span id="cb7-5"><a href="#cb7-5"></a>                   <span class="at">bic =</span> <span class="fu">unlist</span>(ic_results<span class="sc">$</span>bic))</span>
<span id="cb7-6"><a href="#cb7-6"></a></span>
<span id="cb7-7"><a href="#cb7-7"></a>  df_long <span class="ot">&lt;-</span> <span class="fu">pivot_longer</span>(df, <span class="at">cols =</span> <span class="fu">c</span>(aic, bic),</span>
<span id="cb7-8"><a href="#cb7-8"></a>                          <span class="at">names_to =</span> <span class="st">"criterion"</span>,</span>
<span id="cb7-9"><a href="#cb7-9"></a>                          <span class="at">values_to =</span> <span class="st">"value"</span>)</span>
<span id="cb7-10"><a href="#cb7-10"></a></span>
<span id="cb7-11"><a href="#cb7-11"></a>  <span class="fu">ggplot</span>(df_long, <span class="fu">aes</span>(<span class="at">x =</span> lambda, <span class="at">y =</span> value, <span class="at">color =</span> criterion)) <span class="sc">+</span></span>
<span id="cb7-12"><a href="#cb7-12"></a>    <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>    <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"aic"</span> <span class="ot">=</span> <span class="st">"orange"</span>, <span class="st">"bic"</span> <span class="ot">=</span> <span class="st">"skyblue3"</span>),</span>
<span id="cb7-14"><a href="#cb7-14"></a>                       <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"aic"</span> <span class="ot">=</span> <span class="st">"AIC"</span>, <span class="st">"bic"</span> <span class="ot">=</span> <span class="st">"BIC"</span>)) <span class="sc">+</span></span>
<span id="cb7-15"><a href="#cb7-15"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"log(lambda)"</span>,</span>
<span id="cb7-16"><a href="#cb7-16"></a>         <span class="at">y =</span> <span class="st">"Information Criterion"</span>, <span class="at">color =</span> <span class="st">"Criterion"</span>) <span class="sc">+</span></span>
<span id="cb7-17"><a href="#cb7-17"></a>    <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb7-18"><a href="#cb7-18"></a>    <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"top"</span>,</span>
<span id="cb7-19"><a href="#cb7-19"></a>          <span class="at">legend.direction =</span> <span class="st">"horizontal"</span>,</span>
<span id="cb7-20"><a href="#cb7-20"></a>          <span class="at">legend.box =</span> <span class="st">"horizontal"</span>)</span>
<span id="cb7-21"><a href="#cb7-21"></a>}</span>
<span id="cb7-22"><a href="#cb7-22"></a></span>
<span id="cb7-23"><a href="#cb7-23"></a><span class="fu">plot_ic</span>(lambdas_to_try, ic_results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-aic-bic" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-aic-bic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regularisation_files/figure-html/fig-aic-bic-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-aic-bic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.2: Plot of information criteria for best model fit selected through Ridge Regression.
</figcaption></figure>
</div>
</div>
</div>
<p>Now we find the <span class="math inline">\lambda</span> values that minimise the AIC and BIC, and refit the models using these values. It so happens that both AIC and BIC selects the same <span class="math inline">\lambda</span> values:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># Optimal lambdas according to both criteria</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>lambda_aic <span class="ot">&lt;-</span> lambdas_to_try[<span class="fu">which.min</span>(ic_results<span class="sc">$</span>aic)]</span>
<span id="cb8-3"><a href="#cb8-3"></a>lambda_bic <span class="ot">&lt;-</span> lambdas_to_try[<span class="fu">which.min</span>(ic_results<span class="sc">$</span>bic)]</span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="co"># Fit final models using the optimal lambdas</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>mod_aic <span class="ot">&lt;-</span> <span class="fu">fit_model_and_calculate_metrics</span>(X, y, lambda_aic)</span>
<span id="cb8-7"><a href="#cb8-7"></a>mod_bic <span class="ot">&lt;-</span> <span class="fu">fit_model_and_calculate_metrics</span>(X, y, lambda_bic)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For interest sake, we may also produce a plot that traces the coefficients of the model as <span class="math inline">\lambda</span> changes. This can help us understand how the coefficients shrink as <span class="math inline">\lambda</span> increases, and which variables are most important in the model. The plot below shows the Ridge Regression coefficients path for each variable in the model (<a href="#fig-coef-path1" class="quarto-xref">Figure&nbsp;<span>8.3</span></a>).</p>
<div class="cell page-columns page-full">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># Plot the Ridge Regression coefficients path</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>res <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambdas_to_try,</span>
<span id="cb9-3"><a href="#cb9-3"></a>              <span class="at">standardize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="fu">plot</span>(res, <span class="at">xvar =</span> <span class="st">"lambda"</span>)</span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb9-6"><a href="#cb9-6"></a>       <span class="at">legend =</span> <span class="fu">colnames</span>(X), <span class="at">cex =</span> <span class="fl">0.7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display page-columns page-full">
<div id="fig-coef-path1" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-coef-path1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regularisation_files/figure-html/fig-coef-path1-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-coef-path1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.3: Plot of the Ridge Regression coefficients paths.
</figcaption></figure>
</div>
</div>
</div>
<p>So, after having demonstrated the different methods for selecting the optimal <span class="math inline">\lambda</span> value, we can now summarise the results:</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "CV Lambda: 0.001"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "AIC Lambda: 0.3854"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "BIC Lambda: 0.3854"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "CV R-squared: 0.6707"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "AIC R-squared: 0.6025"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "BIC R-squared: 0.6025"</code></pre>
</div>
</div>
<p>Now we can extract the coefficient produced from models selected via the AIC and CV methods.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a>res_aic <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda_aic,</span>
<span id="cb16-2"><a href="#cb16-2"></a>                  <span class="at">standardize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb16-3"><a href="#cb16-3"></a>res_aic</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:  glmnet(x = X, y = y, alpha = 0, lambda = lambda_aic, standardize = FALSE) 

  Df  %Dev Lambda
1  5 13.46 0.3854</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="fu">coef</span>(res_aic)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>6 x 1 sparse Matrix of class "dgCMatrix"
                      s0
(Intercept) -0.021327121
augMean      0.009856026
febRange     0.007118466
febSD       -0.001074341
augSD        0.010696102
annMean      0.008114467</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a>res_cv <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda_cv,</span>
<span id="cb20-2"><a href="#cb20-2"></a>                 <span class="at">standardize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb20-3"><a href="#cb20-3"></a>res_cv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:  glmnet(x = X, y = y, alpha = 0, lambda = lambda_cv, standardize = FALSE) 

  Df  %Dev Lambda
1  5 66.77  0.001</code></pre>
</div>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a><span class="fu">coef</span>(res_cv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>6 x 1 sparse Matrix of class "dgCMatrix"
                     s0
(Intercept) -0.12384440
augMean      0.22200994
febRange     0.04287655
febSD       -0.03446642
augSD        0.02699458
annMean      0.04324177</code></pre>
</div>
</div>
<p>Ridge regression adds a penalty to the size of the coefficients, resulting in their shrinkage towards zero. This penalty affects all coefficients simultaneously. Notably, there is a difference in the model fit obtained using <span class="math inline">\lambda_{AIC}</span> (which is larger) and <span class="math inline">\lambda_{min}</span> (which is smaller). The former model explains 55.69% of the variance, compared to <span class="math inline">\lambda_{min}</span>, which explains 63.37% of the variance.</p>
<p>Although shrinkage affects the absolute magnitude of the coefficients (they are biased estimates of the true relationships between the predictors and the response variable), the coefficients in Ridge Regression retain their general meaning—they still represent the change in the response variable associated with a one-unit change in the predictor variable, holding other predictors constant. While the absolute values of the coefficients may be biased due to regularisation, the relative importance of the predictors can still be interpreted. The magnitude of the coefficients can indicate the relative influence of each predictor on the response variable, even if their exact values are reduced.</p>
<p>Importantly, the predictive ability of the model can improve with shrunk coefficients because Ridge Regression reduces overfitting and enhances the model’s generalisability to new, unseen data. By stabilising the coefficient estimates, the model often achieves better performance on validation and test datasets, which is important should robust predictive analytics be the goal.</p>
</section><section id="example-2-lasso-regression" class="level2 page-columns page-full" data-number="8.7"><h2 data-number="8.7" class="anchored" data-anchor-id="example-2-lasso-regression">
<span class="header-section-number">8.7</span> Example 2: Lasso Regression</h2>
<p>Doing a Lasso Regression is easy. Simply change the <code>alpha</code> parameter to 1 in the <code>glmnet</code> function. The rest of the code remains the same. I’ll show only the final output of this analysis to avoid repetition.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># Print results</span></span>
<span id="cb24-2"><a href="#cb24-2"></a>mod_cv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$model

Call:  glmnet(x = X, y = y, alpha = 1, lambda = lambda, standardize = TRUE) 

  Df %Dev Lambda
1  5   67  0.001

$ssr
[1] 5.332835

$rsq
         s0
Y 0.6701255</code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="fu">coef</span>(mod_cv<span class="sc">$</span>model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>6 x 1 sparse Matrix of class "dgCMatrix"
                     s0
(Intercept) -0.12886019
augMean      0.26097296
febRange     0.03431981
febSD       -0.02497532
augSD        0.02441380
annMean      0.02021480</code></pre>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a><span class="co"># Print results</span></span>
<span id="cb28-2"><a href="#cb28-2"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"CV Lambda:"</span>, lambda_cv))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "CV Lambda: 0.001"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"CV R-squared:"</span>, <span class="fu">round</span>(mod_cv<span class="sc">$</span>rsq, <span class="dv">4</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "CV R-squared: 0.6701"</code></pre>
</div>
</div>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-lasso-regression" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-lasso-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regularisation_files/figure-html/fig-lasso-regression-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-lasso-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.4: Cross-validation statistics for Lasso Regression applied to the seaweed data.
</figcaption></figure>
</div>
</div>
</div>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-coef-path2" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-coef-path2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regularisation_files/figure-html/fig-coef-path2-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-coef-path2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.5: Plot of the Lasso Regression coefficients paths.
</figcaption></figure>
</div>
</div>
</div>
<p>Lasso regression incorporates an L1 penalty term in its cost function, which shrinks some coefficient estimates to exactly zero. By reducing certain coefficients to zero, Lasso effectively eliminates those predictors from the model, which achieves automatic variable selection:</p>
<ul>
<li>When <span class="math inline">\lambda</span> is small, the penalty is minimal, and Lasso behaves similarly to ordinary least squares regression, retaining most coefficients.</li>
<li>When <span class="math inline">\lambda</span> is large, the penalty increases, causing more coefficients to shrink to zero. This results in a sparser model where only the most significant predictors have non-zero coefficients.</li>
</ul>
<p>In our example (<a href="#fig-lasso-regression" class="quarto-xref">Figure&nbsp;<span>8.4</span></a>), we see at <span class="math inline">\lambda_{min}</span>, the number of non-zero coefficients is minimised—all five coefficients remain. At <span class="math inline">\lambda_{1se}</span>, the number of non-zero coefficients decreases to four. Consequently, for higher values of <span class="math inline">\lambda</span>, more predictors will have coefficients exactly equal to zero. This is also seen in <a href="#fig-lasso-regression" class="quarto-xref">Figure&nbsp;<span>8.4</span></a>. In <a href="#fig-coef-path2" class="quarto-xref">Figure&nbsp;<span>8.5</span></a> we can see that the first predictor to reach zero is <code>annMean</code>, then <code>febSD</code>, <code>febRange</code>, and so forth. The implication is that they are excluded from the model and the model is simplified. This leads to several benefits: reduced multicollinearity, improved interpretability, and better generalisation to new data.</p>
<p>Coefficients that remain non-zero after Lasso regularisation are considered more important predictors. Those remaining coefficients can be interpreted similarly to standard linear regression: as the expected change in the response variable for a one-unit change in the predictor, holding other predictors constant.</p>
<p>The <span class="math inline">\lambda</span> parameter controls the amount of bias introduced. While Lasso can produce biased estimates, it reduces variance, often resulting in a model that performs better on new, unseen data. This trade-off enhances predictive accuracy but means that the exact coefficient values may not represent the true underlying relationships as closely as those in an unregularised model.</p>
<p>Despite regularisation, the relative magnitudes of the non-zero coefficients provide a glimpse into predictor importance. Larger absolute values of coefficients indicate stronger relationships with the response variable. The exact numerical values are biased, but ranking predictors by their coefficients still offers useful insight into their relative importance.</p>
</section><section id="example-3-elastic-net-regression" class="level2 page-columns page-full" data-number="8.8"><h2 data-number="8.8" class="anchored" data-anchor-id="example-3-elastic-net-regression">
<span class="header-section-number">8.8</span> Example 3: Elastic Net Regression</h2>
<p>In this last example we’ll look at Elastic Net Regression, which combines the L1 and L2 penalties of Lasso and Ridge Regression. There are now two parameters to optimise: <span class="math inline">\alpha</span> and <span class="math inline">\lambda</span>. The <span class="math inline">\alpha</span> parameter controls the mix between the L1 and L2 penalties, with <span class="math inline">\alpha = 0</span> behaving like Ridge Regression and <span class="math inline">\alpha = 1</span> behaving like Lasso Regression. For <span class="math inline">\alpha</span> values between 0 and 1, Elastic Net combines the strengths of both Lasso and Ridge Regression. Optimisation of <span class="math inline">\alpha</span> and <span class="math inline">\lambda</span> is also done using cross-validation. In practise, the steps are:</p>
<ol type="1">
<li>Set up a grid of <span class="math inline">\alpha</span> values (from 0 to 1) and <span class="math inline">\lambda</span> values to try.</li>
<li>Performs cross-validation for each combination of <span class="math inline">\alpha</span> and <span class="math inline">\lambda</span> using <code>cv.glmnet()</code>.</li>
<li>Select the best <span class="math inline">\alpha</span> and <span class="math inline">\lambda</span> combination based on the minimum mean cross-validated error.</li>
<li>Fit the final model using the best <span class="math inline">\alpha</span> and <span class="math inline">\lambda</span>.</li>
<li>Calculate the performance metrics.</li>
<li>For the Elastic Net model with the best alphaCreate plots similar to those in the Ridge and Lasso examples.</li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1"></a><span class="co"># Define the range of alpha values to try</span></span>
<span id="cb32-2"><a href="#cb32-2"></a>alphas_to_try <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb32-3"><a href="#cb32-3"></a></span>
<span id="cb32-4"><a href="#cb32-4"></a><span class="co"># Define the range of lambda values to try</span></span>
<span id="cb32-5"><a href="#cb32-5"></a>lambdas_to_try <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb32-6"><a href="#cb32-6"></a></span>
<span id="cb32-7"><a href="#cb32-7"></a><span class="co"># Perform grid search with cross-validation</span></span>
<span id="cb32-8"><a href="#cb32-8"></a>cv_results <span class="ot">&lt;-</span> <span class="fu">lapply</span>(alphas_to_try, <span class="cf">function</span>(a) {</span>
<span id="cb32-9"><a href="#cb32-9"></a>  <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> a, <span class="at">lambda =</span> lambdas_to_try,</span>
<span id="cb32-10"><a href="#cb32-10"></a>            <span class="at">standardize =</span> <span class="cn">TRUE</span>, <span class="at">nfolds =</span> <span class="dv">10</span>)</span>
<span id="cb32-11"><a href="#cb32-11"></a>})</span>
<span id="cb32-12"><a href="#cb32-12"></a></span>
<span id="cb32-13"><a href="#cb32-13"></a><span class="co"># Find the best alpha and lambda</span></span>
<span id="cb32-14"><a href="#cb32-14"></a>best_result <span class="ot">&lt;-</span> <span class="fu">which.min</span>(<span class="fu">sapply</span>(cv_results, <span class="cf">function</span>(x) <span class="fu">min</span>(x<span class="sc">$</span>cvm)))</span>
<span id="cb32-15"><a href="#cb32-15"></a>best_alpha <span class="ot">&lt;-</span> alphas_to_try[best_result]</span>
<span id="cb32-16"><a href="#cb32-16"></a>best_lambda <span class="ot">&lt;-</span> cv_results[[best_result]]<span class="sc">$</span>lambda.min</span>
<span id="cb32-17"><a href="#cb32-17"></a></span>
<span id="cb32-18"><a href="#cb32-18"></a><span class="co"># Fit the final model with the best parameters</span></span>
<span id="cb32-19"><a href="#cb32-19"></a>final_model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> best_alpha,</span>
<span id="cb32-20"><a href="#cb32-20"></a>                      <span class="at">lambda =</span> best_lambda,</span>
<span id="cb32-21"><a href="#cb32-21"></a>                      <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb32-22"><a href="#cb32-22"></a></span>
<span id="cb32-23"><a href="#cb32-23"></a><span class="co"># Calculate performance metrics</span></span>
<span id="cb32-24"><a href="#cb32-24"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(final_model, X)</span>
<span id="cb32-25"><a href="#cb32-25"></a>ssr <span class="ot">&lt;-</span> <span class="fu">sum</span>((y <span class="sc">-</span> y_hat) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb32-26"><a href="#cb32-26"></a>rsq <span class="ot">&lt;-</span> <span class="fu">cor</span>(y, y_hat) <span class="sc">^</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Best Alpha: 0.3"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Best Lambda: 0.001"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "R-squared: 0.6706"</code></pre>
</div>
</div>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-elastic-net-regression" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-elastic-net-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regularisation_files/figure-html/fig-elastic-net-regression-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-elastic-net-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.6: Cross-validation statistics for Elastic Net Regression applied to the seaweed data.
</figcaption></figure>
</div>
</div>
</div>
<div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-coef-path3" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-coef-path3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="regularisation_files/figure-html/fig-coef-path3-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-coef-path3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.7: Plot of the Elastic Net Regression coefficients paths.
</figcaption></figure>
</div>
</div>
</div>
<p>The model coefficients are:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a><span class="fu">coef</span>(cv_results[[best_result]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>6 x 1 sparse Matrix of class "dgCMatrix"
                      s1
(Intercept) -0.117063010
augMean      0.240447330
febRange     0.015404286
febSD       -0.007224065
augSD        0.014882111
annMean      0.026504736</code></pre>
</div>
</div>
<p>The interpretation of coefficients in Elastic Net is a blend of Ridge and Lasso. Some coefficients may be shrunk to zero (feature selection), while others are shrunk but remain non-zero (magnitude reduction). The non-zero coefficients retain their general meaning with an emphasis on their relative importance.</p>
</section><section id="theory-driven-and-data-driven-variable-selection" class="level2" data-number="8.9"><h2 data-number="8.9" class="anchored" data-anchor-id="theory-driven-and-data-driven-variable-selection">
<span class="header-section-number">8.9</span> Theory-Driven and Data-Driven Variable Selection</h2>
<p>The choice between theory-driven and data- or statistics-driven variable selection represents an important consideration that can greatly influence model interpretation, its predictive power, and your value as an ecologist. This decision reflects a broader tension in scientific methodology between deductive and inductive reasoning. Each offers advantages and limitations that you should be aware of as an ecologist.</p>
<p>Theory-driven variable selection is core to the scientific method. It relies on <em>a priori</em> knowledge and established ecological theories (as far as they exist in ecology!) to guide your choice of predictors in a model. This aligns closely with the hypothetico-deductive method, where we formulate hypotheses based on existing knowledge and subsequently test these against the data we collect. The strength of this method lies in its interpretability. Models built on theoretical foundations often contribute directly to testing and refining ecological hypotheses. By focusing on variables with known or hypothesised relationships (with mechanisms often rooted in ecophysiological or ecological inquiries), the theory-driven hypothetico-deductive method should lead to more parsimonious models that are less prone to overfitting and more reflecting of reality.</p>
<p>Theory-driven selection is not without its drawbacks. It requires that we have a good grasp of the mechanism underlying our favourite ecological system. This is not always the case in complex systems where the underlying mechanisms are not well understood. Theory-driven selection can then lead to the exclusion of important variables that were not initially hypothesised and it can limit the scope of the analysis and potentially overlook significant relationships in the data.</p>
<p>A naive young ecologist might place undue value on the notion that their hard work collecting diverse data and developing hypotheses should all be reflected in their final model. This can lead to confirmation bias, where one is more likely to select variables that support our hypotheses and ignore those that do not. This bias can compromise the objectivity of the model and lead to skewed results that do not accurately represent the underlying ecological processes.</p>
<p>Moreover, the insistence on including all variables that were initially considered important can result in overly complex models. Such models can be difficult to interpret and may suffer from overfitting, where the model captures noise rather than the true signal in the data. Overfitted models perform well on the data we collected but poorly on new, unseen data. The consequence is a loss of predictive power and generalisability.</p>
<p>Another weakness of theory-driven variable selection is that the reliance on existing theories or the novel, promising hypothsis of the day may lead us to overlook important but unexpected relationships in the data. In complex ecological systems, where our theoretical understanding may be incomplete, some variables could be missed entirely—these might in fact hold the key to the real cause of the ecological patterns we observe. This limitation becomes concerning when studying ecosystems or phenomena that are not well understood or are undergoing rapid changes, such as those affected by climate change or novel anthropogenic pressures.</p>
<p>On the other hand, data-driven approaches, including regularisation techniques, VIF, and forward model variable selection (<a href="multiple_linear_regression.html" class="quarto-xref"><span>Chapter 5</span></a>), allow the data itself to guide variable selection. These methods are increasingly used in today’s era of high-dimensional datasets common in modern ecological research. The primary advantage of data-driven selection lies in its potential for discovery—it can uncover unexpected relationships and generate new hypotheses, which is valuable in complex ecological systems where interactions may not be immediately apparent.</p>
<p>Data-driven methods are well-suited for handling the complexity often encountered in environmental and ecological datasets, where numerous potential predictors may co-occur and interact. They offer a degree of objectivity, reducing the potential for our personal biases in variable selection. But these approaches are not without risks. There’s a danger of identifying relationships that are statistically significant but ecologically meaningless—we refer to this as spurious correlations (e.g.&nbsp;the belief that consuming carrots significantly improves our night vision). Moreover, models with many variables can present significant interpretability challenges, especially when complex interactions are present. This can make it difficult to extract meaningful (plausible) insights from the model and to communicate results to a broader audience.</p>
<p>In practice, the most robust approach to selecting which of the multitude of variables to include in our model often involves a thoughtful combination of theory-driven and data-driven methods. Well-trained ecologists should start with theory-driven variable selection to identify the core predictors based on established ecological principles. We could then employ regularisation techniques to explore additional variables and potential interactions, and use the results to refine our models and generate new hypotheses for future research.</p>
<p>This hybrid approach combines the strengths of both methods. It allows for rigorous hypothesis testing while remaining open to unanticipated and new insights from the data. In ecology, where systems are often characterised by complex, non-linear relationships and interactions that may vary across spatial and temporal scales, this two-pronged approach offers distinct benefits.</p>
<p>Consider how these methods complement theoretical knowledge. Use variable selection methods as tools for prediction, and to assit generating new insights and hypotheses about ecosystems. The choice between theory-driven and data-driven variable selection is not a binary one, but rather a spectrum of approaches.</p>


<!-- -->

</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./non-linear_regression.html" class="pagination-link" aria-label="Nonlinear Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Nonlinear Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./part_B.html" class="pagination-link" aria-label="Non-Parametric Methods">
        <span class="nav-page-text">Non-Parametric Methods</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb38" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb38-1"><a href="#cb38-1"></a><span class="fu"># Regularisation Techniques {#sec-regularisation}</span></span>
<span id="cb38-2"><a href="#cb38-2"></a></span>
<span id="cb38-3"><a href="#cb38-3"></a>Regularisation techniques are invaluable when dealing with complex datasets or situations where traditional methods may fall short. They are used to enhance model stability, improve predictive performance, and increase interpretability, especially when working with multi-dimensional data in multiple linear regression models and multivariate analyses. Regularisation addresses several common challenges in statistical modelling: i) multicollinearity, ii) variable selection, iii) overfitting, and iv) model simplification.</span>
<span id="cb38-4"><a href="#cb38-4"></a></span>
<span id="cb38-5"><a href="#cb38-5"></a>Environmental datasets often contain many independent variables, and it is likely that only some of them are necessary to explain the phenomenon of interest. **Variable selection** is the process of identifying the most important predictors to include in a model. This can be achieved through the application of specialist, domain-specific knowledge, or through statistical or data-driven approaches. Regularisation is an example of the latter, as it can automatically identify the most relevant predictors on statistical grounds, serving as an alternative to traditional variable selection methods such as Variance Inflation Factor (VIF) and stepwise selection (see @sec-multicollinearity and @sec-forward-selection).</span>
<span id="cb38-6"><a href="#cb38-6"></a></span>
<span id="cb38-7"><a href="#cb38-7"></a>**Overfitting** occurs when a model 'explains' the noise in data together with the underlying pattern, which might happen when the model has too many predictors relative to the number of observations. This may also result when variable selection has not been sufficiently addressed. An overfit model performs exceptionally well on training data but fails to generalise to new, unseen data. Additionally, having too many predictors can lead to **multicollinearity** (see @sec-multicollinearity). This is a common issue in multiple linear regression when some of the many predictors included in the model are correlated. Multicollinearity can lead to inflated standard errors, unstable coefficients, and difficulty interpreting the model. Regularisation help manage multicollinearity by shrinking coefficient estimates or setting some to zero.</span>
<span id="cb38-8"><a href="#cb38-8"></a></span>
<span id="cb38-9"><a href="#cb38-9"></a>Effectiveness in variable selection, reducing multicollinearity, and mitigating overfitting all contribute to **model simplification**. Regularisation achieves similar outcomes by shrinking coefficient estimates or setting some to zero, making the model easier to understand, explain, and interpret.</span>
<span id="cb38-10"><a href="#cb38-10"></a></span>
<span id="cb38-11"><a href="#cb38-11"></a>In this chapter, we will discuss three common regularisation techniques: Lasso, Ridge, and Elastic Net Regression.</span>
<span id="cb38-12"><a href="#cb38-12"></a></span>
<span id="cb38-13"><a href="#cb38-13"></a><span class="fu">## Ridge Regression (L2 Regularisation)</span></span>
<span id="cb38-14"><a href="#cb38-14"></a></span>
<span id="cb38-15"><a href="#cb38-15"></a>Ridge regression mathematically 'tames' the wildness of linear regression when faced with multicollinearity. It achieves this by adding a penalty term to the linear regression loss function---a term proportional to the square of the coefficients (the L2 norm). This penalty nudges the coefficients towards zero, effectively shrinking them without forcing them to be exactly zero.</span>
<span id="cb38-16"><a href="#cb38-16"></a></span>
<span id="cb38-17"><a href="#cb38-17"></a>In linear regression, the loss function is typically the Mean Squared Error (MSE), which is the average of the squared residuals (also known as the residual sum of squares, RSS). The optimisation objective is to minimise this loss function. In other words, the linear regression model aims to find the coefficients that minimise the average squared difference between the observed values and the predicted values. The RSS is expressed in @eq-rss:</span>
<span id="cb38-18"><a href="#cb38-18"></a></span>
<span id="cb38-19"><a href="#cb38-19"></a>$$RSS(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2$$ {#eq-rss}</span>
<span id="cb38-20"><a href="#cb38-20"></a></span>
<span id="cb38-21"><a href="#cb38-21"></a>And the MSE, which is the loss function to be minimised, is in @eq-mse:</span>
<span id="cb38-22"><a href="#cb38-22"></a></span>
<span id="cb38-23"><a href="#cb38-23"></a>$$MSE(\beta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2$$ {#eq-mse}</span>
<span id="cb38-24"><a href="#cb38-24"></a></span>
<span id="cb38-25"><a href="#cb38-25"></a>Where:</span>
<span id="cb38-26"><a href="#cb38-26"></a></span>
<span id="cb38-27"><a href="#cb38-27"></a><span class="ss">-   </span>$y_i$ is the observed value for the $i$-th observation.</span>
<span id="cb38-28"><a href="#cb38-28"></a><span class="ss">-   </span>$\beta_0$ is the intercept.</span>
<span id="cb38-29"><a href="#cb38-29"></a><span class="ss">-   </span>$\beta_j$ are the coefficients for the predictors.</span>
<span id="cb38-30"><a href="#cb38-30"></a><span class="ss">-   </span>$x_{ij}$ is the value of the $j$-th predictor variable for the $i$-th observation.</span>
<span id="cb38-31"><a href="#cb38-31"></a><span class="ss">-   </span>$n$ is the number of observations.</span>
<span id="cb38-32"><a href="#cb38-32"></a><span class="ss">-   </span>$p$ is the number of predictors.</span>
<span id="cb38-33"><a href="#cb38-33"></a></span>
<span id="cb38-34"><a href="#cb38-34"></a>The notation $RSS(\beta)$ and $MSE(\beta)$ indicates that these are functions of the coefficients $\beta$. The optimisation objective for linear regression is to find the coefficients $\beta_0$ and $\beta_1$ to $\beta_p$ that minimise the MSE. This can be expressed in Equation @eq-linear-minimisation:</span>
<span id="cb38-35"><a href="#cb38-35"></a></span>
<span id="cb38-36"><a href="#cb38-36"></a>$$\min_{\beta} \left<span class="sc">\{</span> \frac{1}{n} \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 \right<span class="sc">\}</span>$$ {#eq-linear-minimisation}</span>
<span id="cb38-37"><a href="#cb38-37"></a></span>
<span id="cb38-38"><a href="#cb38-38"></a>Ridge regression extends the optimisation of the least squares regression by introducing a penalty term to the loss function. This penalty term is proportional to the square of the L2 norm of the coefficient vector, penalising large coefficient values. Ridge regression is specifically designed to handle multicollinearity and mitigate issues caused by correlated predictors. It also helps prevent overfitting when there are many predictors relative to the sample size, providing a more stable estimation process.</span>
<span id="cb38-39"><a href="#cb38-39"></a></span>
<span id="cb38-40"><a href="#cb38-40"></a>The penalty term is controlled by a hyperparameter<span class="ot">[^regularisation-1]</span> called lambda ($\lambda$) that determines the strength of the penalty. Larger values of $\lambda$ lead to more shrinkage of the coefficients. When $\lambda$ = 0, Ridge Regression is equivalent to ordinary least squares regression. As $\lambda$ approaches infinity, all coefficients (except the intercept) approach zero. To find the optimal $\lambda$, you might have to use techniques like cross-validation. Cross-validation will be discussed later in Section @sec-cross-validation.</span>
<span id="cb38-41"><a href="#cb38-41"></a></span>
<span id="cb38-42"><a href="#cb38-42"></a><span class="ot">[^regularisation-1]: </span>Hyperparameters are configuration settings that are external to your model and not learned from the data itself.</span>
<span id="cb38-43"><a href="#cb38-43"></a></span>
<span id="cb38-44"><a href="#cb38-44"></a>The loss function in Ridge Regression is given by @eq-ridge-loss-function:</span>
<span id="cb38-45"><a href="#cb38-45"></a></span>
<span id="cb38-46"><a href="#cb38-46"></a>$$L_{ridge}(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} \beta_j^2$$ {#eq-ridge-loss-function}</span>
<span id="cb38-47"><a href="#cb38-47"></a></span>
<span id="cb38-48"><a href="#cb38-48"></a>Where $\lambda$ is the regularisation parameter controlling the penalty's strength. Note that typically, the intercept $\beta_0$ is not included in the penalty term.</span>
<span id="cb38-49"><a href="#cb38-49"></a></span>
<span id="cb38-50"><a href="#cb38-50"></a>In Equation @eq-ridge-loss-function, $L_{ridge}(\beta)$ is the Ridge Regression loss function. This loss function includes the residual sum of squares (RSS) plus a penalty term $\lambda \sum_{j=1}^p \beta_j^2$. The optimisation objective in Ridge Regression is to find the values of the coefficients $\beta_1$ through $\beta_p$ that minimise this penalised loss function, while also finding the optimal value for the intercept $\beta_0$.</span>
<span id="cb38-51"><a href="#cb38-51"></a></span>
<span id="cb38-52"><a href="#cb38-52"></a>Ridge regression introduces a bias-variance trade-off. By shrinking the coefficients, it introduces a slight bias, as the model's predictions may not perfectly match the training data. However, this bias is often offset by a significant reduction in variance. The reduced variance means the model's predictions are more stable and less sensitive to small changes in the input data. This trade-off often results in improved overall predictive performance, especially on new, unseen data.</span>
<span id="cb38-53"><a href="#cb38-53"></a></span>
<span id="cb38-54"><a href="#cb38-54"></a>So, Ridge Regression sacrifices a bit of bias (accuracy on the sample data) to gain a lot in terms of reduced variance (generalisation to new data). This is a typical example of the bias-variance trade-off in statistical modelling and machine learning, where we often find that a bit of bias can lead to a much more robust and reliable model.</span>
<span id="cb38-55"><a href="#cb38-55"></a></span>
<span id="cb38-56"><a href="#cb38-56"></a>Unlike some other regularisation methods, such as principal component regression, Ridge Regression maintains the interpretability of the coefficients in terms of their relationship with the outcome. It is also versatile and can be applied to various types of regression models, including linear and logistic regression.</span>
<span id="cb38-57"><a href="#cb38-57"></a></span>
<span id="cb38-58"><a href="#cb38-58"></a><span class="fu">## Lasso Regression (L1 Regularisation)</span></span>
<span id="cb38-59"><a href="#cb38-59"></a></span>
<span id="cb38-60"><a href="#cb38-60"></a>Lasso (Least Absolute Shrinkage and Selection Operator) regression employs a different penalty term compared to Ridge Regression. Instead of squaring the coefficients, Lasso Regression takes their absolute values. The cost function in Lasso Regression is given in Equation @eq-lasso-loss-function:</span>
<span id="cb38-61"><a href="#cb38-61"></a></span>
<span id="cb38-62"><a href="#cb38-62"></a>$$L_{lasso}(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j|$$ {#eq-lasso-loss-function}</span>
<span id="cb38-63"><a href="#cb38-63"></a></span>
<span id="cb38-64"><a href="#cb38-64"></a>In Equation @eq-lasso-loss-function, $L_{lasso}(\beta)$ is the Lasso Regression loss function. It includes the residual sum of squares (RSS) plus a penalty term $\lambda \sum_{j=1}^{p} |\beta_j|$ (L1 norm). This penalty term is the sum of the absolute values of the coefficients, scaled by the regularisation parameter $\lambda$ (similar to Ridge Regression). Lasso regression seeks the values of $\beta_0$ through $\beta_p$ that minimise $L_{lasso}(\beta)$. As with Ridge Regression, the intercept $\beta_0$ is typically not included in the penalty term.</span>
<span id="cb38-65"><a href="#cb38-65"></a></span>
<span id="cb38-66"><a href="#cb38-66"></a>The strength of Lasso Regression lies in its ability to shrink some coefficients all the way to zero, effectively eliminating those variables from the model. This automatic variable selection makes Lasso Regression well-suited for creating sparse models where only the most influential variables are retained. This simplification aids in interpretation and can enhance model performance by reducing noise and overfitting.</span>
<span id="cb38-67"><a href="#cb38-67"></a></span>
<span id="cb38-68"><a href="#cb38-68"></a>Lasso Regression still applies a degree of shrinkage for the coefficients that are not shrunk to zero. Shrinkage reduces their variance and provide more stable models that are less sensitive to fluctuations in the data. Similar to Ridge Regression, Lasso involves a trade-off between bias and variance. The shrinkage introduces a small bias but can greatly reduce variance and result in better overall predictions.</span>
<span id="cb38-69"><a href="#cb38-69"></a></span>
<span id="cb38-70"><a href="#cb38-70"></a>Lasso regression is useful when dealing with datasets that have a large number of potential predictor variables. It helps identify the most relevant predictors. The end results is a simpler and more interpretable model. If you suspect redundancy among your predictor variables, Lasso can prune them and retain only those that provide the best predictive value. As always, the optimal value for $\lambda$ should be determined through techniques like cross-validation.</span>
<span id="cb38-71"><a href="#cb38-71"></a></span>
<span id="cb38-72"><a href="#cb38-72"></a><span class="fu">## Elastic Net Regression</span></span>
<span id="cb38-73"><a href="#cb38-73"></a></span>
<span id="cb38-74"><a href="#cb38-74"></a>Elastic net regression is a hybrid regularisation technique that combines the penalties of Ridge and Lasso Regression. It tries to provide the advantages of both methods and mitigate their drawbacks.</span>
<span id="cb38-75"><a href="#cb38-75"></a></span>
<span id="cb38-76"><a href="#cb38-76"></a>Here, the penalty term is the weighted average of the L1 (Lasso) and L2 (Ridge) penalties. A mixing parameter called alpha ($\alpha$) controls the weighting between the two penalties. When $\alpha$ = 0, Elastic Net is equivalent to Ridge Regression and when $\alpha$ = 1 it is equivalent to Lasso Regression. For values of $\alpha$ between 0 and 1, Elastic Net blends the properties of both methods and provides some flexibility to regularisation.</span>
<span id="cb38-77"><a href="#cb38-77"></a></span>
<span id="cb38-78"><a href="#cb38-78"></a>The cost function in Elastic Net Regression is given in @eq-elastic-net-loss-function:</span>
<span id="cb38-79"><a href="#cb38-79"></a></span>
<span id="cb38-80"><a href="#cb38-80"></a>$$L_{e_net}(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \left( \alpha \sum_{j=1}^{p} |\beta_j| + (1 - \alpha) \sum_{j=1}^{p} \beta_j^2 \right)$$ {#eq-elastic-net-loss-function}</span>
<span id="cb38-81"><a href="#cb38-81"></a></span>
<span id="cb38-82"><a href="#cb38-82"></a>Where $\alpha$ is the mixing parameter, with $0 \leq \alpha \leq 1$.</span>
<span id="cb38-83"><a href="#cb38-83"></a></span>
<span id="cb38-84"><a href="#cb38-84"></a>In @eq-elastic-net-loss-function there is the familiar RSS plus the combined penalty term that is a weighted sum of the L1 and L2 norms. The objective of Elastic Net Regression is again to minimise $L_{e_net}(\beta)$ by seeking optimal values of $\beta_1$ through $\beta_p$.</span>
<span id="cb38-85"><a href="#cb38-85"></a></span>
<span id="cb38-86"><a href="#cb38-86"></a>Like the other regularisation techniques, Elastic Net is also used when you have highly correlated predictors. While Lasso Regression might arbitrarily select one variable from a group and ignore the rest, Elastic Net tends to select groups of correlated features together and so provide a more comprehensive understanding of variable importance. The flexibility of adjusting the $\alpha$ parameter allows you to fine-tune the regularisation to best suit your specific dataset and modelling goals. It balances variable selection (Lasso) and shrinkage (Ridge). Also, Elastic Net can outperform Lasso and Ridge Regression in terms of prediction accuracy when dealing with high-dimensional datasets where the number of predictors exceeds the number of observations.</span>
<span id="cb38-87"><a href="#cb38-87"></a></span>
<span id="cb38-88"><a href="#cb38-88"></a>Elastic net is a good option if you have a dataset with many potential predictor variables and suspect strong correlations among them. Use it when you are uncertain whether pure variable selection (Lasso) or pure shrinkage (Ridge) is the best approach. The challenge is that now we also have to tune the $\alpha$ parameter in addition to the regularisation parameter $\lambda$. A caveat is that Elastic Net retains the interpretability of individual coefficients but the interpretation becomes slightly more nuanced due to the mixed penalty term. This requires a thoughtful approach to understanding the model outputs.</span>
<span id="cb38-89"><a href="#cb38-89"></a></span>
<span id="cb38-90"><a href="#cb38-90"></a><span class="fu">## Cross-Validation {#sec-cross-validation}</span></span>
<span id="cb38-91"><a href="#cb38-91"></a></span>
<span id="cb38-92"><a href="#cb38-92"></a>The values of the hyperparameters ($\lambda$ or $\alpha$) significantly affect the model's performance and generalisation ability and so it necessitates careful optimisation. The <span class="in">`cv.glmnet()`</span> function (see @sec-r-function) automates this process by performing both hyperparameter tuning<span class="ot">[^regularisation-2]</span> and cross-validation. It systematically evaluates different combinations of $\lambda$ or $\alpha$ values across multiple subsets of our data, using cross-validation to estimate their out-of-sample performance. This allows for the selection of the hyperparameter combination that yields the best performance and thus avoids the risk of overfitting and improves model generalisation.</span>
<span id="cb38-93"><a href="#cb38-93"></a></span>
<span id="cb38-94"><a href="#cb38-94"></a><span class="ot">[^regularisation-2]: </span>The goal of hyperparameter tuning is to find the optimal combination of hyperparameters that leads to the best model performance on your specific dataset. This is done by systematically evaluating different hyperparameter values and selecting the combination that yields the best results.</span>
<span id="cb38-95"><a href="#cb38-95"></a></span>
<span id="cb38-96"><a href="#cb38-96"></a>The most widely used cross-validation method is $\text{k}$-fold cross-validation. The dataset is divided into $\text{k}$ equally sized subsets (specified by the user). The subsets are called 'folds'. The model is then trained $\text{k}$ times, each time using $\text{k}-1$ folds for training and the remaining fold for validation. It provides a robust estimate of model performance by utilising all data points for both training and validation. It balances computational cost and bias reduction. But, the choice of k can influence results, and there's a trade-off between bias and variance: lower $\text{k}$ values may lead to higher bias but lower variance, whilst higher $\text{k}$ values do the opposite.</span>
<span id="cb38-97"><a href="#cb38-97"></a></span>
<span id="cb38-98"><a href="#cb38-98"></a>The general approach taken in $\text{k}$-fold cross validation is that, for each combination of hyperparameter values, we:</span>
<span id="cb38-99"><a href="#cb38-99"></a></span>
<span id="cb38-100"><a href="#cb38-100"></a><span class="ss">1.  </span>Perform $\text{k}$-fold cross-validation on the training data.</span>
<span id="cb38-101"><a href="#cb38-101"></a><span class="ss">2.  </span>Calculate the average performance metric (e.g., mean squared error) across all folds.</span>
<span id="cb38-102"><a href="#cb38-102"></a><span class="ss">3.  </span>Select the hyperparameter values that produced the best average performance.</span>
<span id="cb38-103"><a href="#cb38-103"></a></span>
<span id="cb38-104"><a href="#cb38-104"></a>This ensures that the hyperparameters we select are robust and generalissable to unseen data, rather than being overly influenced by the peculiarities of a single training set.</span>
<span id="cb38-105"><a href="#cb38-105"></a></span>
<span id="cb38-106"><a href="#cb38-106"></a>*K*-fold cross-validation is the most frequently-used form of cross-validation, but several other types exist. Some of them are:</span>
<span id="cb38-107"><a href="#cb38-107"></a></span>
<span id="cb38-108"><a href="#cb38-108"></a>**Leave-one-out cross-validation (LOOCV)** is an extreme case of $\text{k}$-fold cross-validation where $\text{k}$ equals the number of data points. This method trains the model on all but one data point and validates on the left-out point, repeating this process for each data point. LOOCV provides an nearly unbiased estimate of model performance but can be computationally expensive for large datasets. It's most often used for small datasets where maximising training data is important. The downside is that LOOCV can suffer from high variance, especially for noisy datasets.</span>
<span id="cb38-109"><a href="#cb38-109"></a></span>
<span id="cb38-110"><a href="#cb38-110"></a>**Stratified cross-validation** ensures each fold maintains the same proportion of samples for each class as in the complete dataset. It useful for imbalanced datasets or when dealing with categorical outcomes. By preserving the class distribution in each fold, stratified cross-validation provides a more representative evaluation of model performance across all classes. Implementing stratification can be complex for multi-class problems or continuous outcomes.</span>
<span id="cb38-111"><a href="#cb38-111"></a></span>
<span id="cb38-112"><a href="#cb38-112"></a>**Holdout validation** is the simplest form of cross-validation. The dataset is split into a training set and a test set. Typically, about 70-80% of the data is used for training and the balance is reserved for testing. The model is trained on the training set and then evaluated on the held-out test set. It is computationally efficient and provides a quick estimate of model performance but it has several limitations. Firstly, because it doesn't make full use of the available data for training, it can be an issue for smaller datasets. Secondly, the results can be highly dependent on the particular split chosen, leading to high variance in performance estimates. This is especially true for smaller datasets or when the split doesn't represent the overall data distribution well. But holdout validation remains useful for large datasets or as a quick initial assessment before applying more complex cross-validation techniques.</span>
<span id="cb38-113"><a href="#cb38-113"></a></span>
<span id="cb38-114"><a href="#cb38-114"></a>The examples will show $\text{k}$-fold cross validation, but you can easily adapt the code to use other cross-validation methods.</span>
<span id="cb38-115"><a href="#cb38-115"></a></span>
<span id="cb38-116"><a href="#cb38-116"></a><span class="fu">## R Function {#sec-r-function}</span></span>
<span id="cb38-117"><a href="#cb38-117"></a></span>
<span id="cb38-118"><a href="#cb38-118"></a>In R, the **glmnet** package provides functions for fitting regularised linear models. The <span class="in">`cv.glmnet()`</span> function performs cross-validated regularisation path selection for the Elastic Net, Lasso, and Ridge Regression models.</span>
<span id="cb38-119"><a href="#cb38-119"></a></span>
<span id="cb38-122"><a href="#cb38-122"></a><span class="in">```{r}</span></span>
<span id="cb38-123"><a href="#cb38-123"></a><span class="co">#| eval: false</span></span>
<span id="cb38-124"><a href="#cb38-124"></a><span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> <span class="cn">NULL</span>, <span class="at">nfolds =</span> <span class="dv">10</span>,</span>
<span id="cb38-125"><a href="#cb38-125"></a>          <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb38-126"><a href="#cb38-126"></a><span class="in">```</span></span>
<span id="cb38-127"><a href="#cb38-127"></a></span>
<span id="cb38-128"><a href="#cb38-128"></a>The function takes the following arguments:</span>
<span id="cb38-129"><a href="#cb38-129"></a></span>
<span id="cb38-130"><a href="#cb38-130"></a><span class="ss">-   </span><span class="in">`x`</span>: A matrix of predictors.</span>
<span id="cb38-131"><a href="#cb38-131"></a><span class="ss">-   </span><span class="in">`y`</span>: A matrix of response variables (but read the help file as this varies depending on the data type).</span>
<span id="cb38-132"><a href="#cb38-132"></a><span class="ss">-   </span><span class="in">`alpha`</span>: The mixing parameter for the Elastic Net penalty. When <span class="in">`alpha = 0`</span>, the model is a Ridge Regression. When <span class="in">`alpha = 1`</span>, the model is a Lasso Regression. The default value is <span class="in">`alpha = 1`</span>.</span>
<span id="cb38-133"><a href="#cb38-133"></a><span class="ss">-   </span><span class="in">`lambda`</span>: A vector of regularisation parameters. The function fits a model for each value of <span class="in">`lambda`</span> and selects the best one based on cross-validation. The default is <span class="in">`lambda = NULL`</span>, which means the function will generate a sequence of <span class="in">`100`</span> values between <span class="in">`10^-2`</span> and <span class="in">`10^2`</span>.</span>
<span id="cb38-134"><a href="#cb38-134"></a><span class="ss">-   </span><span class="in">`nfolds`</span>: The number of folds in the cross-validation. The default is <span class="in">`nfolds = 10`</span>.</span>
<span id="cb38-135"><a href="#cb38-135"></a><span class="ss">-   </span><span class="in">`standardize`</span>: A logical value indicating whether the predictors should be standardised. The default is <span class="in">`standardize = TRUE`</span>.</span>
<span id="cb38-136"><a href="#cb38-136"></a></span>
<span id="cb38-137"><a href="#cb38-137"></a>It is not clearly documented in the function's help file, but the 'glm' in the function name indicates that the function fits a generalised linear model. This implies 'gaussian,' 'binomial,' 'poisson,' 'multinomial,' 'cox,' and 'mgaussian' families are supported, which can be supplied via the <span class="in">`family`</span> argument to the function. The 'net' part of the name indicates that the function fits an Elastic Net, thus allowing choose between Lasso and Ridge by setting <span class="in">`alpha`</span> to 1 or 0 (or something in-between). The 'cv' part of the name indicates that the function performs cross-validation.</span>
<span id="cb38-138"><a href="#cb38-138"></a></span>
<span id="cb38-139"><a href="#cb38-139"></a><span class="fu">## Example 1: Ridge Regression</span></span>
<span id="cb38-140"><a href="#cb38-140"></a></span>
<span id="cb38-141"><a href="#cb38-141"></a>The data I use here should be well-known by now. They are the same seaweed dataset used throughout @sec-multiple-linear-regression. I will use Ridge Regression to predict the response variable <span class="in">`Y`</span> using the predictors <span class="in">`annMean`</span>, <span class="in">`augMean`</span>, <span class="in">`augSD`</span>, <span class="in">`febSD`</span>, and <span class="in">`febRange`</span>.</span>
<span id="cb38-142"><a href="#cb38-142"></a></span>
<span id="cb38-143"><a href="#cb38-143"></a>First, I will read in the data and prepare them in the format required by <span class="in">`cv.glmnet()`</span>. This involves standardising the response variable and predictors and converting them to matrices. I specify the range of $\lambda$ values to try and set up 10-fold cross-validation. I then fit the model and plot the results of the cross-validation.</span>
<span id="cb38-144"><a href="#cb38-144"></a></span>
<span id="cb38-147"><a href="#cb38-147"></a><span class="in">```{r}</span></span>
<span id="cb38-148"><a href="#cb38-148"></a><span class="co"># Ridge Regression with Cross-Validation</span></span>
<span id="cb38-149"><a href="#cb38-149"></a></span>
<span id="cb38-150"><a href="#cb38-150"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb38-151"><a href="#cb38-151"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb38-152"><a href="#cb38-152"></a></span>
<span id="cb38-153"><a href="#cb38-153"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb38-154"><a href="#cb38-154"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb38-155"><a href="#cb38-155"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb38-156"><a href="#cb38-156"></a></span>
<span id="cb38-157"><a href="#cb38-157"></a><span class="co"># Read the data</span></span>
<span id="cb38-158"><a href="#cb38-158"></a>sw <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"data/spp_df2.csv"</span>)</span>
<span id="cb38-159"><a href="#cb38-159"></a></span>
<span id="cb38-160"><a href="#cb38-160"></a><span class="co"># Standardise the response variable and present as a matrix</span></span>
<span id="cb38-161"><a href="#cb38-161"></a>y <span class="ot">&lt;-</span> sw <span class="sc">|&gt;</span></span>
<span id="cb38-162"><a href="#cb38-162"></a>  <span class="fu">select</span>(Y) <span class="sc">|&gt;</span></span>
<span id="cb38-163"><a href="#cb38-163"></a>  <span class="fu">scale</span>(<span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">FALSE</span>) <span class="sc">|&gt;</span></span>
<span id="cb38-164"><a href="#cb38-164"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb38-165"><a href="#cb38-165"></a></span>
<span id="cb38-166"><a href="#cb38-166"></a><span class="co"># Provide the predictors as a matrix</span></span>
<span id="cb38-167"><a href="#cb38-167"></a>X <span class="ot">&lt;-</span> sw <span class="sc">|&gt;</span></span>
<span id="cb38-168"><a href="#cb38-168"></a>  <span class="fu">select</span>(<span class="sc">-</span>X, <span class="sc">-</span>dist, <span class="sc">-</span>bio, <span class="sc">-</span>Y, <span class="sc">-</span>Y1, <span class="sc">-</span>Y2) <span class="sc">|&gt;</span></span>
<span id="cb38-169"><a href="#cb38-169"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb38-170"><a href="#cb38-170"></a></span>
<span id="cb38-171"><a href="#cb38-171"></a><span class="co"># Set up lambda sequence</span></span>
<span id="cb38-172"><a href="#cb38-172"></a>lambdas_to_try <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="sc">^</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb38-173"><a href="#cb38-173"></a></span>
<span id="cb38-174"><a href="#cb38-174"></a><span class="co"># Perform 10-fold cross-validation</span></span>
<span id="cb38-175"><a href="#cb38-175"></a>ridge_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambdas_to_try,</span>
<span id="cb38-176"><a href="#cb38-176"></a>  <span class="at">standardize =</span> <span class="cn">TRUE</span>, <span class="at">nfolds =</span> <span class="dv">10</span>)</span>
<span id="cb38-177"><a href="#cb38-177"></a><span class="in">```</span></span>
<span id="cb38-178"><a href="#cb38-178"></a></span>
<span id="cb38-181"><a href="#cb38-181"></a><span class="in">```{r}</span></span>
<span id="cb38-182"><a href="#cb38-182"></a><span class="co">#| eval: false</span></span>
<span id="cb38-183"><a href="#cb38-183"></a><span class="co"># Plot cross-validation results (ggplot shown)</span></span>
<span id="cb38-184"><a href="#cb38-184"></a><span class="fu">plot</span>(ridge_cv)</span>
<span id="cb38-185"><a href="#cb38-185"></a><span class="in">```</span></span>
<span id="cb38-186"><a href="#cb38-186"></a></span>
<span id="cb38-189"><a href="#cb38-189"></a><span class="in">```{r}</span></span>
<span id="cb38-190"><a href="#cb38-190"></a><span class="co">#| echo: false</span></span>
<span id="cb38-191"><a href="#cb38-191"></a><span class="co">#| fig-width: 4</span></span>
<span id="cb38-192"><a href="#cb38-192"></a><span class="co">#| fig-height: 2.15</span></span>
<span id="cb38-193"><a href="#cb38-193"></a><span class="co">#| fig.cap: "Cross-validation statistics for the Ridge Regression approach applied to the seaweed data."</span></span>
<span id="cb38-194"><a href="#cb38-194"></a><span class="co">#| label: fig-ridge-regression</span></span>
<span id="cb38-195"><a href="#cb38-195"></a></span>
<span id="cb38-196"><a href="#cb38-196"></a>ridge_cv_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">lambda =</span> ridge_cv<span class="sc">$</span>lambda,</span>
<span id="cb38-197"><a href="#cb38-197"></a>                          <span class="at">cvm =</span> ridge_cv<span class="sc">$</span>cvm,</span>
<span id="cb38-198"><a href="#cb38-198"></a>                          <span class="at">cvmd =</span> ridge_cv<span class="sc">$</span>cvsd,</span>
<span id="cb38-199"><a href="#cb38-199"></a>                          <span class="at">nzero =</span> ridge_cv<span class="sc">$</span>nzero)</span>
<span id="cb38-200"><a href="#cb38-200"></a></span>
<span id="cb38-201"><a href="#cb38-201"></a><span class="fu">ggplot</span>(<span class="at">data =</span> ridge_cv_df, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(lambda), <span class="at">y =</span> cvm)) <span class="sc">+</span></span>
<span id="cb38-202"><a href="#cb38-202"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> cvm <span class="sc">-</span> cvmd, <span class="at">ymax =</span> cvm <span class="sc">+</span> cvmd),</span>
<span id="cb38-203"><a href="#cb38-203"></a>                <span class="at">width =</span> <span class="fl">0.0</span>, <span class="at">colour =</span> <span class="st">"dodgerblue4"</span>,</span>
<span id="cb38-204"><a href="#cb38-204"></a>                <span class="at">linewidth =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb38-205"><a href="#cb38-205"></a>  <span class="fu">geom_point</span>(<span class="at">colour =</span> <span class="st">"dodgerblue4"</span>, <span class="at">fill =</span> <span class="st">"white"</span>,</span>
<span id="cb38-206"><a href="#cb38-206"></a>             <span class="at">shape =</span> <span class="dv">21</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb38-207"><a href="#cb38-207"></a>  <span class="fu">geom_line</span>(<span class="at">colour =</span> <span class="st">"dodgerblue4"</span>, <span class="at">linewidth =</span> <span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb38-208"><a href="#cb38-208"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> nzero <span class="sc">/</span> <span class="fu">max</span>(nzero) <span class="sc">*</span> <span class="fu">max</span>(cvm)),</span>
<span id="cb38-209"><a href="#cb38-209"></a>            <span class="at">colour =</span> <span class="st">"magenta"</span>) <span class="sc">+</span></span>
<span id="cb38-210"><a href="#cb38-210"></a>  <span class="fu">scale_y_continuous</span>(</span>
<span id="cb38-211"><a href="#cb38-211"></a>    <span class="at">name =</span> <span class="st">"Mean Squared Error"</span>,</span>
<span id="cb38-212"><a href="#cb38-212"></a>    <span class="at">sec.axis =</span> <span class="fu">sec_axis</span>(<span class="sc">~</span> . <span class="sc">/</span> <span class="fu">max</span>(ridge_cv_df<span class="sc">$</span>cvm) <span class="sc">*</span> <span class="fu">max</span>(ridge_cv_df<span class="sc">$</span>nzero),</span>
<span id="cb38-213"><a href="#cb38-213"></a>                        <span class="at">name =</span> <span class="st">"No. Non-zero Coef."</span>)</span>
<span id="cb38-214"><a href="#cb38-214"></a>  ) <span class="sc">+</span></span>
<span id="cb38-215"><a href="#cb38-215"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">log</span>(lambda))) <span class="sc">+</span></span>
<span id="cb38-216"><a href="#cb38-216"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb38-217"><a href="#cb38-217"></a>  <span class="fu">theme</span>(</span>
<span id="cb38-218"><a href="#cb38-218"></a>    <span class="at">axis.title.y.right =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">"magenta"</span>),</span>
<span id="cb38-219"><a href="#cb38-219"></a>    <span class="at">axis.title.y.left =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">"dodgerblue4"</span>),</span>
<span id="cb38-220"><a href="#cb38-220"></a>    <span class="at">axis.text.y.right =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">"magenta"</span>),</span>
<span id="cb38-221"><a href="#cb38-221"></a>    <span class="at">axis.text.y.left =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">"dodgerblue4"</span>)</span>
<span id="cb38-222"><a href="#cb38-222"></a>  ) <span class="sc">+</span></span>
<span id="cb38-223"><a href="#cb38-223"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">log</span>(ridge_cv<span class="sc">$</span>lambda.min), <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb38-224"><a href="#cb38-224"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">log</span>(ridge_cv<span class="sc">$</span>lambda<span class="fl">.1</span>se), <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb38-225"><a href="#cb38-225"></a>  <span class="fu">geom_label</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(ridge_cv<span class="sc">$</span>lambda.min), <span class="at">y =</span> <span class="fu">max</span>(cvm) <span class="sc">-</span> <span class="fl">0.0035</span>),</span>
<span id="cb38-226"><a href="#cb38-226"></a>             <span class="at">label =</span> <span class="fu">expression</span>(lambda[min]), <span class="at">vjust =</span> <span class="sc">-</span><span class="dv">1</span>,</span>
<span id="cb38-227"><a href="#cb38-227"></a>             <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb38-228"><a href="#cb38-228"></a>  <span class="fu">geom_label</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(ridge_cv<span class="sc">$</span>lambda<span class="fl">.1</span>se), <span class="at">y =</span> <span class="fu">max</span>(cvm) <span class="sc">-</span> <span class="fl">0.0035</span>),</span>
<span id="cb38-229"><a href="#cb38-229"></a>             <span class="at">label =</span> <span class="fu">expression</span>(lambda[<span class="st">`</span><span class="at">1se</span><span class="st">`</span>]), <span class="at">vjust =</span> <span class="sc">-</span><span class="dv">1</span>,</span>
<span id="cb38-230"><a href="#cb38-230"></a>             <span class="at">size =</span> <span class="dv">3</span>)</span>
<span id="cb38-231"><a href="#cb38-231"></a><span class="in">```</span></span>
<span id="cb38-232"><a href="#cb38-232"></a></span>
<span id="cb38-233"><a href="#cb38-233"></a>@fig-ridge-regression, generated from the <span class="in">`cv.glmnet()`</span> object, illustrates the relationship between the regularisation parameter $\lambda$ and the model's cross-validation performance. The *y*-axis represents the mean squared error (MSE) from cross-validation, whilst the *x*-axis shows the $log(\lambda)$ values tested. Red dots indicate the mean MSE for each $\lambda$, with error bars showing ±1 standard error. Two vertical dashed lines highlight important $\lambda$ values: $\lambda_{min}$, which minimises the mean MSE, and $\lambda_{1se}$, the largest $\lambda$ within one standard error of the minimum MSE. One may select the optimal $\lambda$ using either the $\lambda_{min}$ or the $\lambda_{1se}$ rule, accessible via <span class="in">`cv.glmnet_object$lambda.min`</span> and <span class="in">`cv.glmnet_object$lambda.1se`</span>, respectively. To utilise the chosen $\lambda$, one refits the model using <span class="in">`glmnet()`</span> and extract the coefficients.</span>
<span id="cb38-234"><a href="#cb38-234"></a></span>
<span id="cb38-235"><a href="#cb38-235"></a>For performance evaluation, one can calculate the sum of squared residuals (SSR) as the sum of squared differences between observed and predicted values, and the R-squared value as the square of the correlation between observed and predicted values, representing the proportion of variance in the dependent variable that is predictable from the independent variable(s).</span>
<span id="cb38-236"><a href="#cb38-236"></a></span>
<span id="cb38-237"><a href="#cb38-237"></a>The results show that the model explains 67.07% of the variance in the response variable:</span>
<span id="cb38-238"><a href="#cb38-238"></a></span>
<span id="cb38-241"><a href="#cb38-241"></a><span class="in">```{r}</span></span>
<span id="cb38-242"><a href="#cb38-242"></a><span class="co"># Fit models and calculate performance metrics</span></span>
<span id="cb38-243"><a href="#cb38-243"></a>fit_model_and_calculate_metrics <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, lambda) {</span>
<span id="cb38-244"><a href="#cb38-244"></a>  model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda,</span>
<span id="cb38-245"><a href="#cb38-245"></a>                  <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb38-246"><a href="#cb38-246"></a>  y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, X)</span>
<span id="cb38-247"><a href="#cb38-247"></a>  ssr <span class="ot">&lt;-</span> <span class="fu">sum</span>((y <span class="sc">-</span> y_hat) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb38-248"><a href="#cb38-248"></a>  rsq <span class="ot">&lt;-</span> <span class="fu">cor</span>(y, y_hat) <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb38-249"><a href="#cb38-249"></a>  <span class="fu">list</span>(<span class="at">model =</span> model, <span class="at">ssr =</span> ssr, <span class="at">rsq =</span> rsq)</span>
<span id="cb38-250"><a href="#cb38-250"></a>}</span>
<span id="cb38-251"><a href="#cb38-251"></a></span>
<span id="cb38-252"><a href="#cb38-252"></a><span class="co"># Best cross-validated lambda</span></span>
<span id="cb38-253"><a href="#cb38-253"></a>lambda_cv <span class="ot">&lt;-</span> ridge_cv<span class="sc">$</span>lambda.min</span>
<span id="cb38-254"><a href="#cb38-254"></a>mod_cv <span class="ot">&lt;-</span> <span class="fu">fit_model_and_calculate_metrics</span>(X, y, lambda_cv)</span>
<span id="cb38-255"><a href="#cb38-255"></a></span>
<span id="cb38-256"><a href="#cb38-256"></a><span class="co"># Print results</span></span>
<span id="cb38-257"><a href="#cb38-257"></a>mod_cv</span>
<span id="cb38-258"><a href="#cb38-258"></a><span class="in">```</span></span>
<span id="cb38-259"><a href="#cb38-259"></a></span>
<span id="cb38-260"><a href="#cb38-260"></a>As already indicated, an alternative to using <span class="in">`lambda.min`</span> for selecting the optimal $\lambda$ value is to use the 1 SE rule, which is contained in the attribute <span class="in">`lambda.1se`</span>. This reduces the risk of overfitting as it tends to select a simpler model. We can use this value to refit the model and extract the coefficients, as before.</span>
<span id="cb38-261"><a href="#cb38-261"></a></span>
<span id="cb38-262"><a href="#cb38-262"></a>AIC and BIC can also be used to select suitable models. These information criteria penalise the model for the number of parameters used, providing a balance between model complexity and goodness of fit. The <span class="in">`calculate_ic()`</span> function below calculates the AIC and BIC for a given model and returns the results in a list. We can then use this function to calculate the AIC and BIC for each model fit with each $\lambda$ in <span class="in">`lambdas_to_try`</span>:</span>
<span id="cb38-263"><a href="#cb38-263"></a></span>
<span id="cb38-266"><a href="#cb38-266"></a><span class="in">```{r}</span></span>
<span id="cb38-267"><a href="#cb38-267"></a><span class="co"># Calculate AIC and BIC</span></span>
<span id="cb38-268"><a href="#cb38-268"></a>calculate_ic <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, lambda) {</span>
<span id="cb38-269"><a href="#cb38-269"></a>  model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda,</span>
<span id="cb38-270"><a href="#cb38-270"></a>                  <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb38-271"><a href="#cb38-271"></a>  betas <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(<span class="fu">coef</span>(model)[<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb38-272"><a href="#cb38-272"></a>  resid <span class="ot">&lt;-</span> y <span class="sc">-</span> (<span class="fu">scale</span>(X) <span class="sc">%*%</span> betas)</span>
<span id="cb38-273"><a href="#cb38-273"></a>  H <span class="ot">&lt;-</span> <span class="fu">scale</span>(X) <span class="sc">%*%</span></span>
<span id="cb38-274"><a href="#cb38-274"></a>    <span class="fu">solve</span>(<span class="fu">t</span>(<span class="fu">scale</span>(X)) <span class="sc">%*%</span> <span class="fu">scale</span>(X) <span class="sc">+</span> lambda <span class="sc">*</span></span>
<span id="cb38-275"><a href="#cb38-275"></a>            <span class="fu">diag</span>(<span class="fu">ncol</span>(X))) <span class="sc">%*%</span> <span class="fu">t</span>(<span class="fu">scale</span>(X))</span>
<span id="cb38-276"><a href="#cb38-276"></a>  df <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(H))</span>
<span id="cb38-277"><a href="#cb38-277"></a>  log_resid_ss <span class="ot">&lt;-</span> <span class="fu">log</span>(<span class="fu">sum</span>(resid <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb38-278"><a href="#cb38-278"></a>  aic <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X) <span class="sc">*</span> log_resid_ss <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> df</span>
<span id="cb38-279"><a href="#cb38-279"></a>  bic <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X) <span class="sc">*</span> log_resid_ss <span class="sc">+</span> <span class="fu">log</span>(<span class="fu">nrow</span>(X)) <span class="sc">*</span> df</span>
<span id="cb38-280"><a href="#cb38-280"></a>  <span class="fu">list</span>(<span class="at">aic =</span> aic, <span class="at">bic =</span> bic)</span>
<span id="cb38-281"><a href="#cb38-281"></a>}</span>
<span id="cb38-282"><a href="#cb38-282"></a></span>
<span id="cb38-283"><a href="#cb38-283"></a>ic_results <span class="ot">&lt;-</span> <span class="fu">map</span>(lambdas_to_try, <span class="sc">~</span> <span class="fu">calculate_ic</span>(X, y, .x)) <span class="sc">|&gt;</span></span>
<span id="cb38-284"><a href="#cb38-284"></a>  <span class="fu">transpose</span>()</span>
<span id="cb38-285"><a href="#cb38-285"></a><span class="in">```</span></span>
<span id="cb38-286"><a href="#cb38-286"></a></span>
<span id="cb38-287"><a href="#cb38-287"></a>A plot of the change in the information criteria with $log(\lambda)$ is shown in @fig-aic-bic. The optimal $\lambda$ values according to both AIC and BIC can then be used to refit the model and arrive at the coefficients of interest.</span>
<span id="cb38-288"><a href="#cb38-288"></a></span>
<span id="cb38-291"><a href="#cb38-291"></a><span class="in">```{r}</span></span>
<span id="cb38-292"><a href="#cb38-292"></a><span class="co">#| fig-width: 3.5</span></span>
<span id="cb38-293"><a href="#cb38-293"></a><span class="co">#| fig-height: 2.15</span></span>
<span id="cb38-294"><a href="#cb38-294"></a><span class="co">#| fig.cap: "Plot of information criteria for best model fit selected through Ridge Regression."</span></span>
<span id="cb38-295"><a href="#cb38-295"></a><span class="co">#| label: fig-aic-bic</span></span>
<span id="cb38-296"><a href="#cb38-296"></a></span>
<span id="cb38-297"><a href="#cb38-297"></a><span class="co"># Plot information criteria</span></span>
<span id="cb38-298"><a href="#cb38-298"></a>plot_ic <span class="ot">&lt;-</span> <span class="cf">function</span>(lambdas, ic_results) {</span>
<span id="cb38-299"><a href="#cb38-299"></a>  df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">lambda =</span> <span class="fu">log</span>(lambdas),</span>
<span id="cb38-300"><a href="#cb38-300"></a>                   <span class="at">aic =</span> <span class="fu">unlist</span>(ic_results<span class="sc">$</span>aic),</span>
<span id="cb38-301"><a href="#cb38-301"></a>                   <span class="at">bic =</span> <span class="fu">unlist</span>(ic_results<span class="sc">$</span>bic))</span>
<span id="cb38-302"><a href="#cb38-302"></a></span>
<span id="cb38-303"><a href="#cb38-303"></a>  df_long <span class="ot">&lt;-</span> <span class="fu">pivot_longer</span>(df, <span class="at">cols =</span> <span class="fu">c</span>(aic, bic),</span>
<span id="cb38-304"><a href="#cb38-304"></a>                          <span class="at">names_to =</span> <span class="st">"criterion"</span>,</span>
<span id="cb38-305"><a href="#cb38-305"></a>                          <span class="at">values_to =</span> <span class="st">"value"</span>)</span>
<span id="cb38-306"><a href="#cb38-306"></a></span>
<span id="cb38-307"><a href="#cb38-307"></a>  <span class="fu">ggplot</span>(df_long, <span class="fu">aes</span>(<span class="at">x =</span> lambda, <span class="at">y =</span> value, <span class="at">color =</span> criterion)) <span class="sc">+</span></span>
<span id="cb38-308"><a href="#cb38-308"></a>    <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb38-309"><a href="#cb38-309"></a>    <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"aic"</span> <span class="ot">=</span> <span class="st">"orange"</span>, <span class="st">"bic"</span> <span class="ot">=</span> <span class="st">"skyblue3"</span>),</span>
<span id="cb38-310"><a href="#cb38-310"></a>                       <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"aic"</span> <span class="ot">=</span> <span class="st">"AIC"</span>, <span class="st">"bic"</span> <span class="ot">=</span> <span class="st">"BIC"</span>)) <span class="sc">+</span></span>
<span id="cb38-311"><a href="#cb38-311"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"log(lambda)"</span>,</span>
<span id="cb38-312"><a href="#cb38-312"></a>         <span class="at">y =</span> <span class="st">"Information Criterion"</span>, <span class="at">color =</span> <span class="st">"Criterion"</span>) <span class="sc">+</span></span>
<span id="cb38-313"><a href="#cb38-313"></a>    <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb38-314"><a href="#cb38-314"></a>    <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"top"</span>,</span>
<span id="cb38-315"><a href="#cb38-315"></a>          <span class="at">legend.direction =</span> <span class="st">"horizontal"</span>,</span>
<span id="cb38-316"><a href="#cb38-316"></a>          <span class="at">legend.box =</span> <span class="st">"horizontal"</span>)</span>
<span id="cb38-317"><a href="#cb38-317"></a>}</span>
<span id="cb38-318"><a href="#cb38-318"></a></span>
<span id="cb38-319"><a href="#cb38-319"></a><span class="fu">plot_ic</span>(lambdas_to_try, ic_results)</span>
<span id="cb38-320"><a href="#cb38-320"></a><span class="in">```</span></span>
<span id="cb38-321"><a href="#cb38-321"></a></span>
<span id="cb38-322"><a href="#cb38-322"></a>Now we find the $\lambda$ values that minimise the AIC and BIC, and refit the models using these values. It so happens that both AIC and BIC selects the same $\lambda$ values:</span>
<span id="cb38-323"><a href="#cb38-323"></a></span>
<span id="cb38-326"><a href="#cb38-326"></a><span class="in">```{r}</span></span>
<span id="cb38-327"><a href="#cb38-327"></a><span class="co"># Optimal lambdas according to both criteria</span></span>
<span id="cb38-328"><a href="#cb38-328"></a>lambda_aic <span class="ot">&lt;-</span> lambdas_to_try[<span class="fu">which.min</span>(ic_results<span class="sc">$</span>aic)]</span>
<span id="cb38-329"><a href="#cb38-329"></a>lambda_bic <span class="ot">&lt;-</span> lambdas_to_try[<span class="fu">which.min</span>(ic_results<span class="sc">$</span>bic)]</span>
<span id="cb38-330"><a href="#cb38-330"></a></span>
<span id="cb38-331"><a href="#cb38-331"></a><span class="co"># Fit final models using the optimal lambdas</span></span>
<span id="cb38-332"><a href="#cb38-332"></a>mod_aic <span class="ot">&lt;-</span> <span class="fu">fit_model_and_calculate_metrics</span>(X, y, lambda_aic)</span>
<span id="cb38-333"><a href="#cb38-333"></a>mod_bic <span class="ot">&lt;-</span> <span class="fu">fit_model_and_calculate_metrics</span>(X, y, lambda_bic)</span>
<span id="cb38-334"><a href="#cb38-334"></a><span class="in">```</span></span>
<span id="cb38-335"><a href="#cb38-335"></a></span>
<span id="cb38-336"><a href="#cb38-336"></a>For interest sake, we may also produce a plot that traces the coefficients of the model as $\lambda$ changes. This can help us understand how the coefficients shrink as $\lambda$ increases, and which variables are most important in the model. The plot below shows the Ridge Regression coefficients path for each variable in the model (@fig-coef-path1).</span>
<span id="cb38-337"><a href="#cb38-337"></a></span>
<span id="cb38-340"><a href="#cb38-340"></a><span class="in">```{r}</span></span>
<span id="cb38-341"><a href="#cb38-341"></a><span class="co">#| fig-width: 4.5</span></span>
<span id="cb38-342"><a href="#cb38-342"></a><span class="co">#| fig-height: 3.25</span></span>
<span id="cb38-343"><a href="#cb38-343"></a><span class="co">#| fig.cap: "Plot of the Ridge Regression coefficients paths."</span></span>
<span id="cb38-344"><a href="#cb38-344"></a><span class="co">#| label: fig-coef-path1</span></span>
<span id="cb38-345"><a href="#cb38-345"></a></span>
<span id="cb38-346"><a href="#cb38-346"></a><span class="co"># Plot the Ridge Regression coefficients path</span></span>
<span id="cb38-347"><a href="#cb38-347"></a>res <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambdas_to_try,</span>
<span id="cb38-348"><a href="#cb38-348"></a>              <span class="at">standardize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb38-349"><a href="#cb38-349"></a><span class="fu">plot</span>(res, <span class="at">xvar =</span> <span class="st">"lambda"</span>)</span>
<span id="cb38-350"><a href="#cb38-350"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb38-351"><a href="#cb38-351"></a>       <span class="at">legend =</span> <span class="fu">colnames</span>(X), <span class="at">cex =</span> <span class="fl">0.7</span>)</span>
<span id="cb38-352"><a href="#cb38-352"></a><span class="in">```</span></span>
<span id="cb38-353"><a href="#cb38-353"></a></span>
<span id="cb38-354"><a href="#cb38-354"></a>So, after having demonstrated the different methods for selecting the optimal $\lambda$ value, we can now summarise the results:</span>
<span id="cb38-355"><a href="#cb38-355"></a></span>
<span id="cb38-358"><a href="#cb38-358"></a><span class="in">```{r}</span></span>
<span id="cb38-359"><a href="#cb38-359"></a><span class="co">#| echo: false</span></span>
<span id="cb38-360"><a href="#cb38-360"></a></span>
<span id="cb38-361"><a href="#cb38-361"></a><span class="co"># Print results</span></span>
<span id="cb38-362"><a href="#cb38-362"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"CV Lambda:"</span>, lambda_cv))</span>
<span id="cb38-363"><a href="#cb38-363"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"AIC Lambda:"</span>, <span class="fu">round</span>(lambda_aic, <span class="dv">4</span>)))</span>
<span id="cb38-364"><a href="#cb38-364"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"BIC Lambda:"</span>, <span class="fu">round</span>(lambda_bic, <span class="dv">4</span>)))</span>
<span id="cb38-365"><a href="#cb38-365"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"CV R-squared:"</span>, <span class="fu">round</span>(mod_cv<span class="sc">$</span>rsq, <span class="dv">4</span>)))</span>
<span id="cb38-366"><a href="#cb38-366"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"AIC R-squared:"</span>, <span class="fu">round</span>(mod_aic<span class="sc">$</span>rsq, <span class="dv">4</span>)))</span>
<span id="cb38-367"><a href="#cb38-367"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"BIC R-squared:"</span>, <span class="fu">round</span>(mod_bic<span class="sc">$</span>rsq, <span class="dv">4</span>)))</span>
<span id="cb38-368"><a href="#cb38-368"></a><span class="in">```</span></span>
<span id="cb38-369"><a href="#cb38-369"></a></span>
<span id="cb38-370"><a href="#cb38-370"></a>Now we can extract the coefficient produced from models selected via the AIC and CV methods.</span>
<span id="cb38-371"><a href="#cb38-371"></a></span>
<span id="cb38-374"><a href="#cb38-374"></a><span class="in">```{r}</span></span>
<span id="cb38-375"><a href="#cb38-375"></a>res_aic <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda_aic,</span>
<span id="cb38-376"><a href="#cb38-376"></a>                  <span class="at">standardize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb38-377"><a href="#cb38-377"></a>res_aic</span>
<span id="cb38-378"><a href="#cb38-378"></a><span class="fu">coef</span>(res_aic)</span>
<span id="cb38-379"><a href="#cb38-379"></a><span class="in">```</span></span>
<span id="cb38-380"><a href="#cb38-380"></a></span>
<span id="cb38-383"><a href="#cb38-383"></a><span class="in">```{r}</span></span>
<span id="cb38-384"><a href="#cb38-384"></a>res_cv <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lambda_cv,</span>
<span id="cb38-385"><a href="#cb38-385"></a>                 <span class="at">standardize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb38-386"><a href="#cb38-386"></a>res_cv</span>
<span id="cb38-387"><a href="#cb38-387"></a><span class="fu">coef</span>(res_cv)</span>
<span id="cb38-388"><a href="#cb38-388"></a><span class="in">```</span></span>
<span id="cb38-389"><a href="#cb38-389"></a></span>
<span id="cb38-390"><a href="#cb38-390"></a>Ridge regression adds a penalty to the size of the coefficients, resulting in their shrinkage towards zero. This penalty affects all coefficients simultaneously. Notably, there is a difference in the model fit obtained using $\lambda_{AIC}$ (which is larger) and $\lambda_{min}$ (which is smaller). The former model explains 55.69% of the variance, compared to $\lambda_{min}$, which explains 63.37% of the variance.</span>
<span id="cb38-391"><a href="#cb38-391"></a></span>
<span id="cb38-392"><a href="#cb38-392"></a>Although shrinkage affects the absolute magnitude of the coefficients (they are biased estimates of the true relationships between the predictors and the response variable), the coefficients in Ridge Regression retain their general meaning---they still represent the change in the response variable associated with a one-unit change in the predictor variable, holding other predictors constant. While the absolute values of the coefficients may be biased due to regularisation, the relative importance of the predictors can still be interpreted. The magnitude of the coefficients can indicate the relative influence of each predictor on the response variable, even if their exact values are reduced.</span>
<span id="cb38-393"><a href="#cb38-393"></a></span>
<span id="cb38-394"><a href="#cb38-394"></a>Importantly, the predictive ability of the model can improve with shrunk coefficients because Ridge Regression reduces overfitting and enhances the model's generalisability to new, unseen data. By stabilising the coefficient estimates, the model often achieves better performance on validation and test datasets, which is important should robust predictive analytics be the goal.</span>
<span id="cb38-395"><a href="#cb38-395"></a></span>
<span id="cb38-396"><a href="#cb38-396"></a><span class="fu">## Example 2: Lasso Regression</span></span>
<span id="cb38-397"><a href="#cb38-397"></a></span>
<span id="cb38-398"><a href="#cb38-398"></a>Doing a Lasso Regression is easy. Simply change the <span class="in">`alpha`</span> parameter to 1 in the <span class="in">`glmnet`</span> function. The rest of the code remains the same. I'll show only the final output of this analysis to avoid repetition.</span>
<span id="cb38-399"><a href="#cb38-399"></a></span>
<span id="cb38-402"><a href="#cb38-402"></a><span class="in">```{r}</span></span>
<span id="cb38-403"><a href="#cb38-403"></a><span class="co">#| echo: false</span></span>
<span id="cb38-404"><a href="#cb38-404"></a></span>
<span id="cb38-405"><a href="#cb38-405"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb38-406"><a href="#cb38-406"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb38-407"><a href="#cb38-407"></a></span>
<span id="cb38-408"><a href="#cb38-408"></a><span class="co"># Perform 10-fold cross-validation</span></span>
<span id="cb38-409"><a href="#cb38-409"></a>lasso_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> lambdas_to_try,</span>
<span id="cb38-410"><a href="#cb38-410"></a>                      <span class="at">standardize =</span> <span class="cn">TRUE</span>, <span class="at">nfolds =</span> <span class="dv">10</span>)</span>
<span id="cb38-411"><a href="#cb38-411"></a></span>
<span id="cb38-412"><a href="#cb38-412"></a><span class="co"># Fit models and calculate performance metrics</span></span>
<span id="cb38-413"><a href="#cb38-413"></a>fit_model_and_calculate_metrics <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y, lambda) {</span>
<span id="cb38-414"><a href="#cb38-414"></a>  model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> lambda,</span>
<span id="cb38-415"><a href="#cb38-415"></a>                  <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb38-416"><a href="#cb38-416"></a>  y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, X)</span>
<span id="cb38-417"><a href="#cb38-417"></a>  ssr <span class="ot">&lt;-</span> <span class="fu">sum</span>((y <span class="sc">-</span> y_hat) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb38-418"><a href="#cb38-418"></a>  rsq <span class="ot">&lt;-</span> <span class="fu">cor</span>(y, y_hat) <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb38-419"><a href="#cb38-419"></a>  <span class="fu">list</span>(<span class="at">model =</span> model, <span class="at">ssr =</span> ssr, <span class="at">rsq =</span> rsq)</span>
<span id="cb38-420"><a href="#cb38-420"></a>}</span>
<span id="cb38-421"><a href="#cb38-421"></a></span>
<span id="cb38-422"><a href="#cb38-422"></a><span class="co"># Best cross-validated lambda</span></span>
<span id="cb38-423"><a href="#cb38-423"></a>lambda_cv <span class="ot">&lt;-</span> lasso_cv<span class="sc">$</span>lambda.min</span>
<span id="cb38-424"><a href="#cb38-424"></a>mod_cv <span class="ot">&lt;-</span> <span class="fu">fit_model_and_calculate_metrics</span>(X, y, lambda_cv)</span>
<span id="cb38-425"><a href="#cb38-425"></a><span class="in">```</span></span>
<span id="cb38-426"><a href="#cb38-426"></a></span>
<span id="cb38-429"><a href="#cb38-429"></a><span class="in">```{r}</span></span>
<span id="cb38-430"><a href="#cb38-430"></a><span class="co"># Print results</span></span>
<span id="cb38-431"><a href="#cb38-431"></a>mod_cv</span>
<span id="cb38-432"><a href="#cb38-432"></a><span class="fu">coef</span>(mod_cv<span class="sc">$</span>model)</span>
<span id="cb38-433"><a href="#cb38-433"></a></span>
<span id="cb38-434"><a href="#cb38-434"></a><span class="co"># Print results</span></span>
<span id="cb38-435"><a href="#cb38-435"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"CV Lambda:"</span>, lambda_cv))</span>
<span id="cb38-436"><a href="#cb38-436"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"CV R-squared:"</span>, <span class="fu">round</span>(mod_cv<span class="sc">$</span>rsq, <span class="dv">4</span>)))</span>
<span id="cb38-437"><a href="#cb38-437"></a><span class="in">```</span></span>
<span id="cb38-438"><a href="#cb38-438"></a></span>
<span id="cb38-441"><a href="#cb38-441"></a><span class="in">```{r}</span></span>
<span id="cb38-442"><a href="#cb38-442"></a><span class="co">#| eval: false</span></span>
<span id="cb38-443"><a href="#cb38-443"></a><span class="co">#| echo: false</span></span>
<span id="cb38-444"><a href="#cb38-444"></a><span class="co"># Plot cross-validation results (ggplot shown)</span></span>
<span id="cb38-445"><a href="#cb38-445"></a><span class="fu">plot</span>(lasso_cv)</span>
<span id="cb38-446"><a href="#cb38-446"></a><span class="in">```</span></span>
<span id="cb38-447"><a href="#cb38-447"></a></span>
<span id="cb38-450"><a href="#cb38-450"></a><span class="in">```{r}</span></span>
<span id="cb38-451"><a href="#cb38-451"></a><span class="co">#| echo: false</span></span>
<span id="cb38-452"><a href="#cb38-452"></a><span class="co">#| fig-width: 4</span></span>
<span id="cb38-453"><a href="#cb38-453"></a><span class="co">#| fig-height: 2.15</span></span>
<span id="cb38-454"><a href="#cb38-454"></a><span class="co">#| fig.cap: "Cross-validation statistics for Lasso Regression applied to the seaweed data."</span></span>
<span id="cb38-455"><a href="#cb38-455"></a><span class="co">#| label: fig-lasso-regression</span></span>
<span id="cb38-456"><a href="#cb38-456"></a></span>
<span id="cb38-457"><a href="#cb38-457"></a><span class="co"># Plot cross-validation results (default plot method)</span></span>
<span id="cb38-458"><a href="#cb38-458"></a><span class="co"># plot(lasso_cv)</span></span>
<span id="cb38-459"><a href="#cb38-459"></a>lasso_cv_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">lambda =</span> lasso_cv<span class="sc">$</span>lambda,</span>
<span id="cb38-460"><a href="#cb38-460"></a>                          <span class="at">cvm =</span> lasso_cv<span class="sc">$</span>cvm,</span>
<span id="cb38-461"><a href="#cb38-461"></a>                          <span class="at">cvmd =</span> lasso_cv<span class="sc">$</span>cvsd,</span>
<span id="cb38-462"><a href="#cb38-462"></a>                          <span class="at">nzero =</span> lasso_cv<span class="sc">$</span>nzero)</span>
<span id="cb38-463"><a href="#cb38-463"></a></span>
<span id="cb38-464"><a href="#cb38-464"></a><span class="fu">ggplot</span>(<span class="at">data =</span> lasso_cv_df, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(lambda), <span class="at">y =</span> cvm)) <span class="sc">+</span></span>
<span id="cb38-465"><a href="#cb38-465"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> cvm <span class="sc">-</span> cvmd, <span class="at">ymax =</span> cvm <span class="sc">+</span> cvmd),</span>
<span id="cb38-466"><a href="#cb38-466"></a>                <span class="at">width =</span> <span class="fl">0.0</span>, <span class="at">colour =</span> <span class="st">"dodgerblue4"</span>,</span>
<span id="cb38-467"><a href="#cb38-467"></a>                <span class="at">linewidth =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb38-468"><a href="#cb38-468"></a>  <span class="fu">geom_point</span>(<span class="at">colour =</span> <span class="st">"dodgerblue4"</span>, <span class="at">fill =</span> <span class="st">"white"</span>,</span>
<span id="cb38-469"><a href="#cb38-469"></a>             <span class="at">shape =</span> <span class="dv">21</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb38-470"><a href="#cb38-470"></a>  <span class="fu">geom_line</span>(<span class="at">colour =</span> <span class="st">"dodgerblue4"</span>, <span class="at">linewidth =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb38-471"><a href="#cb38-471"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> nzero <span class="sc">/</span> <span class="fu">max</span>(nzero) <span class="sc">*</span> <span class="fu">max</span>(cvm)),</span>
<span id="cb38-472"><a href="#cb38-472"></a>            <span class="at">colour =</span> <span class="st">"magenta"</span>) <span class="sc">+</span></span>
<span id="cb38-473"><a href="#cb38-473"></a>  <span class="fu">scale_y_continuous</span>(</span>
<span id="cb38-474"><a href="#cb38-474"></a>    <span class="at">name =</span> <span class="st">"Mean Squared Error"</span>,</span>
<span id="cb38-475"><a href="#cb38-475"></a>    <span class="at">sec.axis =</span> <span class="fu">sec_axis</span>(<span class="sc">~</span> . <span class="sc">/</span> <span class="fu">max</span>(lasso_cv_df<span class="sc">$</span>cvm) <span class="sc">*</span> <span class="fu">max</span>(lasso_cv_df<span class="sc">$</span>nzero),</span>
<span id="cb38-476"><a href="#cb38-476"></a>                        <span class="at">name =</span> <span class="st">"No. Non-zero Coef."</span>)</span>
<span id="cb38-477"><a href="#cb38-477"></a>  ) <span class="sc">+</span></span>
<span id="cb38-478"><a href="#cb38-478"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">log</span>(lambda))) <span class="sc">+</span></span>
<span id="cb38-479"><a href="#cb38-479"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb38-480"><a href="#cb38-480"></a>  <span class="fu">theme</span>(</span>
<span id="cb38-481"><a href="#cb38-481"></a>    <span class="at">axis.title.y.right =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">"magenta"</span>),</span>
<span id="cb38-482"><a href="#cb38-482"></a>    <span class="at">axis.title.y.left =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">"dodgerblue4"</span>),</span>
<span id="cb38-483"><a href="#cb38-483"></a>    <span class="at">axis.text.y.right =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">"magenta"</span>),</span>
<span id="cb38-484"><a href="#cb38-484"></a>    <span class="at">axis.text.y.left =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">"dodgerblue4"</span>)</span>
<span id="cb38-485"><a href="#cb38-485"></a>  ) <span class="sc">+</span></span>
<span id="cb38-486"><a href="#cb38-486"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">log</span>(lasso_cv<span class="sc">$</span>lambda.min), <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb38-487"><a href="#cb38-487"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">log</span>(lasso_cv<span class="sc">$</span>lambda<span class="fl">.1</span>se), <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb38-488"><a href="#cb38-488"></a>  <span class="fu">geom_label</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(lasso_cv<span class="sc">$</span>lambda.min), <span class="at">y =</span> <span class="fu">min</span>(cvm) <span class="sc">-</span> <span class="fl">0.005</span>),</span>
<span id="cb38-489"><a href="#cb38-489"></a>             <span class="at">label =</span> <span class="fu">expression</span>(lambda[min]),</span>
<span id="cb38-490"><a href="#cb38-490"></a>             <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb38-491"><a href="#cb38-491"></a>  <span class="fu">geom_label</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(lasso_cv<span class="sc">$</span>lambda<span class="fl">.1</span>se), <span class="at">y =</span> <span class="fu">min</span>(cvm) <span class="sc">-</span> <span class="fl">0.0025</span>),</span>
<span id="cb38-492"><a href="#cb38-492"></a>             <span class="at">label =</span> <span class="fu">expression</span>(lambda[<span class="st">`</span><span class="at">1se</span><span class="st">`</span>]),</span>
<span id="cb38-493"><a href="#cb38-493"></a>             <span class="at">size =</span> <span class="dv">3</span>)</span>
<span id="cb38-494"><a href="#cb38-494"></a><span class="in">```</span></span>
<span id="cb38-495"><a href="#cb38-495"></a></span>
<span id="cb38-498"><a href="#cb38-498"></a><span class="in">```{r}</span></span>
<span id="cb38-499"><a href="#cb38-499"></a><span class="co">#| echo: false</span></span>
<span id="cb38-500"><a href="#cb38-500"></a><span class="co">#| fig-width: 4.5</span></span>
<span id="cb38-501"><a href="#cb38-501"></a><span class="co">#| fig-height: 3.25</span></span>
<span id="cb38-502"><a href="#cb38-502"></a><span class="co">#| fig.cap: "Plot of the Lasso Regression coefficients paths."</span></span>
<span id="cb38-503"><a href="#cb38-503"></a><span class="co">#| label: fig-coef-path2</span></span>
<span id="cb38-504"><a href="#cb38-504"></a></span>
<span id="cb38-505"><a href="#cb38-505"></a><span class="co"># Plot the Lasso Regression coefficients path</span></span>
<span id="cb38-506"><a href="#cb38-506"></a>res <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> lambdas_to_try,</span>
<span id="cb38-507"><a href="#cb38-507"></a>              <span class="at">standardize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb38-508"><a href="#cb38-508"></a><span class="fu">plot</span>(res, <span class="at">xvar =</span> <span class="st">"lambda"</span>)</span>
<span id="cb38-509"><a href="#cb38-509"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb38-510"><a href="#cb38-510"></a>       <span class="at">legend =</span> <span class="fu">colnames</span>(X), <span class="at">cex =</span> <span class="fl">0.7</span>)</span>
<span id="cb38-511"><a href="#cb38-511"></a><span class="in">```</span></span>
<span id="cb38-512"><a href="#cb38-512"></a></span>
<span id="cb38-513"><a href="#cb38-513"></a>Lasso regression incorporates an L1 penalty term in its cost function, which shrinks some coefficient estimates to exactly zero. By reducing certain coefficients to zero, Lasso effectively eliminates those predictors from the model, which achieves automatic variable selection:</span>
<span id="cb38-514"><a href="#cb38-514"></a></span>
<span id="cb38-515"><a href="#cb38-515"></a><span class="ss">-   </span>When $\lambda$ is small, the penalty is minimal, and Lasso behaves similarly to ordinary least squares regression, retaining most coefficients.</span>
<span id="cb38-516"><a href="#cb38-516"></a><span class="ss">-   </span>When $\lambda$ is large, the penalty increases, causing more coefficients to shrink to zero. This results in a sparser model where only the most significant predictors have non-zero coefficients.</span>
<span id="cb38-517"><a href="#cb38-517"></a></span>
<span id="cb38-518"><a href="#cb38-518"></a>In our example (@fig-lasso-regression), we see at $\lambda_{min}$, the number of non-zero coefficients is minimised---all five coefficients remain. At $\lambda_{1se}$, the number of non-zero coefficients decreases to four. Consequently, for higher values of $\lambda$, more predictors will have coefficients exactly equal to zero. This is also seen in @fig-lasso-regression. In @fig-coef-path2 we can see that the first predictor to reach zero is <span class="in">`annMean`</span>, then <span class="in">`febSD`</span>, <span class="in">`febRange`</span>, and so forth. The implication is that they are excluded from the model and the model is simplified. This leads to several benefits: reduced multicollinearity, improved interpretability, and better generalisation to new data.</span>
<span id="cb38-519"><a href="#cb38-519"></a></span>
<span id="cb38-520"><a href="#cb38-520"></a>Coefficients that remain non-zero after Lasso regularisation are considered more important predictors. Those remaining coefficients can be interpreted similarly to standard linear regression: as the expected change in the response variable for a one-unit change in the predictor, holding other predictors constant.</span>
<span id="cb38-521"><a href="#cb38-521"></a></span>
<span id="cb38-522"><a href="#cb38-522"></a>The $\lambda$ parameter controls the amount of bias introduced. While Lasso can produce biased estimates, it reduces variance, often resulting in a model that performs better on new, unseen data. This trade-off enhances predictive accuracy but means that the exact coefficient values may not represent the true underlying relationships as closely as those in an unregularised model.</span>
<span id="cb38-523"><a href="#cb38-523"></a></span>
<span id="cb38-524"><a href="#cb38-524"></a>Despite regularisation, the relative magnitudes of the non-zero coefficients provide a glimpse into predictor importance. Larger absolute values of coefficients indicate stronger relationships with the response variable. The exact numerical values are biased, but ranking predictors by their coefficients still offers useful insight into their relative importance.</span>
<span id="cb38-525"><a href="#cb38-525"></a></span>
<span id="cb38-526"><a href="#cb38-526"></a><span class="fu">## Example 3: Elastic Net Regression</span></span>
<span id="cb38-527"><a href="#cb38-527"></a></span>
<span id="cb38-528"><a href="#cb38-528"></a>In this last example we'll look at Elastic Net Regression, which combines the L1 and L2 penalties of Lasso and Ridge Regression. There are now two parameters to optimise: $\alpha$ and $\lambda$. The $\alpha$ parameter controls the mix between the L1 and L2 penalties, with $\alpha = 0$ behaving like Ridge Regression and $\alpha = 1$ behaving like Lasso Regression. For $\alpha$ values between 0 and 1, Elastic Net combines the strengths of both Lasso and Ridge Regression. Optimisation of $\alpha$ and $\lambda$ is also done using cross-validation. In practise, the steps are:</span>
<span id="cb38-529"><a href="#cb38-529"></a></span>
<span id="cb38-530"><a href="#cb38-530"></a><span class="ss">1.  </span>Set up a grid of $\alpha$ values (from 0 to 1) and $\lambda$ values to try.</span>
<span id="cb38-531"><a href="#cb38-531"></a><span class="ss">2.  </span>Performs cross-validation for each combination of $\alpha$ and $\lambda$ using <span class="in">`cv.glmnet()`</span>.</span>
<span id="cb38-532"><a href="#cb38-532"></a><span class="ss">3.  </span>Select the best $\alpha$ and $\lambda$ combination based on the minimum mean cross-validated error.</span>
<span id="cb38-533"><a href="#cb38-533"></a><span class="ss">4.  </span>Fit the final model using the best $\alpha$ and $\lambda$.</span>
<span id="cb38-534"><a href="#cb38-534"></a><span class="ss">5.  </span>Calculate the performance metrics.</span>
<span id="cb38-535"><a href="#cb38-535"></a><span class="ss">6.  </span>For the Elastic Net model with the best alphaCreate plots similar to those in the Ridge and Lasso examples.</span>
<span id="cb38-536"><a href="#cb38-536"></a></span>
<span id="cb38-539"><a href="#cb38-539"></a><span class="in">```{r}</span></span>
<span id="cb38-540"><a href="#cb38-540"></a><span class="co"># Define the range of alpha values to try</span></span>
<span id="cb38-541"><a href="#cb38-541"></a>alphas_to_try <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb38-542"><a href="#cb38-542"></a></span>
<span id="cb38-543"><a href="#cb38-543"></a><span class="co"># Define the range of lambda values to try</span></span>
<span id="cb38-544"><a href="#cb38-544"></a>lambdas_to_try <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb38-545"><a href="#cb38-545"></a></span>
<span id="cb38-546"><a href="#cb38-546"></a><span class="co"># Perform grid search with cross-validation</span></span>
<span id="cb38-547"><a href="#cb38-547"></a>cv_results <span class="ot">&lt;-</span> <span class="fu">lapply</span>(alphas_to_try, <span class="cf">function</span>(a) {</span>
<span id="cb38-548"><a href="#cb38-548"></a>  <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> a, <span class="at">lambda =</span> lambdas_to_try,</span>
<span id="cb38-549"><a href="#cb38-549"></a>            <span class="at">standardize =</span> <span class="cn">TRUE</span>, <span class="at">nfolds =</span> <span class="dv">10</span>)</span>
<span id="cb38-550"><a href="#cb38-550"></a>})</span>
<span id="cb38-551"><a href="#cb38-551"></a></span>
<span id="cb38-552"><a href="#cb38-552"></a><span class="co"># Find the best alpha and lambda</span></span>
<span id="cb38-553"><a href="#cb38-553"></a>best_result <span class="ot">&lt;-</span> <span class="fu">which.min</span>(<span class="fu">sapply</span>(cv_results, <span class="cf">function</span>(x) <span class="fu">min</span>(x<span class="sc">$</span>cvm)))</span>
<span id="cb38-554"><a href="#cb38-554"></a>best_alpha <span class="ot">&lt;-</span> alphas_to_try[best_result]</span>
<span id="cb38-555"><a href="#cb38-555"></a>best_lambda <span class="ot">&lt;-</span> cv_results[[best_result]]<span class="sc">$</span>lambda.min</span>
<span id="cb38-556"><a href="#cb38-556"></a></span>
<span id="cb38-557"><a href="#cb38-557"></a><span class="co"># Fit the final model with the best parameters</span></span>
<span id="cb38-558"><a href="#cb38-558"></a>final_model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> best_alpha,</span>
<span id="cb38-559"><a href="#cb38-559"></a>                      <span class="at">lambda =</span> best_lambda,</span>
<span id="cb38-560"><a href="#cb38-560"></a>                      <span class="at">standardize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb38-561"><a href="#cb38-561"></a></span>
<span id="cb38-562"><a href="#cb38-562"></a><span class="co"># Calculate performance metrics</span></span>
<span id="cb38-563"><a href="#cb38-563"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(final_model, X)</span>
<span id="cb38-564"><a href="#cb38-564"></a>ssr <span class="ot">&lt;-</span> <span class="fu">sum</span>((y <span class="sc">-</span> y_hat) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb38-565"><a href="#cb38-565"></a>rsq <span class="ot">&lt;-</span> <span class="fu">cor</span>(y, y_hat) <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb38-566"><a href="#cb38-566"></a><span class="in">```</span></span>
<span id="cb38-567"><a href="#cb38-567"></a></span>
<span id="cb38-570"><a href="#cb38-570"></a><span class="in">```{r}</span></span>
<span id="cb38-571"><a href="#cb38-571"></a><span class="co">#| echo: false</span></span>
<span id="cb38-572"><a href="#cb38-572"></a></span>
<span id="cb38-573"><a href="#cb38-573"></a><span class="co"># Print results</span></span>
<span id="cb38-574"><a href="#cb38-574"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Best Alpha:"</span>, best_alpha))</span>
<span id="cb38-575"><a href="#cb38-575"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Best Lambda:"</span>, best_lambda))</span>
<span id="cb38-576"><a href="#cb38-576"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"R-squared:"</span>, <span class="fu">round</span>(rsq, <span class="dv">4</span>)))</span>
<span id="cb38-577"><a href="#cb38-577"></a><span class="in">```</span></span>
<span id="cb38-578"><a href="#cb38-578"></a></span>
<span id="cb38-581"><a href="#cb38-581"></a><span class="in">```{r}</span></span>
<span id="cb38-582"><a href="#cb38-582"></a><span class="co">#| eval: false</span></span>
<span id="cb38-583"><a href="#cb38-583"></a><span class="co">#| echo: false</span></span>
<span id="cb38-584"><a href="#cb38-584"></a></span>
<span id="cb38-585"><a href="#cb38-585"></a><span class="co"># Plot the cross-validation results</span></span>
<span id="cb38-586"><a href="#cb38-586"></a><span class="fu">plot</span>(cv_results[[best_result]])</span>
<span id="cb38-587"><a href="#cb38-587"></a><span class="in">```</span></span>
<span id="cb38-588"><a href="#cb38-588"></a></span>
<span id="cb38-591"><a href="#cb38-591"></a><span class="in">```{r}</span></span>
<span id="cb38-592"><a href="#cb38-592"></a><span class="co">#| echo: false</span></span>
<span id="cb38-593"><a href="#cb38-593"></a><span class="co">#| fig-width: 4</span></span>
<span id="cb38-594"><a href="#cb38-594"></a><span class="co">#| fig-height: 2.15</span></span>
<span id="cb38-595"><a href="#cb38-595"></a><span class="co">#| fig.cap: "Cross-validation statistics for Elastic Net Regression applied to the seaweed data."</span></span>
<span id="cb38-596"><a href="#cb38-596"></a><span class="co">#| label: fig-elastic-net-regression</span></span>
<span id="cb38-597"><a href="#cb38-597"></a></span>
<span id="cb38-598"><a href="#cb38-598"></a><span class="co"># Create a data frame for ggplot</span></span>
<span id="cb38-599"><a href="#cb38-599"></a>cv_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb38-600"><a href="#cb38-600"></a>  <span class="at">lambda =</span> cv_results[[best_result]]<span class="sc">$</span>lambda,</span>
<span id="cb38-601"><a href="#cb38-601"></a>  <span class="at">cvm =</span> cv_results[[best_result]]<span class="sc">$</span>cvm,</span>
<span id="cb38-602"><a href="#cb38-602"></a>  <span class="at">cvmd =</span> cv_results[[best_result]]<span class="sc">$</span>cvsd,</span>
<span id="cb38-603"><a href="#cb38-603"></a>  <span class="at">nzero =</span> cv_results[[best_result]]<span class="sc">$</span>nzero</span>
<span id="cb38-604"><a href="#cb38-604"></a>)</span>
<span id="cb38-605"><a href="#cb38-605"></a></span>
<span id="cb38-606"><a href="#cb38-606"></a><span class="co"># Plot using ggplot2</span></span>
<span id="cb38-607"><a href="#cb38-607"></a><span class="fu">ggplot</span>(<span class="at">data =</span> cv_df, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(lambda), <span class="at">y =</span> cvm)) <span class="sc">+</span></span>
<span id="cb38-608"><a href="#cb38-608"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> cvm <span class="sc">-</span> cvmd, <span class="at">ymax =</span> cvm <span class="sc">+</span> cvmd),</span>
<span id="cb38-609"><a href="#cb38-609"></a>                <span class="at">width =</span> <span class="fl">0.0</span>, <span class="at">colour =</span> <span class="st">"dodgerblue4"</span>,</span>
<span id="cb38-610"><a href="#cb38-610"></a>                <span class="at">linewidth =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb38-611"><a href="#cb38-611"></a>  <span class="fu">geom_point</span>(<span class="at">colour =</span> <span class="st">"dodgerblue4"</span>, <span class="at">fill =</span> <span class="st">"white"</span>,</span>
<span id="cb38-612"><a href="#cb38-612"></a>             <span class="at">shape =</span> <span class="dv">21</span>, <span class="at">size =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb38-613"><a href="#cb38-613"></a>  <span class="fu">geom_line</span>(<span class="at">colour =</span> <span class="st">"dodgerblue4"</span>, <span class="at">linewidth =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb38-614"><a href="#cb38-614"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> nzero <span class="sc">/</span> <span class="fu">max</span>(nzero) <span class="sc">*</span> <span class="fu">max</span>(cvm)),</span>
<span id="cb38-615"><a href="#cb38-615"></a>            <span class="at">colour =</span> <span class="st">"magenta"</span>) <span class="sc">+</span></span>
<span id="cb38-616"><a href="#cb38-616"></a>  <span class="fu">scale_y_continuous</span>(</span>
<span id="cb38-617"><a href="#cb38-617"></a>    <span class="at">name =</span> <span class="st">"Mean Squared Error"</span>,</span>
<span id="cb38-618"><a href="#cb38-618"></a>    <span class="at">sec.axis =</span> <span class="fu">sec_axis</span>(<span class="sc">~</span> . <span class="sc">/</span> <span class="fu">max</span>(cv_df<span class="sc">$</span>cvm) <span class="sc">*</span> <span class="fu">max</span>(cv_df<span class="sc">$</span>nzero),</span>
<span id="cb38-619"><a href="#cb38-619"></a>                        <span class="at">name =</span> <span class="st">"No. Non-zero Coef."</span>)</span>
<span id="cb38-620"><a href="#cb38-620"></a>  ) <span class="sc">+</span></span>
<span id="cb38-621"><a href="#cb38-621"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">log</span>(lambda)),</span>
<span id="cb38-622"><a href="#cb38-622"></a>       <span class="at">title =</span> <span class="fu">paste</span>(<span class="st">"Elastic Net (alpha ="</span>, best_alpha, <span class="st">")"</span>)) <span class="sc">+</span></span>
<span id="cb38-623"><a href="#cb38-623"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb38-624"><a href="#cb38-624"></a>  <span class="fu">theme</span>(</span>
<span id="cb38-625"><a href="#cb38-625"></a>    <span class="at">axis.title.y.right =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">"magenta"</span>),</span>
<span id="cb38-626"><a href="#cb38-626"></a>    <span class="at">axis.title.y.left =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">"dodgerblue4"</span>),</span>
<span id="cb38-627"><a href="#cb38-627"></a>    <span class="at">axis.text.y.right =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">"magenta"</span>),</span>
<span id="cb38-628"><a href="#cb38-628"></a>    <span class="at">axis.text.y.left =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">"dodgerblue4"</span>)</span>
<span id="cb38-629"><a href="#cb38-629"></a>  ) <span class="sc">+</span></span>
<span id="cb38-630"><a href="#cb38-630"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">log</span>(best_lambda), <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb38-631"><a href="#cb38-631"></a>  <span class="fu">geom_label</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(best_lambda), <span class="at">y =</span> <span class="fu">min</span>(cvm) <span class="sc">-</span> <span class="fl">0.005</span>),</span>
<span id="cb38-632"><a href="#cb38-632"></a>             <span class="at">label =</span> <span class="fu">expression</span>(lambda[min]),</span>
<span id="cb38-633"><a href="#cb38-633"></a>             <span class="at">size =</span> <span class="dv">3</span>)</span>
<span id="cb38-634"><a href="#cb38-634"></a><span class="in">```</span></span>
<span id="cb38-635"><a href="#cb38-635"></a></span>
<span id="cb38-638"><a href="#cb38-638"></a><span class="in">```{r}</span></span>
<span id="cb38-639"><a href="#cb38-639"></a><span class="co">#| echo: false</span></span>
<span id="cb38-640"><a href="#cb38-640"></a><span class="co">#| fig-width: 4.5</span></span>
<span id="cb38-641"><a href="#cb38-641"></a><span class="co">#| fig-height: 3.25</span></span>
<span id="cb38-642"><a href="#cb38-642"></a><span class="co">#| fig.cap: "Plot of the Elastic Net Regression coefficients paths."</span></span>
<span id="cb38-643"><a href="#cb38-643"></a><span class="co">#| label: fig-coef-path3</span></span>
<span id="cb38-644"><a href="#cb38-644"></a></span>
<span id="cb38-645"><a href="#cb38-645"></a><span class="co"># Plot the Elastic Net regression coefficients path</span></span>
<span id="cb38-646"><a href="#cb38-646"></a>res <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> best_alpha, <span class="at">lambda =</span> lambdas_to_try,</span>
<span id="cb38-647"><a href="#cb38-647"></a>              <span class="at">standardize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb38-648"><a href="#cb38-648"></a><span class="fu">plot</span>(res, <span class="at">xvar =</span> <span class="st">"lambda"</span>)</span>
<span id="cb38-649"><a href="#cb38-649"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,</span>
<span id="cb38-650"><a href="#cb38-650"></a>       <span class="at">legend =</span> <span class="fu">colnames</span>(X), <span class="at">cex =</span> <span class="fl">0.7</span>)</span>
<span id="cb38-651"><a href="#cb38-651"></a><span class="in">```</span></span>
<span id="cb38-652"><a href="#cb38-652"></a></span>
<span id="cb38-653"><a href="#cb38-653"></a>The model coefficients are:</span>
<span id="cb38-654"><a href="#cb38-654"></a></span>
<span id="cb38-657"><a href="#cb38-657"></a><span class="in">```{r}</span></span>
<span id="cb38-658"><a href="#cb38-658"></a><span class="fu">coef</span>(cv_results[[best_result]])</span>
<span id="cb38-659"><a href="#cb38-659"></a><span class="in">```</span></span>
<span id="cb38-660"><a href="#cb38-660"></a></span>
<span id="cb38-661"><a href="#cb38-661"></a>The interpretation of coefficients in Elastic Net is a blend of Ridge and Lasso. Some coefficients may be shrunk to zero (feature selection), while others are shrunk but remain non-zero (magnitude reduction). The non-zero coefficients retain their general meaning with an emphasis on their relative importance.</span>
<span id="cb38-662"><a href="#cb38-662"></a></span>
<span id="cb38-663"><a href="#cb38-663"></a><span class="fu">## Theory-Driven and Data-Driven Variable Selection</span></span>
<span id="cb38-664"><a href="#cb38-664"></a></span>
<span id="cb38-665"><a href="#cb38-665"></a>The choice between theory-driven and data- or statistics-driven variable selection represents an important consideration that can greatly influence model interpretation, its predictive power, and your value as an ecologist. This decision reflects a broader tension in scientific methodology between deductive and inductive reasoning. Each offers advantages and limitations that you should be aware of as an ecologist.</span>
<span id="cb38-666"><a href="#cb38-666"></a></span>
<span id="cb38-667"><a href="#cb38-667"></a>Theory-driven variable selection is core to the scientific method. It relies on *a priori* knowledge and established ecological theories (as far as they exist in ecology!) to guide your choice of predictors in a model. This aligns closely with the hypothetico-deductive method, where we formulate hypotheses based on existing knowledge and subsequently test these against the data we collect. The strength of this method lies in its interpretability. Models built on theoretical foundations often contribute directly to testing and refining ecological hypotheses. By focusing on variables with known or hypothesised relationships (with mechanisms often rooted in ecophysiological or ecological inquiries), the theory-driven hypothetico-deductive method should lead to more parsimonious models that are less prone to overfitting and more reflecting of reality.</span>
<span id="cb38-668"><a href="#cb38-668"></a></span>
<span id="cb38-669"><a href="#cb38-669"></a>Theory-driven selection is not without its drawbacks. It requires that we have a good grasp of the mechanism underlying our favourite ecological system. This is not always the case in complex systems where the underlying mechanisms are not well understood. Theory-driven selection can then lead to the exclusion of important variables that were not initially hypothesised and it can limit the scope of the analysis and potentially overlook significant relationships in the data.</span>
<span id="cb38-670"><a href="#cb38-670"></a></span>
<span id="cb38-671"><a href="#cb38-671"></a>A naive young ecologist might place undue value on the notion that their hard work collecting diverse data and developing hypotheses should all be reflected in their final model. This can lead to confirmation bias, where one is more likely to select variables that support our hypotheses and ignore those that do not. This bias can compromise the objectivity of the model and lead to skewed results that do not accurately represent the underlying ecological processes.</span>
<span id="cb38-672"><a href="#cb38-672"></a></span>
<span id="cb38-673"><a href="#cb38-673"></a>Moreover, the insistence on including all variables that were initially considered important can result in overly complex models. Such models can be difficult to interpret and may suffer from overfitting, where the model captures noise rather than the true signal in the data. Overfitted models perform well on the data we collected but poorly on new, unseen data. The consequence is a loss of predictive power and generalisability.</span>
<span id="cb38-674"><a href="#cb38-674"></a></span>
<span id="cb38-675"><a href="#cb38-675"></a>Another weakness of theory-driven variable selection is that the reliance on existing theories or the novel, promising hypothsis of the day may lead us to overlook important but unexpected relationships in the data. In complex ecological systems, where our theoretical understanding may be incomplete, some variables could be missed entirely---these might in fact hold the key to the real cause of the ecological patterns we observe. This limitation becomes concerning when studying ecosystems or phenomena that are not well understood or are undergoing rapid changes, such as those affected by climate change or novel anthropogenic pressures.</span>
<span id="cb38-676"><a href="#cb38-676"></a></span>
<span id="cb38-677"><a href="#cb38-677"></a>On the other hand, data-driven approaches, including regularisation techniques, VIF, and forward model variable selection (@sec-multiple-linear-regression), allow the data itself to guide variable selection. These methods are increasingly used in today's era of high-dimensional datasets common in modern ecological research. The primary advantage of data-driven selection lies in its potential for discovery---it can uncover unexpected relationships and generate new hypotheses, which is valuable in complex ecological systems where interactions may not be immediately apparent.</span>
<span id="cb38-678"><a href="#cb38-678"></a></span>
<span id="cb38-679"><a href="#cb38-679"></a>Data-driven methods are well-suited for handling the complexity often encountered in environmental and ecological datasets, where numerous potential predictors may co-occur and interact. They offer a degree of objectivity, reducing the potential for our personal biases in variable selection. But these approaches are not without risks. There's a danger of identifying relationships that are statistically significant but ecologically meaningless---we refer to this as spurious correlations (e.g. the belief that consuming carrots significantly improves our night vision). Moreover, models with many variables can present significant interpretability challenges, especially when complex interactions are present. This can make it difficult to extract meaningful (plausible) insights from the model and to communicate results to a broader audience.</span>
<span id="cb38-680"><a href="#cb38-680"></a></span>
<span id="cb38-681"><a href="#cb38-681"></a>In practice, the most robust approach to selecting which of the multitude of variables to include in our model often involves a thoughtful combination of theory-driven and data-driven methods. Well-trained ecologists should start with theory-driven variable selection to identify the core predictors based on established ecological principles. We could then employ regularisation techniques to explore additional variables and potential interactions, and use the results to refine our models and generate new hypotheses for future research.</span>
<span id="cb38-682"><a href="#cb38-682"></a></span>
<span id="cb38-683"><a href="#cb38-683"></a>This hybrid approach combines the strengths of both methods. It allows for rigorous hypothesis testing while remaining open to unanticipated and new insights from the data. In ecology, where systems are often characterised by complex, non-linear relationships and interactions that may vary across spatial and temporal scales, this two-pronged approach offers distinct benefits.</span>
<span id="cb38-684"><a href="#cb38-684"></a></span>
<span id="cb38-685"><a href="#cb38-685"></a>Consider how these methods complement theoretical knowledge. Use variable selection methods as tools for prediction, and to assit generating new insights and hypotheses about ecosystems. The choice between theory-driven and data-driven variable selection is not a binary one, but rather a spectrum of approaches.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ajsmit/BCB_Stats/edit/main/regularisation.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div></div></footer><script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>