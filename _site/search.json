[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Biostatistics Book",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\n1  Introduction\n",
    "section": "",
    "text": "1.1 The Scientific Method in Practice\nAnswering questions about the natural world using a scientific workflow requires that we draw on many years’ of accumulated knowledge and experience. The workflow unpacks into roughly the following sequence of steps:\nThis textbook deals with many of these steps (except for 1, 5, and 7). This knowledge is codified in the form of the statistical method, which provides a systematic framework for collecting,1 analysing, and interpreting data. In this chapter, I introduce the fundamental concepts of inferential statistics, which allow us to make inferences about populations based on sample data. I also provide an overview of the types of statistical methods used in inferential statistics, and discuss the importance of understanding the assumptions underlying these methods.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-scientific-method-in-practice",
    "href": "intro.html#the-scientific-method-in-practice",
    "title": "\n1  Introduction\n",
    "section": "",
    "text": "Look around you at the world, be curious about it, and ask questions to figure out an explanation for the pattern or phenomenon that tickled your interest.\nCreate an unambiguous statement of the question you want to answer, think about what is causing the pattern or phenomenon you observed, and how you might go about measuring the response (the thing you observed initially).\nTranslate this question into a testable hypothesis. This is the statement that you can test using the data you will collect.\nDesign an experiment or sampling campaign to collect data that will allow you to test this hypothesis. Clearly understand what the data you’ll collect will look like, both for the response and the explanatory variables. For example, do you have a categorical or continuous predictor, is the response continuous, binary, ordinal, etc.? For this, you should have a firm grasp of the various kinds of Data Classes and Structures in R.\nThink deeply about any confounding influences that might affect your data, and specify exactly what additional data you will have to collect to isolate the hypothesised influence in your analysis. You need to fully understand all the ways that factors not considered in your hypothesis might affect your study’s outcome. Omissions cannot be rectified after the fact without repeating the entire experiment or sampling work. It requires knowledge and experience to avoid confounding influences ruining your work.\nDepending on your experiment’s design (4) and the nature of the data you’ll obtain (4, 5), choose the appropriate statistical methods to analyse them. You should be able to develop a good idea of what statistical methods you’ll use—even before the experiment has been done! Decide on the parametric test, or, should the statistical god with the dice not provide an outcome that favours your expectations, you can also decide upfront on a non-parametric equivalent. It is important not to decide on the statistical method after you’ve collected the data. This is called p-hacking, and it is almost a cardinal sin in science.\nDo the experiment or go out into the world to sample, and collect the data. Have fun—this is why we do science, afterall!\nGo have a few drinks after a hard day’s work and celebrate your success.\nAnalyse your newly-collected data. This will include explaratory data analyses (see Exploring With Summaries and Descriptions and Exploring With Figures), and then the application of the statistical methods you chose in step 6.\nCommunicate your results in tables and figures.\n\n\n1 Yes, statistics also informs us about how to collect data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-statistical-toolbox",
    "href": "intro.html#the-statistical-toolbox",
    "title": "\n1  Introduction\n",
    "section": "\n1.2 The Statistical Toolbox",
    "text": "1.2 The Statistical Toolbox\nWith inferential statistics you can analyse data obtained from representative samples to draw conclusions or test hypotheses about populations or processes. I broadly categorise these methods into four main types, each serving different research applications2:\n2 This categorisation reflects my teaching approach, based on the order in which I think topics need to be covered, rather than a strict classification by statisticians. It is intended to provide a high-level overview of the types of statistical methods used in inferential statistics.\n\nHypothesis Tests: These parametric and non-parametric techniques assess whether sample data provide evidence for or against a specific claim (hypothesis) about population parameters such as their means, proportions, variances, or correlations between variables. Common hypothesis tests include:\n\nComparisons of group means or medians for a continuous variable (e.g., t-tests, ANOVA, Mann-Whitney U test)\nComparisons of group proportions for a categorical variable (e.g., \\chi-square test, Fisher’s exact test)\nAssessments of the relationship between two continuous or ordinal variables (e.g., Pearson’s correlation, Spearman’s rank correlation)\n\n\n\nRegression Analysis: Regression with its parametric and non-parametric offerings lets us analyse the relationship between a response variable and one or more predictor variables. Regression models estimate coefficients representing the predictor effects, allow for prediction of the response, and enable hypothesis tests on the predictors. Common regression models include:\n\nLinear regression for continuous response variables\nLogistic regression for binary response variables\nGeneralised linear models (GLMs) for non-normal response variables\nVarious non-linear regressions for complex relationships, such as generalised additive models (GAMs)\n\n\nSurvival Analysis: Methods like the Kaplan-Meier estimator and Cox proportional hazards model analyse time-to-event data, where the interest lies in modelling the waiting times until certain events occur. I do not cover survival analysis in this book or any of my modules.\nMultivariate Analysis: This includes an assortment of methods to analyse multiple response and predictor variables simultaneously. Dimension reduction methods, such as canonical correlation analysis (CCA) and non-metric multidimensional scaling (nMDS), help simplify complex datasets by identifying key patterns and relationships. Classification, including cluster analysis, is used to group similar observations together based on their characteristics. Multivariate approaches make fewer assumptions about the data’s distribution, and there are techniques to deal with parametric and non-parametric data types (often without discrimination). Although these methods are not covered in this textbook, they are taught in my Quantitative Ecology module, which will eventually be developed into its own textbook.\n\nThe above methods include parametric or non-parametric (sometimes called ‘robust’) methods. Parametric methods assume that the data follow a specific distribution (e.g., normal, Poisson), while non-parametric methods make fewer assumptions about the data distribution. I will cover the parametric methods first, in Part A, followed by non-parametric methods in Part B. Part C of the book will look at semi-parametric methods, which combine aspects of both parametric and non-parametric approaches.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#i.-parametric-methods-known-distribution",
    "href": "intro.html#i.-parametric-methods-known-distribution",
    "title": "\n1  Introduction\n",
    "section": "\n1.3 I. Parametric Methods (Known Distribution)",
    "text": "1.3 I. Parametric Methods (Known Distribution)\nParametric statistics rely on specific assumptions about the underlying probability distribution of the population from which the sample data were drawn. Biologists are taught that our data must be normally distributed, but this an unreasonable expectation considering the widely varying data sources we will encounter. Some biological processes simply do not generate normally distributed data!\nNevertheless, parametric statistics have through convention (rather than best practice) become the starting point for introductory forays into statistics. This is not terrible, because, should we be fortunate enough to have normally distributed data, parametric methods are more powerful than their non-parametric counterparts; however, they are also more sensitive to violations of some assumptions about our data.\nThe staple parametric statistics, such as the t-test, ANOVAs, Pearson correlation, and simple linear regression, require that two key assumptions are met: i) that our data (or sometimes the residuals) are normality distributed and ii) that the variances are homoscedastic. Section X is devoted to statistical tests that we may use to test the assumptions. However, because of the Central Limit Theorem, parametric methods can withstand moderate violations of the normality assumption when the sample size is large.3\n3 The CLT states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, even if the underlying population is not perfectly normal.4 In fact, many statistical tests that would ordinarily require the assumption of normality to be met have been extended to other probability distributions, as can be seen from the practice to append the word ‘Generalised’ to the name of the test. For example, the Generalised Additive Model (GAM) is a generalised semi-parametric method, Generalised Non-Linear Models (GNLMs) permit fitting non-linear models to non-parametric data, and Generalised Linear and Non-Linear Mixed-Effects Models (GLMMs and GNLMMs) do the same with hierarchical data.A common mistake biologists makes is to think that parametric tests only apply to normal data. This is not true. Generalised linear models (GLMs) extend the statistical framework to accommodate non-normal error distributions, such as Poisson for count data or binomial for binary outcomes.4 GLMs require that the distribution of the response variable belongs to the exponential family of distributions and that a suitable link function is chosen to connect the mean of the response to the linear predictor. Therefore, the defining characteristic of parametric methods is the assumption of a known distribution for the response variable, not necessarily that is is normal.\nWithin the parametric statistics framework, we can divide the methods into four groups depending on the type of question we are asking. We can ask questions about i) difference in means, ii) differences in proportions, iii) relationships between variables, or iv) the effect of one or more predictors on a response variable.\nA. Hypotheses About the Means of Groups\nThe simplest form of comparison is to test whether the sample means of two or more groups differ.5 Although this seems quite unimaginative, comparisons of the measures of central tendency are very common statistical tests in biology. Because this concept is so simple to understand, it serves as a good starting point for learning about hypothesis testing and the interpretation of the statistics which tell us about the strength of the evidence for or against our hypotheses.\n5 When it comes to central tendency, the mean is the parameter that is being compared by parametric statistics. Non-parametric statistics, on the other hand, consider the median as the statistic of central tendency.You might have hypotheses that require you to compare the means of the outcomes of different experimental treatments, differences in the number of sea urchins among populations of kelp, or the number of species within replicate samples taken from different vegetation types. Look at some of the following examples to see if any of them resonate with your own research question, and then use this as a guide to find the appropriate statistical test in this book.\nOne-Sample t-Test (Section X.X.X)\nExample: Is the mean height of a sample of Protea sp. grown in a specific experimental landscape (given below) different from the known (established a priori) average height of the same species (163.3 \\pm 15.5 cm) in the general population?\n\n\n   Height\n1     150\n2     152\n3     148\n8     150\n9     149\n10    148\n\n\nThe example requires that you have one normally-distributed continuous outcome variable with independent observations and that you want to compare its mean value against a known population mean established a priori.\nIn this case, you’ll want to use the R function t.test(). Since this function can accommodate data with equal or unequal variances6 via the var.equal argument, you only need to assure the data are normally distributed. The test can be one-sided or two-sided. Alternatively, consider non-parametric alternatives, such as the Wilcoxon signed-rank test.\n6 A t-test for equal variances is typically called the Student’s t-test, while a t-test for unequal variances is called Welch’s t-test. By default, the t.test() function in R performs Welch’s t-test, which is more robust to unequal variances.Two-Sample t-Test (Section X.X.X)\n\nExample: Is the average number of leopard cubs born per female leopard in the Overberg region different from that in the Cederberg region? The dataset is:\n\n\n      Region Cubs_Per_Female\n1   Overberg               2\n2   Overberg               3\n3   Overberg               2\n18 Cederberg               3\n19 Cederberg               2\n20 Cederberg               1\n\n\nThis requires that we obtain two samples of continuous, normally-distributed measurements. In other words, our experiment or sampling campaign will include two groups (sometimes two treatments, other times a treatment and a control) and we collect a sample of measurements of the response in both of them. This is again catered for by the t.test() function, and, as before, we don’t have to fuss too much about the variances as equal and unequal variances can be accommodated. If the normality assumption is not met, consider a non-parametric alternative such as the Mann-Whitney U test.\nA variant of the two-sample t-test is the paired t-test, which is used when the two samples are related (not independent); for example, the same individuals are measured before and after applying a treatment.\nAnalysis of Variance (ANOVA) for &gt;2 Samples (Section X.X.X)\n\nExample: Is the chirp rate of bladder grasshoppers different between the four seasons?\n\n\n\n\n\n\nSeason\nChirp Rate\n\n\n\n1\nSpring\n17.7\n\n\n2\nSpring\n13.9\n\n\n3\nSpring\n15.7\n\n\n58\nWinter\n10.2\n\n\n59\nWinter\n4.0\n\n\n60\nWinter\n10.6\n\n\n\nChirp Rate Data for Bladder Grasshoppers Across Four Seasons\n\nWe have three or more samples of continuous, normally-distributed observations. These data must also have more-or-less equal variances, so the homoscedasticity assumption is important. The aov() function in R is used to perform the ANOVA, which can be one-way, two-way, a repeated measures ANOVA, or an ANCOVA.7 If the normality or homoscedasticity assumptions are not met, consider non-parametric alternatives, such as the Kruskal-Wallis test, or try transforming the data.\n7 A repeated measures ANOVA is used when the same subjects are measured at different time points or under different conditions. A two-way ANOVA is used when there are two independent variables (there are also higher-order ANOVAs but they become more of a pain to interpret and require cumbersome experimental designs). An ANCOVA is used when you want to compare the means of groups while controlling for the effect of a continuous covariate. There are many kinds of ANOVA designs and each relates to specific experimental designs well beyond the scope of this book. Tony Underwood provides a pedantic overview of ANOVA designs in his book Experiments in Ecology (Underwood 1997) if you really want to go there.Analysis of Covariance (ANCOVA)* (Section X.X.X)\nExample: We have a set of data about African penguins and we want to determine if there are differences between male and female penguins in terms of their mean foraging time, and if that difference is influenced by their diving depth. The dataset is as follows:\n\n\n\n\n\nSex\nForaging time (hr)\nDiving depth (m)\n\n\n\nMale\n1.2\n10\n\n\nMale\n1.5\n15\n\n\nMale\n1.8\n20\n\n\nFemale\n2.0\n25\n\n\nMale\n2.3\n30\n\n\nFemale\n2.5\n35\n\n\nFemale\n2.8\n40\n\n\nFemale\n3.0\n45\n\n\nMale\n3.3\n50\n\n\nMale\n3.5\n55\n\n\n\nForaging time and diving depth of African penguin.\n\nIn this example, we are interested in the mean foraging time of male and female penguins, controlling for their diving depth. An ANCOVA focuses on the differences in means (the categorical variable), and the continuous covariates (diving depth) is specifically controlled for to remove its effect from the dependent variable. This reduces the error variance and so more accurately assesses the comparison of group means. The assumptions of normality and homoscedasticity apply. The functions aov() accommodates the categorical and continuous predictors.\nMultivariate Analysis of Variance (MANOVA)\nMANOVAs are similar to ANOVAs, except here you have multiple dependent variables, all independent, continuous, and normally-distributed. This is useful when you want to compare the means of multiple groups across multiple dependent variables. For example, you might want to compare the average foraging time together with diving depth of African penguins in three colonies (two in South Africa and one in Namibia) around the coast. The manova() function in R is used to perform a MANOVA and there are similar variants to what we have seen in ANOVA.\nB. Hypotheses About the Proportions of Groups\nYou can compare the proportions of groups using tests for proportions when the outcome variable is binary (e.g., success/failure, presence/absence, up/down, day/night). These tests are used to determine if the proportion of successes differs between groups. Use the following tests to compare group proportions:\nOne-Sample Test for Proportions\nExample: Is the proportion of African penguins foraging in a specific colony different from the known proportion of the same species in the general population? The data might look like this:\n\nSample data: 55 of the 100 penguins observed were foraging in a specific colony\nThe known proportion of penguins foraging in the general population is 60%\n\nIn this scenario, we are comparing the proportion of a single sample (the proportion of foraging African penguins in a specific colony) to a known population proportion. The data must consist of a binary outcome variable (e.g., foraging vs. not foraging) and the observations must be independent. The prop.test() function in R is used to perform this test, which can be either one-sided or two-sided. If the requirement of independent observations is not met, consider non-parametric alternatives, such as the sign test.\nTwo-Sample Test for Proportions\nExample: Is the proportion of endangered sea turtles successfully reaching the ocean different between two beaches? Here are data:\n\n\n\n\n\nBeach\nSuccesses\nObserved\n\n\n\nBeach A\n75\n100\n\n\nBeach B\n65\n120\n\n\n\nNumber of Sea Turtles Reaching the Ocean on Two Beaches\n\nHere we compare the proportions from two independent samples (e.g., the proportion of sea turtles successfully reaching the ocean on Beach A versus Beach B). As before, the data yield a binary outcome (e.g., reached the ocean vs. did not reach the ocean) for each group, and the observations within each group are independent. The prop.test() function is used it has one-sided or two-sided options. If the sample sizes are small or expected frequencies are low, consider using Fisher’s exact test instead of the proportion test. If the assumption of independent observations within groups is violated, you may need to consider methods that account for dependency in the data, such as Generalised Estimating Equations (GEE) or mixed-effects models.\nChi-square Test for Count Data\nExample: Is there an association between vegetation type and the presence of leopards in different areas of Kruger National Park? A hypothetical dataset:\n\n\n\n\n\n\nPresence\nAbsence\n\n\n\nGrassland\n20\n30\n\n\nWoodland\n25\n40\n\n\nShrubland\n35\n15\n\n\n\nContingency Table of Plant Species and Insect Occurrence\n\nHere we examine the relationship between two categorical variables (vegetation type and leopard presence) within Kruger National Park. The data are organised into a contingency table, where each cell represents the count or frequency of observations for a specific combination of categories. The chi-square test of independence is used to determine if there’s a significant association between the variables.\nAs with other categorical tests, the data yield discrete outcomes (e.g., savanna, woodland, or riverine for vegetation type; present or absent for leopard presence). The observations should be independent, meaning the presence of a leopard in one area should not influence its presence in another.\nThe chisq.test() function in R is commonly used for this analysis. This test compares the observed frequencies in each cell of the contingency table to the frequencies that would be expected if there were no association between vegetation type and leopard presence.\nIf the sample size is large and the expected frequencies in each cell are adequate (typically &gt; 5), the chi-square test is appropriate. However, if the sample size is small or if there are cells with low expected frequencies, consider using Fisher’s exact test instead.\nIf the assumption of independence is violated (e.g., if the data include multiple observations from the same leopard individuals or territories), you may need to consider more advanced methods that account for dependency in the data, such as log-linear models or Generalised Estimating Equations (GEE).\nFisher’s Exact Test\nExample: Is there a significant association between the presence of certain plant species and the occurrence of rare fynbos endemic insects in the Cape Floristic Region? Here are the data:\n\n\n\n\n\n\nPresent\nAbsent\n\n\n\nPlant A\n2\n8\n\n\nPlant B\n3\n7\n\n\n\nContingency Table of Plant Species and Insect Occurrence\n\nFisher’s Exact Test is used when we have two categorical variables and want to determine if there’s a significant association between them, particularly when sample sizes are small or when we have sparse data in some categories. This test is especially useful in ecological studies where rare species or events are being investigated.\nIn this example we examine the relationship between the presence of specific plant species and the occurrence of rare fynbos endemic insects. The data are organised into a 2x2 contingency table, where each cell represents the count of observations for a combination of presence/absence of the plant species and the insect species.\nThe test calculates the exact probability of observing the given set of cell frequencies under the null hypothesis of no association. It does not rely on approximations and it more accurate than the chi-square test for small samples. Use the fisher.test() function to perform this analysis. Like other categorical tests, the observations should be independent, meaning the presence of an insect in one area should not influence its presence in another.\nFisher’s Exact Test is particularly appropriate when:\n\nThe total sample size is less than 1000\nThe expected frequency in any cell of the contingency table is less than 5\nYou’re dealing with rare events or species\n\nIf the sample size becomes very large, Fisher’s Exact Test can become computationally intensive, and the chi-square test may be more practical.\nIf the assumption of independence is violated (e.g., if the data include multiple observations from the same locations over time), you may need to consider more advanced methods that account for dependency in the data, such as mixed-effects models or Generalised Estimating Equations (GEE).\nC. Hypotheses About the Strength of Association\nExample: Is there a relationship between the foraging time and diving depth of African penguins?\n\n\n\n\n\nForaging time (hr)\nDiving depth (m)\n\n\n\n1.2\n10\n\n\n1.5\n15\n\n\n1.8\n20\n\n\n2.0\n25\n\n\n2.3\n30\n\n\n2.5\n35\n\n\n2.8\n40\n\n\n3.0\n45\n\n\n3.3\n50\n\n\n3.5\n55\n\n\n\nForaging time and diving depth of African penguin.\n\nYou’ll want to use a Pearson’s correlation to determine if there is a linear relationship between two continuous variables, both of them normally distributed and homoscedastic. A correlation analysis does not presume causation and does not provide a predictive model, both of which are the domain of regression. The strength of the relationship is quantified by the correlation coefficient called Pearson’s rho, which ranges from -1 to 1. Use the cor.test(..., method = \"pearson\") function in R to perform this analysis.\nNon-parametric alternatives such as the Spearman’s rank correlation or Kendall’s tau correlation (see ‘II. Non-Parametric Methods’) are available and implemented with the same R function.\nD. Modelling and Predicting Causal Relationships\nThe relationship between one or a few predictors and an outcome can be represented by a function, which is a model that reconstructs part of the ‘reality’ of the observed phenomenon. Regression analysis helps you understand how changes in the continuous predictor variable(s) drive changes in a continuous outcome variable. The model quantifies the strength of the associations and makes predictions for new data points. You may use regression models for hypothesis testing and for identifying which predictor variables have the most substantial impact on the outcome.\nSimple Linear Regression\nExample: The same dataset of foraging time and diving depth of African penguins can be used to model the relationship between these two variables. Does diving depth depend on foraging time?\nWhat is different now is that we are interested in predicting the diving depth (response) of penguins based on their foraging time (predictor). Assuming there is a linear response, we can use a simple linear regression model to quantify the relationship between these two continuous variables. The model provides an equation that describes how the diving depth changes as the foraging time increases. The assumptions of normality and homoscedasticity apply to the residuals, and are accessed after having fit the model.\nThis calls for a simple linear regression model and you can fit it using the lm() function in R. The model can also be specified as a generalised linear model (GLM) with glm(..., family = gaussian).\nIf assumptions fail, apply data transformations (e.g., log, square root), robust regression (rlm() in MASS package), or consider non-linear models.\nPolynomial Regression\nI’ll not provide an example here. It suffices to say that a polynomial regression is effectively a simple linear regression that allows for a curvilinear relationship between the predictor and the outcome. To accomplish this, the model includes polynomial terms (e.g., quadratic, cubic, which are simply powers of the predictor) to capture the non-linear patterns in the data. The model can be fit using the lm() function in R.\nAssess the relationship between x vs. y by making a scatterplot of the data and eye balling a best fit curve through the scatter of points. Is the line curvy or bendy? Do you know in advance if a more complicated model describes the response? If the answer is ‘yes’ to the first and ‘no’ to the second question, then a polynomial regression might be just the thing for you.\nMultiple Linear Regression (MLR)\nExample: I’ve added a second predictor to the dataset of foraging time and diving depth of African penguins. Does diving depth depend on the penguins’ body mass index (BMI) and foraging time?\n\n\n\n\n\nBMI\nForaging time (hr)\nDiving depth (m)\n\n\n\n1.2\n1.2\n10\n\n\n1.5\n1.5\n15\n\n\n1.8\n1.8\n20\n\n\n2.0\n2.0\n25\n\n\n2.3\n2.3\n30\n\n\n2.5\n2.5\n35\n\n\n2.8\n2.8\n40\n\n\n3.0\n3.0\n45\n\n\n3.3\n3.3\n50\n\n\n3.5\n3.5\n55\n\n\n\nForaging time and diving depth of African penguin.\n\nThe only difference between this example and the simple linear regression is that we now have two predictors (foraging time and BMI) instead of one. The predictors can be continuous (as in the example) and/or categorical. If you are more concerned with the means of the categorical variables, consider an ANCOVA as an alternative option. The multiple linear regression model can be extended to include interaction terms between predictors. You can quantify the relationship between both predictors and the outcome simultaneously, and ask which of the two best predicts the response. The same assumptions apply as in the simple linear regression and we hope for a linear relationship between x_1 and x_2 vs. y. Other considerations are provided in the chapter on MLR.\nThe R functions lm() and glm(..., family = gaussian) accommodate situations such as these where we have multiple predictors.\nGeneralised Linear Models (GLM)\nGLMs are a class of regression models that extend the simple linear regression framework to accommodate various types of response distributions. As such, they can accommodate data that violate the assumptions of normality and homoscedasticity, as well as situations where the response variable is not continuous.\nUse GLMs to model count data (e.g., number of occurrences), binary outcomes (e.g., success/failure), and other non-continuous response variables that cannot be adequately represented by a normal distribution. Unlike linear models, which assume a normal error distribution, GLMs specify the distribution of the response variable using a probability distribution from the exponential family, such as the Gaussian (normal), binomial, Poisson, or negative binomial distributions.\nGLMs incorporate a link function that relates the linear predictor (a linear combination of the predictor variables) to the expected value of the response variable. This link function can take various forms, including the identity (linear), logit (for binary data), probit, or other transformations, depending on the nature of the response variable and the desired relationship between the predictors and the outcome.\nThe glm() function is a staple for fitting GLMs. It is designed to handle the exponential family distributions and will allow you to specify the appropriate distribution and link function for your data and research question. A few common types of GLMs are presented next.\nLogistic Regression (Chapter 6)\nYou’ll encounter binomial data in experiments or processes with binary outcomes, such as presence/absence, success/failure, or alive/dead. To model this type of data, you will want to use logistic regression. Logistic regression estimates the log-odds of the outcome as a linear combination of the predictor variables. The logistic function is then used to convert these log-odds into probabilities, which range from 0 to 1, so it is suitable for predicting the likelihood of the binary outcomes.\n\n\nUse When: You have a binary outcome variable and want to model the relationship between predictors and the probability of the outcome.\n\nData Requirements: Binary outcome, continuous or categorical predictors.\n\nAssumptions: Linear relationship between the log-odds of the outcome and predictors.\n\nDiagnostics: Check for influential observations, multicollinearity, and overall model fit.\n\nIf Assumptions Fail: Consider interactions, alternative link functions (probit, complementary log-log) in glm(), or non-linear logistic regression, zero-inflated models when excess zeroes.\n\nR Function: glm(..., family = binomial)\n\n\nModel Selection: Stepwise regression, regularisation techniques, information criteria (AIC, BIC).\n\nPoisson Regression (Chapter 6)\nTypical examples of count data include the number of offspring, parasites, or seeds. Poisson regression is used to model the relationship between predictors and the count outcome. The model assumes that the count data follow a Poisson distribution, where the mean and variance are equal. Poisson regression is suitable for data with a single count outcome.\n\n\nUse When: You have count data and want to model the relationship between predictors and the count outcome.\n\nData Requirements: Count outcome, continuous or categorical predictors.\n\nAssumptions: Equidispersion (variance equals the mean).\n\nDiagnostics: Check for overdispersion, excess zeros, and overall model fit.\n\nIf Assumptions Fail: Negative binomial regression (glm.nb() in the MASS package, overdispersion), zero-inflated models (zeroinfl() in the pscl package, excess zeros).\n\nR Function: glm(..., family = poisson)\n\n\nNegative Binomial Regression\nNegative binomial regression is an extension of Poisson regression that accommodates overdispersion, where the variance exceeds the mean. It is used when the count data exhibit more variability than expected under a Poisson distribution. The model assumes that the count data follow a negative binomial distribution, which has an additional parameter to account for overdispersion. Biological and ecological processes such as species abundance, parasite counts, and gene expression often exhibit overdispersion.\n\n\nUse When: You have count data with overdispersion and want to model the relationship between predictors and the count outcome.\n\nData Requirements: Count outcome, continuous or categorical\n\nAssumptions: Overdispersion (variance exceeds the mean).\n\nDiagnostics: Check for overdispersion, excess zeros, and overall model fit.\n\nR Function: glm.nb() in MASS package\n\nGamma Regression\nGamma regression is for modelling continuous, positive outcomes that exhibit a right-skewed distribution and possibly also a non-constant variance (heteroscedasticity). The gamma distribution is well suited for continuous measurements where the variability increases as the mean increases. You might encounter this kind of distribution in growth rates, enzyme activity levels, species abundance data, and other phenomena or processes characterised by positive, skewed data.\n\n\nUse When: You have a continuous, positive outcome and want to model the relationship between predictors and the outcome.\n\nData Requirements: Continuous, positive outcome, continuous or categorical predictors.\n\nAssumptions: Outcome values are positive, potentially non-constant variance.\n\nDiagnostics: Check for overall model fit, influential observations, and residual\n\nR Function: glm(..., family = Gamma)\n\n\nBeta Regression\nBeta regression is a statistical technique appropriate when the response variable is a continuous proportion or rate bounded between 0 and 1. These types of data might, for example, arise in ecology where one might study the proportions of time animals spend exhibiting different behaviours, the relative abundances of species in a community, or the proportions of habitat patches comprising a landscape. Proportional data inherently exhibit heteroscedasticity (non-constant variance).\n\n\nUse When: You have a proportional outcome (0 &lt; y &lt; 1) and want to model the relationship between predictors and the outcome.\n\nData Requirements: Proportional outcome (0 &lt; y &lt; 1), continuous or categorical predictors.\n\nAssumptions: Outcome values within (0,1), potentially non-constant variance.\n\nDiagnostics: Check for overall model fit, influential observations, and residual analysis.\n\nIf Assumptions Fail: Transformations, consider alternative link functions, or zero/one-inflated beta regression.\n\nR Function: betareg() in the betareg package\n\nModelling Non-Linear Relationships\nWe use non-linear models when the relationship between predictor variables and the outcome variable is not linear. This non-linearity arises from the predictor variables themselves being non-linearly related to the outcome or from the model’s parameters (coefficients) appearing non-linearly in the functional form. The visualised response curve is typically curved, rather than a straight line. These models are often derived from theoretical understanding or prior knowledge about the underlying mechanisms governing the relationship between the predictors and the outcome variables.\nNon-Linear Least Squares (NLS) Regression (Chapter 7)\n\n\nUse When: The relationship between the predictors and the outcome is non-linear.\n\nData Requirements: Continuous outcome, continuous predictors.\n\nAssumptions: Appropriate functional form, normality, and homoscedasticity of residuals.\n\nDiagnostics: Check residual plots, normality of residuals, and leverage/influence points.\n\nR Function: nls() (for non-linear regression models with user-specified functions)\n\nGeneralised Non-Linear Models (GNLMs)\nGNLMs are an extension of generalised linear models (GLMs) that allow for non-linear relationships between the predictors and the outcome variable. GNLMs are used when the relationship between the predictors and the outcome is non-linear, and the outcome variable follows a non-normal distribution. GNLMs are particularly useful for count data, binary outcomes, and other non-continuous response variables that exhibit non-linear relationships with the predictors.\nLinear and Non-Linear Hierarchical Models (Mixed-Effects Models)\nHierarchical models are used when data are structured hierarchically, such as when multiple observations are nested within higher-level units (e.g., plants within fields, sheep within rangelands). These models account for the correlation between observations within the same group and allow for the estimation of both fixed effects (population-level parameters) and random effects (group-level parameters). Hierarchical models are also known as multilevel models or mixed-effects models.\nLinear Mixed-Effects Models (LMMs) (Section X.X.X)\n\n\nUse When: You have nested or hierarchical data structures and the relationship between the predictors and the outcome is linear.\n\nData Requirements: Continuous outcome, continuous predictors, potentially with nested or hierarchical data structures.\n\nAssumptions: Normality, homoscedasticity of residuals, correct specification of random effects structure.\n\nIf Assumptions Fail: Consider transformations, robust regression, or non-linear mixed-effects models.\n\nDiagnostics: Check residual plots, normality of residuals, and leverage/influence points, assess random effects structure.\n\nR Function: lmer() in the lme4 package (for linear mixed-effects models with user-specified functions)\n\nNon-Linear Mixed-Effects Models (NLMMs) (Chapter 7)\n\n\nUse When: You have nested or hierarchical data structures and the relationship between the predictors and the outcome is non-linear.\n\nData Requirements: Continuous outcome, continuous predictors, potentially with nested or hierarchical data structures.\n\nAssumptions: Appropriate functional form, normality, and homoscedasticity of residuals, correct specification of random effects structure.\n\nIf Assumptions Fail: Generalised non-linear mixed models (GNLMMs) and generalised additive mixed models (GAMMs) can be used when the assumptions of non-linear mixed models (NLMMs) are violated. Else, consult a statistician.\n\nDiagnostics: Check residual plots, normality of residuals, and leverage/influence points, assess random effects structure.\n\nR Function: nlme() in the nlme package (for non-linear mixed-effects models with user-specified functions)\n\nGeneralised Linear and Non-Linear Mixed-Effects Models (GLMMs and GNLMMs)\nGLMMs and GNLMMs combine the flexibility of regression model generalisation (i.e. by accommodating non-Gaussian distribution families) with the ability to account for nested or hierarchical data structures. GLMMs are used when the outcome variable is not normally distributed (a different, known distribution) and the data are structured hierarchically. GLMMs include both fixed effects (population-level parameters) and random effects (group-level parameters) and can accommodate a wide range of outcome distributions, including binary, count, and continuous outcomes.\n\n\nUse When: You have non-normally distributed outcome data and nested or hierarchical data structures.\n\nData Requirements: Binary outcome, continuous or categorical predictors, potentially with nested or hierarchical data structures.\n\nAssumptions: Linear relationship between the log-odds of the outcome and predictors, correct specification of random effects structure.\n\nDiagnostics: Check residual plots, normality of residuals, and leverage/influence points, assess random effects structure.\n\nR Function: glmer() in the lme4 package\n\nOther Regression Models\nZero-Inflated Models\n\n\nUse When: You have count data with an excess of zeros and want to model the zero-inflation separately from the count process.\n\nData Requirements: Count outcome, continuous or categorical\n\nAssumptions: Correct specification of zero-inflation and count processes, no omitted variables.\n\nDiagnostics: Check zero-inflation and count process, overall model fit.\n\nR Function: zeroinfl() in the pscl package\n\nSurvival Analysis\n\n\nData Requirements: Time-to-event outcome, continuous or categorical predictors.\n\nAssumptions: Proportional hazards, non-informative censoring.\n\nDiagnostics: Check proportional hazards assumption, influential observations, and overall model fit.\n\nR Function: survival::coxph()\n\n\nTime Series Analysis\n\n\nData Requirements: Time-ordered data, potentially with autocorrelation.\n\nAssumptions: Stationarity, no autocorrelation in residuals.\n\nDiagnostics: Check autocorrelation, stationarity, and overall model fit.\n\nR Function: arima(), auto.arima() in the forecast package\n\nStructural Equation Modelling (SEM)\n\n\nData Requirements: Continuous outcome, continuous\n\nAssumptions: Correct specification of the structural model, no omitted variables, no measurement error.\n\nDiagnostics: Check model fit, parameter estimates, and overall model validity.\n\nR Function: sem() in the lavaan package\n\nBayesian Regression\n\n\nData Requirements: Continuous outcome, continuous or categorical predictors.\n\nAssumptions: Correct specification of priors, likelihood, and model structure.\n\nDiagnostics: Check for convergence, posterior predictive checks, and overall model fit.\n\nR Function: brms::brm()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#ii.-non-parametric-methods-distribution-free",
    "href": "intro.html#ii.-non-parametric-methods-distribution-free",
    "title": "\n1  Introduction\n",
    "section": "\n1.4 II. Non-Parametric Methods (Distribution-Free)",
    "text": "1.4 II. Non-Parametric Methods (Distribution-Free)\nNon-parametric statistics are statistical methods that do not rely on assumptions about the specific form or parameters of the population distribution. They are also referred to as distribution-free methods. These methods often use ranks or other order statistics of the data rather than the actual data values themselves.\nA. Hypotheses About Groups\nOne-Sample Tests for Medians\nUse a one-sample test to compare the median of a single sample to a known population median. It is as an alternative to one-sample t-tests when the data do not meet the assumptions of parametric tests.\n\nWilcoxon signed-rank test\nSign test\n\nTwo-Sample Tests for Medians (Section X.X.X)\nUse two-sample tests to compare the medians of two independent or related samples. Use it when the assumptions of parametric two-sample tests are violated.\n\nMann-Whitney U test (two independent groups)\nWilcoxon rank-sum test (two independent groups)\nKruskal-Wallis test (multiple groups)\nFriedman test (related samples)\nB. Hypotheses About Proportions\n\n\nChi-Square Test for Independence: Comparing proportions of two groups\nC. Correlation Analysis for Tests of Association\nUse non-parametric correlation to assess the strength and direction of a relationship between two continuous (or ordinal) variables when the assumptions of parametric correlation tests cannot be met.\nSpearman’s Rank Correlation (Chapter 2)\nA non-parametric measure of the strength and direction of association between two variables.\nKendall’s Tau Correlation (Chapter 2)\nA non-parametric measure of the strength and direction of association between two variables.\nD. Regression Analysis\nQuantile Regression (Section X.X.X)\nModels different quantiles of the response distribution.\nRobust Regression (Section X.X.X)\nLess sensitive to outliers than ordinary least squares regression.\nKernel Density Estimation\nKDE is a non-parametric method for visualising the distribution of a continuous variable. Unlike histograms, which bin data into discrete intervals, KDE creates a smooth curve that represents the estimated probability density function (PDF) of the underlying data. It does this by placing a kernel function (often a symmetric curve like a Gaussian or Epanechnikov) at each data point and summing up the contributions of these kernels across the entire range of the variable. The bandwidth of the kernel controls the smoothness of the resulting density estimate. Wider bandwidths lead to smoother curves but may obscure finer details, while narrower bandwidths reveal more local fluctuations but can be noisy. KDE is useful when the underlying distribution of the data is unknown or non-standard and it offers a convenient way to visualise and understand the shape and spread of the data without being constrained by parametric assumptions.\nLocal Regression (LOESS)\nLOESS (Locally Estimated Scatterplot Smoothing) is a non-parametric regression technique that produces a smooth curve through a set of data points by fitting simple models to localised subsets of the data. It achieves this by weighting the data points in each subset, with higher weights assigned to points closer to the point being estimated. The model used for local fitting is typically a low-degree polynomial, although other choices are possible.\nLOESS is primarily used for data exploration and visualisation. It is best known for smoothing scatterplots and revealing underlying trends or patterns in the data. It is advantageous because it doesn’t assume any particular functional form for the relationship between the predictors and the response variable, so it to adapts to various data shapes. But LOESS does not provide a single, easily interpretable equation for the entire dataset, making it less suitable for making predictions or drawing global inferences. It can also be computationally demanding with large datasets as it fits separate models in the vicinity of locally-selected points.\nPenalised Regression\nPenalised regression (also known as regularisation) is used to enhance the performance of regression models. This might be desirable when dealing with high-dimensional data or when the predictor variables are highly collinear. It introduces a penalty to the regression objective function which discourages the model from having overly complex or large coefficients. This effectively prevents overfitting. Common types of penalised regression include Ridge regression (L2 regularisation), which adds the sum of the squared coefficients as a penalty term, and Lasso regression (L1 regularisation), which adds the sum of the absolute values of the coefficients. The penalty terms encourage simpler models by shrinking some coefficients towards zero, with Lasso potentially setting some coefficients exactly to zero, thus performing variable selection. The balance between fitting the data well and maintaining model simplicity helps in improving the model’s generalisation to new data. Penalised regression methods can achieve a trade-off between bias and variance and result in more robust and interpretable models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#iii.-semi-parametric-methods",
    "href": "intro.html#iii.-semi-parametric-methods",
    "title": "\n1  Introduction\n",
    "section": "\n1.5 III. Semi-Parametric Methods",
    "text": "1.5 III. Semi-Parametric Methods\nSemi-parametric methods combine parametric and non-parametric techniques to provide a balance between flexibility and efficiency. These methods are useful when the assumptions of parametric tests are violated, but the data do not meet the requirements for non-parametric tests. Semi-parametric methods are often more powerful than non-parametric tests, as they make fewer assumptions about the data distribution. These methods are particularly useful when the sample size is small or when the data are skewed or have outliers.\nGeneralised Additive Models (GAMs) (Chapter 11)\n\n\nUse When: You have non-linear relationships between predictors and outcome.\n\nR Function: gam() in the mgcv package; also gamm4() in the gamm4 package\n\nData Requirements: Continuous, binary, or categorical outcome, continuous or categorical predictors, potentially with nested or hierarchical data structures.\n\nAdvantages: Flexible modelling of non-linear relationships using smoothing functions, can handle mixed-effects structures.\n\nLimitations: Interpretation can be challenging, potential overfitting.\n\nGeneralised Estimating Equations (GEEs)\n\n\nUse When: You have correlated data and non-normally distributed outcomes.\n\nR Function: geeglm() in the geepack package; also functions in the gee package\n\nData Requirements: Correlated data, non-normal outcomes, continuous or categorical predictors.\n\nAdvantages: Robust to misspecification of the correlation structure, can handle non-normal outcomes, flexible in handling missing data.\n\nLimitations: Assumes correct specification of the correlation structure, may be less efficient than mixed-effects models.\n\nSemi-Parametric Survival Models\n\n\nUse When: You have time-to-event data and want to model the hazard function.\n\nR Function: coxph() in the survival package\n\nData Requirements: Time-to-event data, censoring, continuous or categorical predictors.\n\nAssumptions: Proportional hazards assumption, independence of censoring.\n\nDiagnostics: Check proportional hazards assumption, influential observations, goodness\n\nSpline Regression\n\n\nUse When: You have non-linear relationships between predictors and outcome.\n\nR Function: lm() with splines, gam() in the mgcv package\n\nData Requirements: Continuous outcome, continuous predictors.\n\nAssumptions: Linearity within each spline, potentially non-constant variance.\n\nDiagnostics: Check for overall model fit, influential observations, and residual analysis.\n\nIf Assumptions Fail: Transformations, consider alternative link functions, or penalised regression.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#iv.-machine-learning-methods",
    "href": "intro.html#iv.-machine-learning-methods",
    "title": "\n1  Introduction\n",
    "section": "\n1.6 IV. Machine Learning Methods",
    "text": "1.6 IV. Machine Learning Methods\nMachine learning methods are a set of algorithms that can learn patterns from data without being explicitly programmed. These methods are particularly useful for prediction, classification, and clustering tasks. Machine learning models can handle complex relationships in the data and are often more flexible than traditional statistical models. However, they can be more computationally intensive and may require more data to train effectively.\nRandom Forests\nA machine learning method that uses an ensemble of decision trees to predict an outcome.\nSupport Vector Machines\nA machine learning method that finds the optimal hyperplane to separate two classes of data.\nEnsemble Methods\nA machine learning technique that combines the predictions of multiple models to improve accuracy.\nNeural Networks\nA machine learning method that uses interconnected nodes to model complex relationships in data.\nDeep Learning\nA subset of machine learning that uses neural networks with multiple layers to model complex relationships in data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#v.-miscellaneous-methods",
    "href": "intro.html#v.-miscellaneous-methods",
    "title": "\n1  Introduction\n",
    "section": "\n1.7 V. Miscellaneous Methods",
    "text": "1.7 V. Miscellaneous Methods\nBootstrapping\nA resampling method for estimating the sampling distribution of a statistic.\nPermutation Tests\nA non-parametric method for testing hypotheses by randomly permuting the data.\nMonte Carlo Simulation\nA method for estimating the distribution of a statistic by generating random samples from a known distribution.\nBayesian Methods\nA statistical approach that uses Bayes’ theorem to update prior beliefs based on observed data.\nDimensionality Reduction\nAlso called muitvariate analyses. A set of techniques for reducing the number of variables in a dataset while preserving important information.\nClustering\nA set of unsupervised learning techniques for grouping similar data points together.\nFeature Selection\nA process for identifying the most important variables in a dataset for predicting an outcome.\nRegularisation\nSee penalised regression. A technique for preventing overfitting by adding a penalty term to the model coefficients.\nCross-Validation\nA method for estimating the performance of a model by splitting the data into training and test sets.\nHyperparameter Tuning\nThe process of selecting the optimal values for the parameters of a machine learning model.\nModel Evaluation\nThe process of assessing the performance of a model using metrics such as accuracy, precision, recall, and F1 score.\nModel Interpretation\nThe process of understanding how a model makes predictions by examining the relationship between the input variables and the output.\nModel Deployment\nThe process of putting a trained model into production so that it can be used to make predictions on new data.\nModel Monitoring\nThe process of tracking the performance of a deployed model over time to ensure that it continues to make accurate predictions.\nModel Explainability\nThe process of explaining how a model makes predictions in a way that is understandable to humans.\nModel Fairness\nThe process of ensuring that a model does not discriminate against certain groups of people based on sensitive attributes.\nModel Robustness\nThe process of ensuring that a model performs well on new data that is different from the training data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderwood AJ (1997) Experiments in ecology: Their logical design and interpretation using analysis of variance. Cambridge university press",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "simple_linear_regression.html",
    "href": "simple_linear_regression.html",
    "title": "3  Linear Regression",
    "section": "",
    "text": "3.1 Simple Linear Regression\nLinear models help us answer questions like:\nBy assuming a linear relationship between variables, these models provide a clear and interpretable way to quantify and predict biological outcomes. For example, should a linear model describe the relationship between body mass (g) and age (years), we can predict the body mass of a particular species of fish would increase by 230 g for every additional year of age up to the age of five years (however, please see the von Bertalanffy model in Chapter 7.6).\nThe simple linear model is given by: Y_{i}=\\beta \\cdot X_{i}+\\alpha+\\epsilon \\tag{3.1} Where:",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "simple_linear_regression.html#sec-simple-linear-regression",
    "href": "simple_linear_regression.html#sec-simple-linear-regression",
    "title": "3  Linear Regression",
    "section": "",
    "text": "How does body mass change with age in a particular species?\nDoes the number of offspring depend on the amount of food available?\nHow does a species’ geographic distribution change with temperature?\n\n\n\n\nY_{i} is the i-th measurement of the dependent variable,\nX_{i} is the i-th measurement of the independent variable,\n\\alpha is the intercept (the value of Y when X=0),\n\\beta is the slope (the change in Y for a one-unit change in X), and\n\\epsilon is the error term (residual; see box ‘The residuals, \\epsilon_i’).\n\n\n\n\n\n\n\nThe residuals, \\epsilon_i\n\n\n\nIn most regression models, such as linear regressions and those discussed in Chapter 7, we assume that the residuals are independent and identically distributed (i.i.d.). This implies that each residual \\epsilon_i is drawn from the same probability distribution and that they are mutually independent. When the residuals follow a normal distribution, this can be expressed as \\epsilon_i \\sim N(0, \\sigma^2), where:\n\n\\epsilon_i represents the residual for the i-th observation,\nN(0, \\sigma^2) denotes a normal distribution with a mean of 0 and a variance of \\sigma^2.\n\nThe requirement of a zero mean for residuals implies that, on average, the model’s predictions neither systematically overestimate nor underestimate the true values. The constant variance assumption ensures that the spread or dispersion of residuals around the mean remains consistent across all levels of the predictor variables. This ensures that the model’s accuracy is uniform across the range of data.\nThe requirement for independence indicates that the residual for any given observation is not influenced by or correlated with the residuals of other observations. It also means that the residual for an observation does not depend on the order in which the observations were collected (i.e. no serial correlation or auto-correlation). Independence ensures that each data point contributes unique information to the model and prevents any systematic patterns from influencing the estimates of the model’s parameters.\nViolation of any of these assumptions could lead to biased or inefficient parameter estimates.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "simple_linear_regression.html#nature-of-the-data",
    "href": "simple_linear_regression.html#nature-of-the-data",
    "title": "3  Linear Regression",
    "section": "3.2 Nature of the Data",
    "text": "3.2 Nature of the Data\nThe experimenter must ensure the following key requirements for a simple linear regression:\n\nCausality: There should be a theoretical or philosophical basis for expecting a causal relationship, where the independent variable (X) influences or determines the dependent variable (Y).1 It is assumed that changes in X cause changes in Y.\nIndependence of Observations:\n\nThe observations or measured values of Y must be independent of each other. For each value of X, there should be only one corresponding value of Y, or if there are replicate Y values, they must be statistically independent and not influence each other.\nThe observations of Y must also be independently across the range of X values. This means that the value of Y at one point should not influence the value of Y at another point.2\n\nIndependent Variable Scale: The independent variable (X) should be measured on a continuous scale, such as integers, real numbers, intervals, or ratios.\nDependent Variable Scale: Similarly, the dependent variable (Y) should also be measured on a continuous scale, such as integers, real numbers, intervals, or ratios.3\n\n1 The independent and dependent variables are also called the predictor and response variables, respectively. The predictor is often under the experimenter’s control (in which case it is a fixed effects model), while the response is the variable predicted to respond in the manner hypothesised.2 If Y not independent across the range of X, use a different type of regression model, such as a linear mixed-effects model.3 The dependent variable can also be ordinal, but this is less common. If this is the case, use *ordinal (logistic) regression instead.What if my data are not continuous?\n\nIf the independent variable is ordinal, use ordinal regression.\nIf the dependent variable is ordinal, use ordinal (logistic) regression.\n\nWhat if I have more than one independent variable?\n\nUse multiple linear regression.\n\nAdditional assumptions and requirements are discussed next in Section 3.3.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "simple_linear_regression.html#sec-linear-model-assumptions",
    "href": "simple_linear_regression.html#sec-linear-model-assumptions",
    "title": "3  Linear Regression",
    "section": "3.3 Assumptions",
    "text": "3.3 Assumptions\nThe following assumptions are made when performing a simple linear regression; 1-3 must be tested after fitting the linear model:\n\nNormality: For each value of X, there is a corresponding normal distribution of Y values. Each value of Y is randomly sampled from this normal distribution.\nHomoscedasticity: The variances of the Y distributions corresponding to each X value should be approximately equal.\nLinearity: There exists a linear relationship between the variables Y and X.\nMeasurement Error: It is assumed that the measurements of X are obtained without error. However, in practical scenarios, this is rarely the case. Therefore, we assume any measurement error in X to be negligible.\n\nSee Section 3.8 for more information about how to proceed when assumptions 1-3 are violated.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "simple_linear_regression.html#outliers-and-their-impact-on-simple-linear-regression",
    "href": "simple_linear_regression.html#outliers-and-their-impact-on-simple-linear-regression",
    "title": "3  Linear Regression",
    "section": "3.4 Outliers and Their Impact on Simple Linear Regression",
    "text": "3.4 Outliers and Their Impact on Simple Linear Regression\nIn simple linear regression, outliers can have significant detrimental effects on the analysis and the reliability of the results. Outliers are data points that deviate substantially from the overall pattern or trend observed in the data, and their presence can lead to biased parameter estimates, inflated standard errors, distorted confidence and prediction intervals, violation of assumptions, and masking of underlying patterns.\nSpecifically, they can greatly impact the estimation of the slope and intercept due to their influence on the process of minimising the sum of squared residuals. Their presence can increase the standard errors of the regression coefficients, making it harder to detect significant relationships between the independent and dependent variables. Furthermore, the inclusion of outliers in the dataset can distort the calculation of confidence and prediction intervals for individual observations, preventing accurate inference and prediction. Their presence may also lead to violations of the assumptions of linear regression, such as the normality of residuals and the constant variance of errors (homoscedasticity). Lastly, extreme outliers can mask underlying patterns or relationships in the data and hinder our ability to discern the true nature of the associations between variables.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "simple_linear_regression.html#r-function",
    "href": "simple_linear_regression.html#r-function",
    "title": "3  Linear Regression",
    "section": "3.5 R Function",
    "text": "3.5 R Function\nThe lm() function in R is used to fit linear models. It can be used to carry out simple linear regression, multiple linear regression, and more.\nThe general form of the function written in R is:\n\nlm(formula, data, ...)\n\nwhere formula is a symbolic description of the model to be fitted, and data is the data frame containing the variables. The ... argument is used to pass additional arguments to the function (consult ?lm). For example:\n\n1lm(y ~ x, data = df)\n\n\n1\n\nYou can read the statement y ~ x as “y is modelled as a function of x.”\n\n\n\n\nThe above statement fits a simple linear regression model with y as the dependent variable and x as the independent variable. The data frame df contains the variables named x and y.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "simple_linear_regression.html#example-the-penguin-dataset",
    "href": "simple_linear_regression.html#example-the-penguin-dataset",
    "title": "3  Linear Regression",
    "section": "3.6 Example: The Penguin Dataset",
    "text": "3.6 Example: The Penguin Dataset\nThe following example workflow uses the penguin dataset from the palmerpenguins package to demonstrate how to perform a simple linear regression in R. The data are in Table 3.1.\nAlthough we can also do a correlation here, we will use a simple linear regression because we want to develop a predictive model that can be used to estimate the bill length of Adelie penguins based on their body mass—this is a permissible application of a simple linear regression even though the two variables are not assumed to be causally related.\n\n\n\n\n\n\n\n\n\n\n\nBill length (mm)\nBody mass (g)\n\n\n\n\n39.1\n3750\n\n\n39.5\n3800\n\n\n40.3\n3250\n\n\n36.7\n3450\n\n\n39.3\n3650\n\n\n38.9\n3625\n\n\n\n\n\n\n\n\nTable 3.1: Size measurements for adult foraging Adelie penguins near Palmer Station, Antarctica.\n\n\n\n\n\nDo an Exploratory Data Analysis (EDA)\n\ndim(Adelie)\n\n[1] 151   8\n\nsummary(Adelie)\n\n      species          island   bill_length_mm  bill_depth_mm  \n Adelie   :151   Biscoe   :44   Min.   :32.10   Min.   :15.50  \n Chinstrap:  0   Dream    :56   1st Qu.:36.75   1st Qu.:17.50  \n Gentoo   :  0   Torgersen:51   Median :38.80   Median :18.40  \n                                Mean   :38.79   Mean   :18.35  \n                                3rd Qu.:40.75   3rd Qu.:19.00  \n                                Max.   :46.00   Max.   :21.50  \n flipper_length_mm  body_mass_g       sex          year     \n Min.   :172       Min.   :2850   female:73   Min.   :2007  \n 1st Qu.:186       1st Qu.:3350   male  :73   1st Qu.:2007  \n Median :190       Median :3700   NA's  : 5   Median :2008  \n Mean   :190       Mean   :3701               Mean   :2008  \n 3rd Qu.:195       3rd Qu.:4000               3rd Qu.:2009  \n Max.   :210       Max.   :4775               Max.   :2009  \n\n\nWe see that the dataset contains 344 observations of 8 variables. We shall focus on the body_mass_g and bill_length_mm variables for this example. Importantly, the two variables are continuous, which seems to satisfy the requirements for a simple linear regression. We will also restrict this analysis to the Adelie penguins (n = 152). Is the relationship between the body mass and bill length of the penguins linear? Let’s find out.\n\n\nCreate a Plot\nConstruct a scatter plot of the data and include a best fit straight line:\n\nggplot(Adelie,\n       aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Body mass (g)\", y = \"Bill length (mm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 3.1: Scatter plot of the Palmer Station Adelie penguin data with a best fit line.\n\n\n\n\n\nAlthough there is some scatter in the data (Figure 3.1), there appears to be a positive relationship between the body mass and bill length of the penguins. This relationship might be amenable for modelling with a linear relationship and we shall continue to explore this.\n\n\nState the Hypothesis\n\nNull Hypothesis (H_0): there is no relationship between the body mass of the penguins and their bill length.\nAlternative Hypothesis (H_A): there is a relationship between the two variables.\n\nThis can be written as:\nH_{0}:\\beta = 0 \\tag{3.2}\nAs seen above, this hypothesis concerns the slope of the regression line, \\beta. If the slope is zero, then there is no relationship between the two variables. Regression models also tests an hypothesis about the intercept, \\alpha, but this is less commonly reported.\n\n\nFit the Model\nSince the assumptions of a linear regression can only be tested after fitting the model, we first fit the model and then test the assumptions.\n\nmod1 &lt;- lm(bill_length_mm ~ body_mass_g,\n           data = Adelie)\nsummary(mod1)\n\n\nCall:\nlm(formula = bill_length_mm ~ body_mass_g, data = Adelie)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4208 -1.3690  0.1874  1.4825  5.6168 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.699e+01  1.483e+00  18.201  &lt; 2e-16 ***\nbody_mass_g 3.188e-03  3.977e-04   8.015 2.95e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.234 on 149 degrees of freedom\nMultiple R-squared:  0.3013,    Adjusted R-squared:  0.2966 \nF-statistic: 64.24 on 1 and 149 DF,  p-value: 2.955e-13\n\n\n\n\nTest the Assumptions\nAssumptions of normality, homoscedasticity, and linearity must be tested (Section 7.3).\nWe already noted that a linear model will probably be appropriate for the data (see Figure 3.1), so we proceed with the other assumptions.\nTo facilitate the production of the diagnostic plots, we will use the broom package’s augment() function to add the residuals to the data within the original dataset (now appearing as the tidied dataset, mod1_data). This will allow us to create the diagnostic plots more easily, and later we can also use it to look for the presence of outliers (Section 3.6.6).\n\nlibrary(broom)\n\nmod1_data &lt;- augment(mod1)\n\nNormality\nI first check the normality assumption using one of several options (Options 1-3). Here I use the Shapiro-Wilk test, a Residual Q-Q plot, and a histogram of the residuals.\nOption 1: Perform the Shapiro-Wilk test on the residuals. The Shapiro-Wilk test is useful for detecting departures from normality in small sample sizes. The hypothesis is:\n\nH0: the residuals are normally distributed.\nHA: the residuals are not normally distributed.\n\n\nshapiro.test(residuals(mod1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(mod1)\nW = 0.99613, p-value = 0.9637\n\n\nThe p-value is greater than 0.05, so I reject the alternative hypothesis. I conclude that the residuals are normally distributed.\nOption 2: Create a Residual Q-Q plot to visually assess the normality of the residuals:\n\n\n\n\n\n\n\n\nFigure 3.2: Diagnostics plots the linear regression, mod1, for assumption testing.\n\n\n\n\n\nThe residuals are plotted against a theoretical normal distribution. The residuals fall along the line without major deviations, therefore the residuals are normally distributed (Figure 3.2 A).\nOption 3: Create a histogram of the residuals to visually assess the normality of the residuals:\nThe histogram of the residuals appears to be normally distributed (Figure 3.2 B).\nHomoscedasticity\nI now examine the homoscedasticity assumption. The residuals should be approximately equal across all values of the independent variable. There are several options.\nOption 1: I will use the Breusch-Pagan test to test for homoscedasticity.\nThe Breusch-Pagan test is used to assess the presence of heteroscedasticity (non-constant variance) in the residuals of a regression model.\nThe hypothesis is:\n\nH0: the residuals are homoscedastic.\nHA: the residuals are heteroscedastic.\n\n\nlibrary(lmtest)\nbptest(mod1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  mod1\nBP = 1.6677, df = 1, p-value = 0.1966\n\n\nThe p-value is greater than 0.05, so I reject the alternative hypothesis. I conclude that the residuals are homoscedastic.\nOption 2: Create a plot of the residuals against the fitted values to visually assess homoscedasticity:\nThe residuals are scattered evenly around zero from short through to long bill lengths, indicating that the residuals have constant variance (Figure 3.2 C).\nOption 3: Create a plot of the standardised residuals against the independent variable to visually assess homoscedasticity:\nThe residuals are scattered evenly around zero from low through to high bill lenghts, indicating that the residuals have constant variance (Figure 3.2 D).\nOther tests for homoscedasticity include the Goldfeld-Quandt (lmtest::gqtest) test, Levene’s test (car::leveneTest), and others.\n\n\nCheck for outliers\nHow do we identify outliers in linear regression analysis? There are several approaches (see Figure 3.3):\n\nDifference in Fits (DFFITS): DFFITS is a measure of the impact of each observation on the predicted values (fitted values) of the model. It quantifies how much the predicted values would change if an observation were removed from the analysis. DFFITS values &gt; \\text{Threshold} = 2 \\sqrt{\\frac{p}{n}} indicate observations that have a substantial impact on the predicted values and may be influential or outliers. Here, p is the number of parameters in the model (including the intercept, i.e. 2 in a simple linear regression) and n is the number of observations.\nCook’s Distance Plot: Cook’s distance is a measure of the influence of each observation on the estimated regression coefficients. The Cook’s distance plot shows the Cook’s distance values for each observation against the row numbers (or observation numbers). Points with large Cook’s distance values (typically greater than \\frac{4}{n}) indicate observations that are potentially influential and may have a significant impact on the regression results.\nResiduals vs Leverage Plot: This plot displays the standardised residuals against the leverage values (hat values) for each observation. Leverage values measure the influence of an observation on the fitted values (predicted values) of the model. The plot helps identify outliers and influential observations. Points with high leverage (typically greater than 2-3 times the average leverage) and large residuals are considered influential observations that may warrant further investigation or potential removal from the analysis.\nCook’s Distance vs Lev./(1-Lev.) Plot: This plot combines information from Cook’s distance and leverage values. The x-axis represents the leverage values divided by (1 minus the leverage values), which is a transformation that spreads out the points for better visualisation. The y-axis shows the Cook’s distance values. This plot helps identify influential observations by considering both their impact on the regression coefficients (Cook’s distance) and their influence on the fitted values (leverage). Points in the top-right corner of the plot indicate observations that are potentially influential and may require further examination or removal.\n\n\n1cooksd_thresh &lt;- 4 / nrow(mod1_data)\n2dffits_threshold &lt;- 2 * sqrt(2 / nrow(Adelie))\n\nmod1_data &lt;- mod1_data %&gt;%\n  mutate(index = row_number(),\n         leverage = hatvalues(mod1),\n         dffits = dffits(mod1),\n         colour = ifelse(.cooksd &gt; cooksd_thresh, \"black\", \"pink\"))\n\n\n1\n\nCalculate thresholds for Cook’s distance.\n\n2\n\nCalculate the threshold for DFFITS.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.3: Diagnostic plots for visual inspection of outliers in the pernguin data. A) Difference in Fits (DFFITS) for mod1. B) Cook’s distance. C) Residuals vs. leverage. D) Cook’s distance vs. Lev./(1-Lev.). Outliers are identified beyond the Cook’s distance threshold (4/n) and are plotted in black and their row numbers in dark red. The vertical dashed blue lines in C) and D) are positioned at 2 times the average leverage. The horizontal red dashed lines in B) and D) are located at the Cook’s distance threshold. A) to C) are custom ggplot2 plots corresponding to plot(mod1, which = c(4, 5, 6)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.4: Plot of the linear regression resulting from mod1 with the outliers identified using Cook’s distance highlighted.\n\n\n\n\n\nOnce we have found them (Figure 3.4), what do we do with outliers? There are a few strategies:\n\nRemove them: If the outliers are due to data entry errors or other issues, it may be appropriate to remove them from the analysis. However, this should be done with caution, as outliers may be functionally important in the dataset if they represent rare, extreme events.\nRobust regression methods: When there is certainty that the outliers are part of the observed response and represent extreme but rare occurrences, robust regression techniques such as M-estimation or least trimmed squares, which are less sensitive to the presence of outliers, could be used.\nTransformation of variables: Applying appropriate transformations (e.g., logarithmic, square root) to the variables can sometimes reduce the impact of outliers.\n\n\n\nInterpret the Results\nNow that we have tested the assumptions, we can interpret the results of the model fitted in Section 3.6.4. The slope of the regression line is 0.003188 mm/g, with a standard error of \\pm 0.0003977. The p-value is less than 0.001, so we reject the null hypothesis that the slope is zero. We conclude that there is a significant relationship between the body mass of the penguins and their bill length.\nThe fit of the model is given by the multiple R^2 value, which is 0.3013. This means that 30.13% of the variation in bill length can be explained by body mass. The remaining ~70% is due to other factors not included in the model. The intercept of the model is 26.99 mm, with a standard error of \\pm 0.0003977. The intercept is the value of the dependent variable when the independent variable is zero. In this case, it is the bill length of a penguin with a body mass of zero grams, which is not a meaningful value.\nThe significance of the overall fit of the model can be assessed using an analysis of variance (ANOVA) test. The p-value is less than 0.001, so we reject the null hypothesis that the model does not explain a significant amount of the variation in the data against an F-value of 64.25 on 1 and 149 degrees of freedom. We conclude that the model is a good fit for the data.\n\n\nReporting\nI provide example Methods, Results, and Discussion sections in a format more-or-less suited for inclusion in a scientific manuscript. Feel free to use it as a template and edit it as necessary to describe your study.\nMethods\nStudy data\nThe data analysed in this study were derived from the Palmer Penguins dataset, a comprehensive collection of measurements from three penguin species (Adelie, Chinstrap, and Gentoo) collected in the Palmer Archipelago, Antarctica. The dataset includes variables species, island, bill length, bill depth, flipper length, body mass, and sex of the penguins. This dataset has been made publicly available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\nStatistical analysis\nThe primary objective of our statistical analysis was to investigate the relationship between the penguins’ body mass and bill length. For this purpose, we employed a simple linear regression model to quantify the extent to which the independent variable predicts bill length.\nWe fitted a simple linear regression model using the lm() function in R version 4.4.0 (R Core Team, 2024). The model included bill length as the dependent variable, and body mass as continuous predictor. We ensured all assumptions for linear regression were assessed including linearity, independence, homoscedasticity, and normality of residuals.\nAfter fitting the model, diagnostic plots were generated using the plot() function in R to visually assess the residuals for any patterns indicating potential violations of regression assumptions. Additionally, the Shapiro-Wilk test was conducted to confirm the normality of the residuals. The presence of heteroscedasticity was evaluated using the Breusch-Pagan test.\nThe adequacy of the model fit was judged based on the coefficient of determination (R2), which provided insight into the variance in body mass explained by the predictors. The significance of the regression coefficients was determined using t-tests, and the overall model fit was evaluated by an F-test.\nResults\n\n\n\n\n\n\n\n\nFigure 3.5: Plot of bill length as a function of body mass for Adelie penguins sampled at the Palmer Station. The straight line indicates the best fit regression line and the blue shading is the 95% confidence interval.\n\n\n\n\n\nThe regression coefficient for bill length with respect to body mass was estimated to be approximately 3.2 \\times 10^{-3} mm/g \\pm 3.977 \\times 10^{-4} (mean slope \\pm SE) (p &lt; 0.001, t = 8.015), indicating a significant dependence of bill length on body mass (Figure 3.5).\nThe multiple R^2 value of the model was 0.3013, suggesting that approximately 30.13% of the variability in bill length can be accounted for by changes in body mass. This indicates that while bill length variation is notably influenced by body mass, about 69.87% of the variation is attributable to other factors not included in the model.\nThe overall fit of the model, assessed by an ANOVA, strongly supported the model’s validity (F = 64.25, p &lt; 0.001, d.f. = 1, 149) and confirms that a linear model provides adequate support for predicting penguin bill length from body mass.\nDiscussion\nIn conclusion, the statistical analysis confirms a significant relationship between body mass and bill length in penguins. Although the model explains a substantial portion of the variation, future studies should consider additional variables that could account for the remaining variability in bill length. This would enhance our understanding of the morphological adaptations of penguins in their natural habitat.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "simple_linear_regression.html#confidence-and-prediction-intervals",
    "href": "simple_linear_regression.html#confidence-and-prediction-intervals",
    "title": "3  Linear Regression",
    "section": "3.7 Confidence and Prediction Intervals",
    "text": "3.7 Confidence and Prediction Intervals\nConfidence intervals estimate the range within which the true mean of the dependent variable (Y) is likely to fall for a given value of the independent variable (X). In other words, if you were to repeat your experiment many times and calculate the mean response at a specific X value each time, the confidence interval would contain the true population mean a certain percentage of the time (e.g., 95%). Therefore, a 95% confidence interval means you can be 95% confident that the interval contains the true mean response for the population at that particular X value. It’s about the average, not individual data points.\nPrediction intervals, on the other hand, provide a range of Y values that are likely to contain a single new observation of the dependent variable for a given value of the independent variable X. These intervals account for the variability around individual observations and are generally wider than confidence intervals because they include both the variability of the estimated mean response and the variability of individual observations around that mean. Continuing with the Adelie penguin data, the confidence and prediction intervals are shown in Figure 3.6.\n\n# Predict values with confidence intervals\npred_conf &lt;- as.data.frame(predict(mod1,\n                                   newdata = Adelie,\n                                   interval = \"confidence\"))\n\n# Predict values with prediction intervals\npred_pred &lt;- as.data.frame(predict(mod1,\n                                   newdata = Adelie,\n                                   interval = \"prediction\"))\n\n# Add body mass to the data frame\nresults &lt;- cbind(Adelie, pred_conf, pred_pred[,2:3])\n\n# Rename columns for clarity\nnames(results)[c(9:13)] &lt;- c(\"fit\", \"lwr_conf\", \"upr_conf\",\n                             \"lwr_pred\", \"upr_pred\")\n\nggplot(data = results, aes(x = body_mass_g, y = fit)) +\n  geom_line(linewidth = 0.4, colour = \"red\") +\n  geom_ribbon(aes(ymin = lwr_pred, ymax = upr_pred),\n              alpha = 0.2, fill = \"red\") +\n  geom_ribbon(aes(ymin = lwr_conf, ymax = upr_conf),\n              alpha = 0.2, fill = \"blue\") +\n  geom_point(aes(y = bill_length_mm), shape = 1) +\n  labs(x = \"Body mass (g)\", y = \"Bill length (mm)\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 3.6: Plot of pernguin data with the confidence interval (blue) and prediction interval (pink) around the fitted values.\n\n\n\n\n\nConfidence and prediction intervals are relevant for understanding the uncertainty associated with a linear regression model’s predictions. While confidence intervals focus on quantifying the uncertainty around the estimated mean response, prediction intervals comprehensively assess the variability that can be expected for individual observations. We can use both when interpreting the results of a linear regression analysis.\nConfidence intervals are useful when the primary interest lies in making inferences about the mean response at specific values of the independent variable(s). For instance, in a study examining the relationship between soil nutrient levels and plant biomass, confidence intervals can help determine the range of mean biomass that can be expected for a given level of soil nutrients. This information may be valuable for crop management practices, such as designing fertilisation strategies or assessing the impact of nutrient depletion on plant productivity.\nPrediction intervals, on the other hand, are more relevant when the goal is to predict the value of an individual observation or to assess the range of values that future observations might take. For example, in a study investigating the relationship between ambient temperature and the growth rate of a species of fish, prediction intervals provide a range of growth rates that an individual fish might exhibit based on the observed temperature. This information is invaluable in aquaculture, for instance, where predicting individual growth patterns can inform decisions about optimal stocking densities or feed management strategies.\nThe relative widths of confidence and prediction intervals can provide insights into the variability in the data. If the prediction intervals are substantially wider than the confidence intervals, it may indicate a high level of variability in individual observations around the mean response, which could suggest the presence of influential factors or sources of variation that are not accounted for by the current model, such as microhabitat differences or genetic variation within the studied population.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "simple_linear_regression.html#sec-assumption-fails",
    "href": "simple_linear_regression.html#sec-assumption-fails",
    "title": "3  Linear Regression",
    "section": "3.8 What Do I Do When Some Assumptions Fail?",
    "text": "3.8 What Do I Do When Some Assumptions Fail?\n\nFailing Assumptions of Normality and Homoscedasticity\nI will use the sparrow data from Zar (1999) to demonstrate what to do when the assumptions of normality and homoscedasticity are violated. I will fit a linear model to the data and then check the assumptions.\n\n\n\n\n\n\n\n\nFigure 3.7: Scatter plot of the sparrow dataset with a best fit line.\n\n\n\n\n\nFigure 3.7 is a scatter plot of the sparrow data with a best fit line. At first glance, the linear model seems to almost perfectly describe the relationship of wing length on age. I will fit a linear model to the data and then check the assumptions.\n\nmod2 &lt;- lm(wing ~ age, data = sparrows)\nsummary(mod2)\n\n\nCall:\nlm(formula = wing ~ age, data = sparrows)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30699 -0.21538  0.06553  0.16324  0.22507 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.71309    0.14790   4.821 0.000535 ***\nage          0.27023    0.01349  20.027 5.27e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2184 on 11 degrees of freedom\nMultiple R-squared:  0.9733,    Adjusted R-squared:  0.9709 \nF-statistic: 401.1 on 1 and 11 DF,  p-value: 5.267e-10\n\n\nCheck the assumption of normality of residuals using the Shapiro-Wilk test, a histogram, and a residual Q-Q plot.\n\nshapiro.test(residuals(mod2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(mod2)\nW = 0.84542, p-value = 0.02487\n\n\nThe p-value for the Shapiro-Wilk test is &lt; 0.05, indicating that the residuals are not normally distributed. The histogram and Q-Q plot of the residuals also show that the residuals are not normally distributed (Figure 3.8 and Figure 3.9). In the Residual Q-Q plot, the points deviate from the straight line, indicating non-normality—note the S-shaped curvature to the data.\n\nhist(residuals(mod2))\n\n\n\n\n\n\n\nFigure 3.8: A histogram of the residual of the linear regression, mod2.\n\n\n\n\n\n\nplot(mod2, which = 2)\n\n\n\n\n\n\n\nFigure 3.9: A Residual Q-Q plot of the linear regression, mod2.\n\n\n\n\n\nIt is enough to know that the normality assumption is not met – I cannot proceed with a simple linear regression. However, let us for completeness also look at the homoscedasticity assumption. I will use the Breusch-Pagan test to check for homoscedasticity, followed by a plot of residuals against fitted values.\n\nbptest(mod2)\n\n\n    studentized Breusch-Pagan test\n\ndata:  mod2\nBP = 1.6349, df = 1, p-value = 0.201\n\n\nThe p-value for the Breusch-Pagan test is &gt; 0.05, indicating that the residuals are homoscedastic. The plot of residuals against fitted values shows gives a slightly different impression (Figure 3.10).\n\nplot(mod2, which = 1)\n\n\n\n\n\n\n\nFigure 3.10: A plot of residuals against fitted values for the linear regression, mod2.\n\n\n\n\n\nThe assumptions of normality and homoscedasticity are violated (it is sufficient that one or the other fails, not both). As already noted, I cannot proceed with the linear model. I will need to consider alternative models or transformations to address these issues.\nWhen the assumptions of normality and homoscedasticity are violated, I have some options—these broadly group into transforming the data and using a non-parametric test.\nTransforming the data can sometimes help attain normality and homoscedasticity. Common transformations include the logarithmic, square root, and inverse transformations. However, be cautious when interpreting the results of transformed data, as the transformed coefficients may not be directly interpretable.\nI will show the Theil-Sen estimator (also known as Sen’s slope estimator) as a robust non-parametric replacement for a simple linear model. It calculates the median of the slopes of all pairs of sample points to determine the overall slope of the line.\n\nlibrary(mblm)\n\nmod3 &lt;- mblm(wing ~ age, data = sparrows)\nsummary(mod3)\n\n\nCall:\nmblm(formula = wing ~ age, dataframe = sparrows)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.44524 -0.31190 -0.00714  0.06905  0.14048 \n\nCoefficients:\n            Estimate     MAD V value Pr(&gt;|V|)    \n(Intercept)  0.75000 0.18532      91 0.000244 ***\nage          0.27619 0.00956      91 0.000244 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.244 on 11 degrees of freedom\n\n\nThe interpretation of the Theil-Sen estimator is similar to the simple linear regression. The Theil-Sen estimator provides a robust estimate of the slope of the relationship between age and wing length. The slope of the line is 0.28 (\\pm 0.19 mean absolute deviation) (V value = 91, p &lt; 0.001), indicating that for each additional day of age, the wing length increases by 0.28 cm. The intercept of the line is 0.75, indicating that the wing length is ~0.8 cm when the age is 0 days.\n\n\nMy Data Do Not Display a Linear Response\nIn simple linear regression, the dependent variable Y is expected to exhibit a straight-line relationship with the independent variable X. However, several factors can cause deviations from a linear pattern.\nStatistical assumptions underlying linear regression can affect the appearance of a linear response. The normality assumption is important but primarily pertains to the residuals rather than the Y vs. X plot. A scatterplot of Y vs. X might deviate from a linear pattern due to the non-normality of the residuals or heteroscedasticity, where the variability of the residuals changes with the level of X. Addressing these issues and then reassessing the linearity of the relationship is a logical first step. Refer to Section 3.8 for more details on how to proceed.\nOutliers in the data can significantly impact the regression line, leading to misleading results (Section 3.6.6). Measurement errors in the independent variable can also lead to biased and inconsistent estimations, which may require revisiting the data collection process to address systemic problems. Variable bias, where excluding relevant variables distorts the observed relationship, could also explain seemingly nonlinear responses. Considering multiple predictor variables in a regression model (Chapter 5) might be more appropriate in such situations.\nIt’s important to note that simple linear regression might not be suitable for all scenarios. For instance, the dependent variable Y might inherently follow a different probability distribution, such as a Poisson or a binomial distribution, rather than a normal distribution. This is particularly relevant in count data or binary outcome scenarios. In such cases, other types of models like Poisson regression or logistic regression, accommodated by generalised linear models (GLM; Chapter 6), would be more appropriate.\nLastly, if the data do not exhibit a linear relationship even after addressing these issues, the relationship between the variables may really be nonlinear. This can occur when the underlying functional relationship between X and Y is better described by exponential, logarithmic, or other more complex mechanistic responses. In such cases, nonlinear regression (Chapter 7) or generalised additive models (GAM; Chapter 11) might be necessary to describe the relationship between the variables accurately.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "polynomial_regression.html",
    "href": "polynomial_regression.html",
    "title": "4  Polynomial Regression",
    "section": "",
    "text": "Polynomial regressions may resemble non-linear regression in terms of the visual appearance of the regression line (i.e. with bends and curves), but they handle non-linearity by transforming the independent variable X into higher powers (e.g., X^2, X^3), which are then included in the model along with coefficients that are linear in terms of estimation. For instance, a cubic (of order, degree, or power 3; denoted as m) polynomial regression model (Figure 7.1 A) and is expressed as:\nY_i = \\alpha + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3 + \\epsilon_i \\tag{4.1}\nWhere:\n\nY_i is the response variable for the i-th observation,\nX_i is the predictor variable for the i-th observation,\n\\alpha is the intercept,\n\\beta_1, \\beta_2, and \\beta_3 are the coefficients for the linear, quadratic, and cubic terms, respectively, and\n\\epsilon_i is the error term for the i-th observation (the residuals).\n\nIt is worth noting that higher order polynomials can lead to overfitting, where the model captures the noise in the data rather than the inherent pattern. This can result in poor generalisation to new data and poor predictive performance. Overfitting becomes more likely as m increases. The m of a polynomial should not exceed n-1, where n is the number of data points. An m greater than 4 or 5 is rarely justified.1 If m=n-1, the polynomial will fit the data perfectly (i.e., R^2=1). For example, a linear regression (m=1) fits two data points exactly, a quadratic regression (m = 2) fits three data points perfectly, and so on. Therefore, always consider the trade-off between model complexity and generalisation when using polynomial regression.\n1 The appropriate maximum m can be determined using methods such as the backward-elimination or forward-selection multiple-regression procedure.Another complication is that the biological interpretation of more complex (higher order) models may be lacking. However, polynomial regression are more often than not used for prediction rather than their interpretability.\n&lt;THE REST OF THIS CHAPTER IS TO BE DEVELOPED.&gt;",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Polynomial Regression</span>"
    ]
  },
  {
    "objectID": "multiple_linear_regression.html",
    "href": "multiple_linear_regression.html",
    "title": "5  Multiple Linear Regression",
    "section": "",
    "text": "5.1 Multiple Linear Regression\nMultiple linear regression helps us answer questions such as:\nMultiple linear regression extends the simple linear regression model to include several independent variables. The model is expressed as: Y_i = \\alpha + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_k X_{ik} + \\epsilon_i \\tag{5.1} Where:\nWhen including a categorical variable in a multiple linear regression model, dummy (indicator) variables are used to represent the different levels of the categorical variable. Let’s assume we have a categorical variable C with three levels: C_1, C_2, and C_3. We can represent this categorical variable using two dummy variables:\nC_1 is considered the reference category and does not get a dummy variable. This way, we avoid multicollinearity (see Section 5.6.4). R’s lm() function will automatically convert the categorical variables to dummy variables (sometimes called treatment coding). The first level of the alphabetically sorted categorical variable is taken as the reference level. See Section 8.5 for more information about how to include categorical variables in a multiple linear regression model. At the end of the chapter you’ll find alternative ways to assess categorical variables in a multiple linear regression model (Section 5.9).\nAssume we also have k continuous predictors X_{1}, X_{2}, \\ldots, X_{k}. The multiple linear regression model with these predictors and the categorical variable can be expressed as: Y_i = \\alpha + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_k X_{ik} + \\gamma_1 D_{i1} + \\gamma_2 D_{i2} + \\epsilon_i \\tag{5.2} Where:",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "multiple_linear_regression.html#multiple-linear-regression",
    "href": "multiple_linear_regression.html#multiple-linear-regression",
    "title": "5  Multiple Linear Regression",
    "section": "",
    "text": "How do various environmental factors influence the population size of a species? Factors like average temperature, precipitation levels, and habitat area can be used to predict the population size of a species in a given region. Which of these factors are most important in determining the population size?\nWhat are the determinants of plant growth in different ecosystems? Variables such as soil nutrient content, water availability, and light exposure can help predict the growth rate of plants in various ecosystems. How do these factors interact to influence plant growth?\nHow do genetic and environmental factors affect the spread of a disease in a population? The incidence of a disease might depend on factors like genetic susceptibility, exposure to pathogens, and environmental conditions (e.g., humidity and temperature). What is the relative importance of these factors in determining the spread of the disease?\n\n\n\n\nY_i is the response variable for the i-th observation,\n\nX_{i1}, X_{i2}, \\ldots, X_{ik} are the k predictor variables for the i-th observation,\n\n\\alpha is the intercept,\n\n\\beta_1, \\beta_2, \\ldots, \\beta_k are the coefficients for the k predictor variables, and\n\n\\epsilon_i is the error term for the i-th observation (the residuals).\n\n\n\n\nD_1: Equals 1 if C = C_2, 0 otherwise.\n\nD_2: Equals 1 if C = C_3, 0 otherwise.\n\n\n\n\n\nY_i is the dependent variable for observation i.\n\n\\alpha is the intercept term.\n\n\\beta_1, \\beta_2, \\ldots, \\beta_k are the coefficients for the continuous independent variables X_{i1}, X_{i2}, \\ldots, X_{ik}.\n\nD_{i1} and D_{i2} are the dummy variables for the categorical predictor C.\n\n\\gamma_1 and \\gamma_2 are the coefficients for the dummy variables, representing the effect of levels C_2 and C_3 relative to the reference level C_1.\n\n\\epsilon_i is the error term for observation i.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "multiple_linear_regression.html#nature-of-the-data",
    "href": "multiple_linear_regression.html#nature-of-the-data",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.2 Nature of the Data",
    "text": "5.2 Nature of the Data\nYou are referred to the discussion in simple linear regression (Section 3.1). The only added consideration is that the data should be multivariate, i.e., it should contain more than one predictor variable. The predictor variables are generally continuous, but there may also be categorical variables.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "multiple_linear_regression.html#assumptions",
    "href": "multiple_linear_regression.html#assumptions",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.3 Assumptions",
    "text": "5.3 Assumptions\nBasically, this is as already discussed in simple linear regression (Section 3.1)—in multiple linear regression, the same assumptions apply to the response relative to each of the predictor variables. In Section 5.6.7 I will assess the assumptions in an example dataset. An additional consideration is that the predictors must not be highly correlated with each other (multicollinearity) (see Section 5.6.4).",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "multiple_linear_regression.html#outliers",
    "href": "multiple_linear_regression.html#outliers",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.4 Outliers",
    "text": "5.4 Outliers\nAgain, this is as discussed in simple linear regression (Section 3.1). In multiple linear regression, the same considerations apply to the response relative to each of the predictor variables.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "multiple_linear_regression.html#sec-r-function",
    "href": "multiple_linear_regression.html#sec-r-function",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.5 R Function",
    "text": "5.5 R Function\nThe lm() function in R is used to fit a multiple linear regression model. The syntax is similar to that of the lm() function used for simple linear regression, but with multiple predictor variables. The function takes the basic form:\nlm(formula, data)\nFor a multiple linear regression with only continuous predictor variables (as in Equation 5.1), the formula is:\n\nlm(response ~ predictor1 + predictor2 + ... + predictorN,\n   data = dataset)\n\nInteraction effects are implemented by including the product of two variables in the formula. For example, to include an interaction between predictor1 and predictor2, we can use:\n\nlm(response ~ predictor1 * predictor2, data = dataset)\n\nWhen we have both continuous and categorical predictor variables (Equation 5.2), the formula is:\n\nlm(response ~ continuous_predictor1 + continuous_predictor2 + ...\n   + continuous_predictorN + factor(categorical_predictor1) +\n     factor(categorical_predictor2) + ...\n   + factor(categorical_predictorM),\n   data = dataset)",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "multiple_linear_regression.html#sec-example1",
    "href": "multiple_linear_regression.html#sec-example1",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.6 Example 1: The Seaweed Dataset",
    "text": "5.6 Example 1: The Seaweed Dataset\nLoad some data produced in the analysis by Smit et al. (2017). Please refer to the chapter Deep Dive into Gradients on Tangled Bank for the data description.\nThis dataset is suitable for a multiple linear regression because it has continuous response variables (\\beta_\\text{sør}, \\beta_\\text{sim}, and \\beta_\\text{sne}, the Sørenesen dissimilarity, the turnover component of \\beta-diversity, and the nestedness-resultant component of \\beta-diversity, respectively), continuous predictor variables (the mean climatological temperature for August, the mean climatological temperature for the year, the temperature range for February and August, and the SD of February and August), and a categorical variable (the bioregional classification of the samples).\n\nsw &lt;- read.csv(\"data/spp_df2.csv\")\nrbind(head(sw, 3), tail(sw, 3))[,-1]\n\n       dist  bio    augMean   febRange      febSD     augSD    annMean\n1     0.000  BMP 0.00000000 0.00000000 0.00000000 0.0000000 0.00000000\n2    51.138  BMP 0.05741369 0.09884404 0.16295271 0.3132800 0.01501846\n3   104.443  BMP 0.15043904 0.34887754 0.09934163 0.4188239 0.02602247\n968 102.649 ECTZ 0.41496099 0.11330069 0.24304493 0.7538546 0.52278161\n969  49.912 ECTZ 0.17194242 0.05756093 0.18196664 0.3604341 0.24445006\n970   0.000 ECTZ 0.00000000 0.00000000 0.00000000 0.0000000 0.00000000\n              Y        Y1          Y2\n1   0.000000000 0.0000000 0.000000000\n2   0.003610108 0.0000000 0.003610108\n3   0.003610108 0.0000000 0.003610108\n968 0.198728140 0.1948882 0.003839961\n969 0.069337442 0.0443038 0.025033645\n970 0.000000000 0.0000000 0.000000000\n\n\nWe will do a multiple linear regression analysis to understand the relationship between some of the environmental variables and the seaweed species. Specifically, we will consider only the variables augMean, febRange, febSD, augSD, and annMean as predictors of the species composition as measured by \\beta_\\text{sør} (Y in the data file).\nThe model, which we will call full_mod1 below, can be stated formally as Equation 5.3: Y = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + \\beta_5 X_5 + \\epsilon \\tag{5.3} Where:\n\n\nY is the response variable, the mean Sørensen dissimilarity,\nthe predictors X_1, X_2, X_3, X_4, and X_5 correspond to augMean, febRange, febSD, augSD, and annMean, respectively, and\n\n\\epsilon is the error term.\n\nBut before we jump into multiple linear regression, let’s warm up by first fitting some simple linear regressions.\nSimple Linear Models\nFor interest sake, let’s fit simple linear models for each of the predictors against the response variable. Let’s look at relationships between the continuous predictors and the response in the East Coast Transition Zone (ECTZ), ignoring the other bioregions for now. We will first fit the simple linear models and then create scatter plots of the response variable \\beta_\\text{sør} against each of the predictor variables. To these plots, we will add a best fit (regression) lines.\n\nsw_ectz &lt;- sw |&gt; filter(bio == \"ECTZ\")\n\npredictors &lt;- c(\"augMean\", \"febRange\", \"febSD\", \"augSD\", \"annMean\")\n\n# Fit models using purrr::map and store in a list\nmodels &lt;- map(predictors, ~ lm(as.formula(paste(\"Y ~\", .x)),\n                               data = sw_ectz))\n\nnames(models) &lt;- predictors\n\nmodel_summaries &lt;- map(models, summary)\nmodel_summaries\n\n$augMean\n\nCall:\nlm(formula = as.formula(paste(\"Y ~\", .x)), data = sw_ectz)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.180961 -0.059317 -0.008346  0.045695  0.192444 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.060104   0.007359   8.168 1.01e-14 ***\naugMean     0.346011   0.010899  31.748  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07721 on 287 degrees of freedom\nMultiple R-squared:  0.7784,    Adjusted R-squared:  0.7776 \nF-statistic:  1008 on 1 and 287 DF,  p-value: &lt; 2.2e-16\n\n\n$febRange\n\nCall:\nlm(formula = as.formula(paste(\"Y ~\", .x)), data = sw_ectz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.21744 -0.08311 -0.01543  0.07536  0.25699 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.092722   0.009638   9.621   &lt;2e-16 ***\nfebRange    0.181546   0.008897  20.405   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1048 on 287 degrees of freedom\nMultiple R-squared:  0.592, Adjusted R-squared:  0.5905 \nF-statistic: 416.4 on 1 and 287 DF,  p-value: &lt; 2.2e-16\n\n\n$febSD\n\nCall:\nlm(formula = as.formula(paste(\"Y ~\", .x)), data = sw_ectz)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.24267 -0.10709 -0.02587  0.08888  0.39171 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.12018    0.01168   10.29   &lt;2e-16 ***\nfebSD        0.17166    0.01245   13.79   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1272 on 287 degrees of freedom\nMultiple R-squared:  0.3985,    Adjusted R-squared:  0.3964 \nF-statistic: 190.1 on 1 and 287 DF,  p-value: &lt; 2.2e-16\n\n\n$augSD\n\nCall:\nlm(formula = as.formula(paste(\"Y ~\", .x)), data = sw_ectz)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.307683 -0.111051 -0.003922  0.086322  0.308041 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.12781    0.01231   10.38   &lt;2e-16 ***\naugSD        0.08793    0.00720   12.21   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.133 on 287 degrees of freedom\nMultiple R-squared:  0.3419,    Adjusted R-squared:  0.3396 \nF-statistic: 149.1 on 1 and 287 DF,  p-value: &lt; 2.2e-16\n\n\n$annMean\n\nCall:\nlm(formula = as.formula(paste(\"Y ~\", .x)), data = sw_ectz)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.144251 -0.051607 -0.005023  0.045095  0.145173 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.053883   0.006309   8.541 7.94e-16 ***\nannMean     0.332150   0.008667  38.325  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0663 on 287 degrees of freedom\nMultiple R-squared:  0.8365,    Adjusted R-squared:  0.836 \nF-statistic:  1469 on 1 and 287 DF,  p-value: &lt; 2.2e-16\n\n\nThe individual models show that, for each predictor, the estimate of the coefficients (for slope) and the test for the overall hypothesis are both significant (p &lt; 0.05 in all cases; refer to the model output). All the predictor variables are therefore good predictors of the structure of seaweed species composition along.\n\n# Create individual plots for each predictor\nplts1 &lt;- map(predictors, function(predictor) {\n  ggplot(sw_ectz, aes_string(x = predictor, y = \"Y\")) +\n    geom_point(shape = 1, colour = \"dodgerblue4\") +\n    geom_smooth(method = \"lm\", col = \"magenta\", fill = \"pink\") +\n    labs(title = paste(\"Y vs\", predictor),\n         x = predictor,\n         y = \"Y\") +\n    theme_bw()\n})\n\n# Name the list elements for easy reference\nnames(plts1) &lt;- predictors\n\nggpubr::ggarrange(plotlist = plts1, ncol = 2,\n                  nrow = 3, labels = \"AUTO\")\n\n\n\n\n\n\nFigure 5.1: Individual simple linear regressions fitted to the variables augMean, febRange, febSD, augSD, and annMean as predictors of the seaweed species composition as measured by the Sørensen dissimilarity, Y.\n\n\n\n\nFigure 5.1 is a series of scatter plots showing the relationship between the response variable \\beta_\\text{sør} and each of the predictor variables. The blue line represents the linear regression fitted to the data. We see that the relationship between the response variable and each of the predictors is positive and linear. Each of the models are significant, as indicated by the p-values in the model summaries. These simple models do not tell us how some predictors might act together to influence the response variable.\nTo consider combined effects and interactions between predictor variables, we must explore multiple linear regression models that include all the predictors. Multiple regression will give us a more integrated understanding of how various environmental variables jointly influence species composition along the coast. In doing so, we can control for confounding variables, improve model fit, deal with multicollinearity, test for interaction effects, and enhance predictive power.\nWe will fit this multiple regression model next.\nState the Hypotheses for a Multiple Linear Regression\nAs with all inferential statistics, we need to consider the hypotheses when performing multiple linear regression.\nThe null hypothesis (H_0) states that there is no significant relationship between the Sørensen diversity index and any of the the climatological variables entered into the model, implying that the coefficients for all predictors are equal to zero. The alternative hypothesis (H_A), on the other hand, states that there is a significant relationship between the Sørensen diversity index and the climatological variables, positing that at least one of the coefficients is not equal to zero.\nThe hypotheses can be divided into two kinds: those dealing with the main effects and the one assessing the overall model stated in Equation 5.3.\nMain effects hypotheses\nThe main effects hypotheses test, for each predictor, X_i, if the predictor has a significant effect on the response variable Y.\nH_0: There is no linear relationship between the environmental variables (augMean, febRange, febSD, augSD, and annMean) and the community composition as measured by \\beta_\\text{sør} (in Y). Formally, for each predictor variable X_i:\n\nH_0: \\beta_i = 0 \\text{ for } i = 1, 2, 3, 4, 5\n\nWhere \\beta_i are the coefficients of the predictors in the multiple linear regression model.\nH_A: There is a linear relationship between the environmental variables (augMean, febRange, febSD, augSD, and annMean) and the species composition as measured by \\beta_\\text{sør}:\n\nH_A: \\beta_i \\neq 0 \\text{ for } i = 1, 2, 3, 4, 5\n\nOverall hypothesis\nIn addition to testing the individual predictors, X_i, we can also test a hypothesis about the overall significance of the model (F-test), which examines whether the model as a whole explains a significant amount of variance in the response variable Y. A significant F-test would suggest that at least one predictor (excluding the intercept) in the model is likely to be significantly related to the response, but it requires further investigation of individual predictors and potential multicollinearity to fully understand the relationships. For the overall model hypothesis:\nNull Hypothesis (H_0):\n\nH_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 0\n\nAlternative Hypothesis (H_A):\n\nH_A: \\exists \\, \\beta_i \\neq 0 \\text{ for at least one } i\nFit the Model\nWe fit two models:\n\na full model that includes an intercept term and the five environmental variables, and\na null model that includes only an intercept term.\n\nThe reason the null model is included is to compare the full model with a model that has no predictors. This comparison will help us determine which of the predictors are useful in explaining the response variable—we will see this in action in the forward model selection process later on (Section 5.6.5).\n\n# Select only the variables that will be used in model building\nsw_sub1 &lt;- sw_ectz[, c(\"Y\", \"augMean\", \"febRange\",\n                      \"febSD\", \"augSD\", \"annMean\")]\n\n# Fit the full and null models\nfull_mod1 &lt;- lm(Y ~ augMean + febRange + febSD +\n                 augSD + annMean, data = sw_sub1)\nnull_mod1 &lt;- lm(Y ~ 1, data = sw_sub1)\n\n# Add fitted values from the full model to the dataframe\nsw_ectz$.fitted &lt;- fitted(full_mod1)\n\nDealing With Multicollinearity\nSome of the predictor variables may be correlated with each other and this can lead to multicollinearity. When predictor variables are highly correlated, the model may not be able to distinguish the individual effects of each predictor. Consequently, the model becomes less precise and harder to interpret due to the coefficients’ inflated standard errors (Graham (2003)). One can create a plot of pairwise correlations to visually inspect the correlation structure of the predictors. I’ll not do this here, but you can try it on your own.\nA formal way to detect multicollinearity is to calculate the variance inflation factor (VIF) for each predictor variable. The VIF measures how much the variance of the estimated regression coefficients is increased due to multicollinearity. A VIF value greater than 5 or 10 indicates a problematic amount of multicollinearity.\n\ninitial_formula &lt;- as.formula(\"Y ~ .\")\n\nthreshold &lt;- 10 # Define a threshold for VIF values\n\n# Extract the names of the predictor variables\npredictors &lt;- names(vif(full_mod1))\n\n# Iteratively remove collinear variables\nwhile (TRUE) {\n  # Calculate VIF values\n  vif_values &lt;- vif(full_mod1)\n  print(vif_values) # Print VIF values for debugging\n  max_vif &lt;- max(vif_values)\n  \n  # Check if the maximum VIF is above the threshold\n  if (max_vif &gt; threshold) {\n    # Find the variable with the highest VIF\n    high_vif_var &lt;- names(which.max(vif_values))\n    cat(\"Removing variable:\",\n        high_vif_var,\n        \"with VIF:\",\n        max_vif,\n        \"\\n\")\n    \n    # Update the formula to exclude the high VIF variable\n    updated_formula &lt;- as.formula(paste(\"Y ~ . -\", high_vif_var))\n    \n    # Refit the model without the high VIF variable\n    full_mod1 &lt;- lm(updated_formula, data = sw_sub1)\n    \n    # Update the environment data frame to reflect the removal\n    sw_sub1 &lt;- sw_sub1[, !(names(sw_sub1) %in% high_vif_var)]\n  } else {\n    break\n  }\n}\n\n  augMean  febRange     febSD     augSD   annMean \n27.947767 10.806635  8.765732  2.497739 31.061900 \nRemoving variable: annMean with VIF: 31.0619 \n  augMean  febRange     febSD     augSD \n 2.290171 10.648752  8.637679  1.616390 \nRemoving variable: febRange with VIF: 10.64875 \n augMean    febSD    augSD \n1.423601 1.674397 1.585055 \n\n\nRegularisation techniques such as ridge regression, lasso regression, or elastic net can also be used to deal with multicollinearity. These advanced techniques add a penalty term to the regression model that shrinks the coefficients towards zero, which can help to reduce the impact of multicollinearity. However, these techniques are not covered in this guide. Please refer to Chapter 8 for more information on regularisation techniques.\nPerform Forward Selection\nIt might be that not all of the variables included in the full model are necessary to explain the response variable. We can use a stepwise regression to select the best combination (subset) of predictors that best explains the response variable. To do this, we will use the stepAIC function that lives in the MASS package.\nstepAIC() works by starting with the null model and then adding predictors one by one, selecting the one that improves the model the most as seen in the reduction of the AIC values along the way. This process continues until no more predictors can be added to improve the model (i.e. to further reduce the AIC). Progress is tracked as the function runs.\n\n# Perform forward selection\nmod1 &lt;- stepAIC(null_mod1,\n                scope = list(lower = null_mod1, upper = full_mod1),\n                direction = \"forward\")\n\nStart:  AIC=-1044.97\nY ~ 1\n\n          Df Sum of Sq    RSS     AIC\n+ augMean  1    6.0084 1.7108 -1478.4\n+ febSD    1    3.0759 4.6433 -1189.9\n+ augSD    1    2.6394 5.0797 -1163.9\n&lt;none&gt;                 7.7192 -1045.0\n\nStep:  AIC=-1478.41\nY ~ augMean\n\n        Df Sum of Sq    RSS     AIC\n+ febSD  1   0.36036 1.3504 -1544.8\n+ augSD  1   0.31243 1.3984 -1534.7\n&lt;none&gt;               1.7108 -1478.4\n\nStep:  AIC=-1544.77\nY ~ augMean + febSD\n\n        Df Sum of Sq    RSS     AIC\n+ augSD  1   0.10568 1.2448 -1566.3\n&lt;none&gt;               1.3504 -1544.8\n\nStep:  AIC=-1566.32\nY ~ augMean + febSD + augSD\n\n\nThe model selection process shows that as we add more variables to the model, the AIC value decreases. We can infer from this that the multiple regression model provides a better fit that simple linear models that use the variables in isolation.\nWe also see that stepAIC() has not removed any variables from the full model. Probably one reason for failing to remove any variables is that the VIF process has already accomplished this by virtue of dealing with multicollinearity. This means that all the variables retained in mod1 are important in explaining the response variable.\nAdded-Variable Plots (Partial Regression Plots)\nBefore looking at the output in more detail, I’ll introduce partial regression plots as a means to examine the relationship between the response variable and each predictor variable. Although they can be calculated by hand, the car package provides a convenient function, avPlots(), to create these plots.\nAdded variable plots are also sometimes called ‘partial regression plots’ or ‘individual coefficient plots.’ They are used to display the relationship between a response variable and an individual predictor variable while accounting for the effect of other predictor variables in a multiple regression model (the marginal effect).\n\n# Create partial regression plots\navPlots(mod1, col = \"dodgerblue4\", col.lines = \"magenta\")\n\n\n\n\n\n\nFigure 5.2: Partial regression plots for mod1 with the selected variables augMean, febSD, and augSD.\n\n\n\n\nWhat insights can we draw from the added-variable plots? Although there are better ways to assess the model fit, we can already make some observations about the linearity of the model or the presence of outliers. The slope of the line in an added variable plot corresponds to the regression coefficient for that predictor in the full multiple regression model. Seen in this way, it visually indicates the magnitude and direction of each predictor’s effect. In Figure 5.2, the added-variable plot for augMean shows a tighter clustering of points around the regression line and a strong linear relationship (steep slope) with the response variable; the plots for febSD and augSD, on the other hand, show a weaker response and more scatter about the regression line. Importantly, this suggests that augMean has a stronger and more unique contribution to the multiple-variable model than the other two variables.\nThere are also insights to be made about possible multicollinearity using added-variable plots. These plots are not a definitive test for multicollinearity, but they can provide some clues. Notably, if a predictor shows a strong relationship with the response variable in a simple correlation but appears to have little relationship in the added-variable plot, it might indicate collinearity with other predictors. This discrepancy suggests that the predictor’s effect on the response is being masked by the presence of other correlated predictors.\nModel Diagnostics\nWe are back in the territory of parametric statistics, so we need to check the assumptions of the multiple linear regression model (similar to those of simple linear regression). We can do this by making the various diagnostic plots. all of them consider various aspects of the residuals, which are simply the differences between the observed and predicted values.\nDiagnostic plots of final model\nYou have been introduced to diagnostic plots in the context of simple linear regression (Section 3.1). They are also useful in multiple linear regression. Although plot.lm() can easily do this, here I use autoplot() from the ggfortify package. When applied to the final model, mod1, the plot will in its default setting show four diagnostic plots: residuals vs. fitted values, normal Q-Q plot, scale-location plot, and residuals vs. leverage plot. Note, this is for the full model inclusive of the combined contributions of all the predictors, so we will not see separate plots for each predictor as we have seen in the added-variable plots or component plus residual plots.\n\n# Generate diagnostic plots \nautoplot(mod1, shape = 21, colour = \"dodgerblue4\",\n         smooth.colour = \"magenta\") +\n  theme_bw()\n\n\n\n\n\n\nFigure 5.3: Diagnostic plots to assess the fit of the final multiple linear regression model, mod1.\n\n\n\n\nResiduals vs. Fitted Values: In this plot we can assess linearity and homoscedasticity of the residuals. If the seaweed gods were with us, we’d expect the points to be randomly scattered about a horizontal line situation at zero. This would indicate that the relationship between the predictors selected by the forward selection process (augMean, febSD, and augSD) and the response variable (Y) is linear, and the variance of the residuals is constant across the range of fitted values. In this plot, there’s a very slight curvature which might suggest a potential issue with the linearity assumption—it is minute and I’d suggest not worrying about it. The variance of the residuals seems to decrease slightly at higher fitted values, indicating a mild case of heteroscedasticity.\nQ-Q Plot (Quantile-Quantile Plot): This plot is used to check the normality of the residuals. The points should fall approximately along a straight diagonal line if the residuals are normally distributed. Here we see that the points generally follow the line although some deviations may be seen at the tails. These deviations are not that extreme and again I don’t think this is not a big concern.\nScale-Location Plot: This plot should reveal potential issues with homoscedasticity. The square root of the standardised residuals is used here to make it easier to spot patterns, so we would like the points to be randomly scattered around the horizontal red line. Here, the line slopes slightly downward and this indicates that the variance of the residuals might decrease as the fitted values increase. We can also see evidence of this in a plot of the observed values vs. the predictors in Figure 5.3.\nResiduals vs. Leverage: This diagnostic highlights influential points (outliers). Points with high leverage (far from the mean of the predictors) can be expected to exert a strong influence on the regression line, tilting it in some direction. Cook’s distance (indicated by the yellow line) helps identify such outliers. In our seaweed data a few points could have a high leverage, but since they don’t seem to cross the Cook’s distance thresholds, I doubt they are overly worrisome.\nConsidering that no glaring red flags were raised by the diagnostic plots, I doubt that they are severe enough to invalidate the model. However, if you cannot stand these small issues, you could i) consider transforming the predictor or response variables to address your concerns about heteroscedasticity, ii) investigate the outliers (high leverage points) to confirm if they are valid data points or errors, or iii) try robust regression methods that are less sensitive to outliers and heteroscedasticity.\nComponent plus residual plots\nComponent plus residual plots offer another way to assess the fit of the model in multiple regression models. Unlike simple linear regression where we only had one predictor variable, here we have several. So, we need to assure ourselves that there is a linear relationship between each predictor variable and the response variable (we could already see this in the added-variable plots in Section 5.6.6). We can make component plus residual plots using the crPlots() function in the car package. It displays the relationship between the response variable and each predictor variable. If the relationship is linear, the points should be randomly scattered about a best fit line and the spline (in pink in Figure 5.4) should plot nearly on top of the linear regression line.\n\n# Generate component plus residual plots\ncrPlots(mod1, col = \"dodgerblue4\", col.lines = \"magenta\")\n\n\n\n\n\n\nFigure 5.4: Component plus residual diagnostic plots to assess the fit of the final multiple linear regression model, mod1.\n\n\n\n\nUnderstanding the Model Fit\nThe above model selection process has led us to the mod1 model, which can be stated formally as: Y = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\epsilon \\tag{5.4} Where:\n\n\nY: The response variable, the mean Sørensen dissimilarity.\n\nX_1, X_2, and X_3: The predictors corresponding to augMean, febSD, and augSD, respectively.\n\n\\epsilon: The error term.\n\nWe have convinced ourselves that the model is a good fit for the data, and we can proceed to examine the model’s output. The fitted model can be explored in two ways: by applying the summary() function or by using the anova() function. The summary() function provides a detailed output of the model, while the anova() function provides a table of deviance values that can be used to compare models.\nThe model summary\n\n# Summary of the selected model\nsummary(mod1)\n\n\nCall:\nlm(formula = Y ~ augMean + febSD + augSD, data = sw_sub1)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.153994 -0.049229 -0.006086  0.045947  0.148579 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.028365   0.007020   4.040 6.87e-05 ***\naugMean     0.283335   0.011131  25.455  &lt; 2e-16 ***\nfebSD       0.049639   0.008370   5.930 8.73e-09 ***\naugSD       0.022150   0.004503   4.919 1.47e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06609 on 285 degrees of freedom\nMultiple R-squared:  0.8387,    Adjusted R-squared:  0.837 \nF-statistic: 494.1 on 3 and 285 DF,  p-value: &lt; 2.2e-16\n\n\nThe first part of the summary() function’s output is the Coefficients section. This is where the main effects hypotheses are tested (this model does not have interactions—if there were, they’d appear here, too). The important components of the coefficients part of the model summary are:\n\n\n(Intercept): This row provides information about where the regression line intersects the y-axis.\nMain Effects:\n\n\naugMean, febSD, and augSD: These rows give the model coefficients associated with the slopes of the regression lines fit to those predictor variables. They indicate the rate of change in the response variable for a one-unit change in the predictor variable.\n\nEstimate, Std. Error, t value, and Pr(&gt;|t|): These columns contain the statistics used to interpret the hypotheses about the main effects. In the Estimate column are the coefficients for the y-intercept and the main effects’ slopes, and Std. Error indicates the variability of the estimate. The t value is obtained by dividing the coefficient by its standard error. The p-value tests the null hypothesis that the coefficient is equal to zero and significance codes are provided as a quick visual reference (their use is sometimes frowned upon by statistics purists). Using this information, we can quickly see that, for example, augMean has a coefficient of 0.2833 \\pm 0.0111 and the slope of the line is highly significant, i.e. there is a significant effect of Y due to the temperature gradient set up by augMean.\n\n\n\n\n\n\n\n\n\nThe intercept and slope coefficients\n\n\n\nThe interpretation of the coefficients is a bit more complicated in multiple linear regression compared to what we are accustomed to in simple linear regression. Let us look at some greater detail at the intercept and the slope coefficients:\nIntercept (\\alpha): ) The intercept is the expected value of the response variable, Y, when all predictor variables are zero. It is not always meaningful, but it can be useful in some cases.\nSlope Coefficients (\\beta_1, \\beta_2, \\ldots, \\beta_k): Each slope coefficient, \\beta_j, represents the expected change in the response variable, Y, for a one-unit increase in the predictor variable, X_j, holding all other predictor variables constant. This partial effect interpretation implies that \\beta_j accounts for the direct contribution of X_j to Y while removing the confounding effects of other predictors in the model. Figure 5.2 provides a visual representation of this concept and isolates the effect of each predictor variable on the response variable.\nTherefore, in the context of our model (Equation 5.4) for this analysis, the partial interpretation is as follows:\n\n\n\\beta_1: Represents the change in Y for a one-unit increase in X_1, holding X_2 and X_3 constant.\n\n\\beta_2: Represents the change in Y for a one-unit increase in X_2, holding X_1 and X_3 constant.\n\n\\beta_3: Represents the change in Y for a one-unit increase in X_3, holding X_1 and X_2 constant.\n\n\n\nThere are also several overall model fit statistics—it is here where you’ll find the information you need to assess the hypothesis about the overall significance of the model. Residual standard error indicates the average distance between observed and fitted values. Multiple R-squared and Adjusted R-squared values tell us something about the model’s goodness of fit. The latter adjusts for the number of predictors in the model, and is the one you must use and report in multiple linear regressions. As you also know, higher numbers approaching 1 are better, with 1 suggesting that the model perfectly captures all of the variability in the data. The F-statistic and its associated p-value test the overall significance of the model and examines whether all regression coefficients are simultaneously equal to zero. You can also use the brief overview of the residuals, but I don’t find this particularly helpful—best examine the residuals in a histogram.\nThe ANOVA tables\n\nanova(mod1)\n\nAnalysis of Variance Table\n\nResponse: Y\n           Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \naugMean     1 6.0084  6.0084 1375.660 &lt; 2.2e-16 ***\nfebSD       1 0.3604  0.3604   82.507 &lt; 2.2e-16 ***\naugSD       1 0.1057  0.1057   24.196 1.473e-06 ***\nResiduals 285 1.2448  0.0044                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis function provides a sequential analysis of variance (Type I ANOVA) table for the regression model (see more about Type I ANOVA, below). As such, this function can also be used to compare nested models. Used on a single model, it gives a more interpretable breakdown of the variability in the response variable Y and assesses the contribution of each predictor variable in explaining this variability.\nThe ANOVA table firstly shows the degrees of freedom (Df) for each predictor variable added sequentially to the model, as well as the residuals. For each predictor, the degrees of freedom is typically 1. For the residuals, however, it represents the total number of observations minus the number of estimated parameters. The Sum of Squares (Sum Sq) indicates the variability in Y attributable to each predictor, and the mean sum of squares (Mean Sq) is the sum of squares divided by the degrees of freedom.\nThe F value is calculated as the ratio of the predictor’s mean square to the residual mean square tests. It is used in testing the null hypothesis that the predictor has no effect on Y. Whether or not we accept the alternative hypothesis (reject the null) is given by the p-value (Pr(&gt;F)) that goes with each F-statistic. You know how that works.\nBecause this is a sequential ANOVA, the amount of variance in Y explained by each predictor (or group of predictors) is calculated by adding the predictors to the model in sequence (as specified in the model formula). For example, the Sum of Squares for augMean (6.0084) represents the amount of variance explained by adding augMean to a model that doesn’t include any predictors yet. The Sum of Squares for febSD 0.3604) represents the amount of variance explained by adding febSD to a model that already includes augMean—this improvement indicates that febSD explains some of the variance in Y that augMean doesn’t.\n\n\n\n\n\n\nOrder in which predictors are assessed in multiple linear regression\n\n\n\nThe interpretation of sequential ANOVA (Type I) is inherently dependent on the order in which predictors are entered. In mod1 the order is first augMean, then febSD, and last comes augSD. This order might not be the most meaningful for interpreting the sequential sums of squares and their significance in the ANOVA table. How, then, does one decide on the order of predictors in the model?\n\nIf you have a strong theoretical or causal basis for thinking that certain predictors influence others, you can enter them in that order.\nIf you have a hierarchy of predictors based on their importance or general vs. specific nature, you can enter them hierarchically.\nYou can manually fit models with different predictor orders and compare the ANOVA tables to see how the results change. This can be time-consuming but might offer insights into the sensitivity of your conclusions to the order of entry.\nYou can use automated model selection procedures, such as stepwise regression, to determine the best order of predictors. This is a more objective approach but can be criticised for being data-driven and not theory-driven.\nUse Type II or Type III ANOVAs, which are are not order-dependent and can be used to assess the significance of predictors after accounting for all other predictors in the model. However, they have their own limitations and assumptions that need to be considered.\n\nMy advice would be to have sound theoretical reasons for the order of predictors in the model.\n\n\nBoth ways of looking at the model fit of mod1—summary() and anova()—show that forward selection retained the variables augMean, febSD, and augSD. These three predictors should be used together to explain the response, Y.\nLet’s make a plot of the full model with all the initial predictors and the selected model with the predictors chosen by the forward selection process.\n\n# Add fitted values from the selected model to the dataframe\nsw_ectz$.fitted_selected &lt;- fitted(mod1)\n\n# Create the plot of observed vs fitted values for the selected model\nggplot(sw_ectz, aes(x = .fitted_selected, y = Y)) +\n  geom_point(shape = 1, colour = \"black\", alpha = 1.0) +\n  geom_point(aes(x = .fitted), colour = \"red\",\n             shape = 1, alpha = 0.4) +\n  geom_abline(intercept = 0, slope = 1,\n              color = \"blue\", linetype = \"dashed\") +\n  labs(x = \"Fitted Values\",\n       y = \"Observed Values\") +\n  theme_bw()\n\n\n\n\n\n\nFigure 5.5: Plot of observed vs. predicted value obtained from the final multiple linear regression model (mod) with the selected variables augMean, febSD, and augSD as predictors (black points), and the initial model with also annMean and febRange (red points).\n\n\n\n\nReporting\nA Results section should be written in a format sutable for inclusion in your report or publication. Present the results in a clear and concise manner, with tables and figures used to help substantiate your findings. The results should be interpreted in the context of the research question and the study design. The limitations of the analysis should also be discussed, along with any potential sources of bias or confounding. Here is an example.\nResults\nThe model demonstrates a strong overall fit, as indicated by the high R^2 value of 0.839 and an adjusted R^2 of 0.837, suggesting that approximately 83.7% of the variance in the mean Sørensen dissimilarity is explained by the predictors augMean, febSD, and augSD. All predictors in the model are statistically significant, with augMean showing the strongest effect (\\beta_1 = 0.283, p &lt; 0.0001) (Figure 5.2). The predictors febSD and augSD also have significant positive relationships with the response variable (\\beta_2 = 0.050, p = 0.0001; \\beta_3 = 0.022, p = 0.0001). A sequential ANOVA further confirms the significance of each predictor variable in the model, with all F-values indicating that the inclusion of each predictor significantly improves the model fit (p &lt; 0.0001 in all cases). Our model therefore provides clear support for the mean temperatures in August, the standard deviation of temperatures in February, and the standard deviation of temperatures in August as strong predictors of the mean Sørensen dissimilarity, with each contributing uniquely to the explanation of variability in the response variable.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "multiple_linear_regression.html#sec-mlr-interaction",
    "href": "multiple_linear_regression.html#sec-mlr-interaction",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.7 Example 2: Interaction of Distance and Bioregion",
    "text": "5.7 Example 2: Interaction of Distance and Bioregion\nOur seaweed dataset includes two additional variables that we have not yet considered. These are the continuous variable dist which represents the geographic distance between the seaweed samples taken along the coast of South Africa, and the categorical variable bio which is the bioregional classification of the seaweed samples.\nThese two new variables lend themselves to a few interesting questions. For example:\n\nIs the geographic distance between samples related to the Sørensens dissimilarity of the seaweed flora?\nDoes the average Sørensens dissimilarity vary among the bioregions to which the samples belong?\nIs the effect of geographic distance on the Sørensens dissimilarity different for each bioregion?\n\nThe most complex model is (3), the one that answers the question about whether the effect of dist on the response variable Y is different for each bioregion. Questions (1) and (2) are subsets of this more inclusive question. To fully answer these quesitons, let’s first consider the full model, which includes an interaction term between the continuous predictor dist and the categorical predictor bio. When we finally test our model, we will also have to consider the simpler models that do not include the interaction term.\n‘Interaction’ means that the effect of one predictor on the response variable is contingent on the value of another predictor. For example, we might have reason to suspect that the relationship of the Sørensens dissimilarity with the geographic distance between samples is different between the west coast compared to, say, the east coast. This is indeed a plausible expectation, but we will test this formally below.\nThe full multiple linear regression model with the interaction terms can be formally expressed as Equation :\n\\begin{align}\nY &= \\alpha + \\beta_1 \\text{dist} + \\beta_2 \\text{bio}_{\\text{B-ATZ}} + \\beta_3 \\text{bio}_{\\text{BMP}} \\nonumber \\\\\n  &\\quad + \\beta_4 \\text{bio}_{\\text{ECTZ}} + \\beta_5 (\\text{dist} \\times \\text{bio}_{\\text{B-ATZ}}) \\nonumber \\\\\n  &\\quad + \\beta_6 (\\text{dist} \\times \\text{bio}_{\\text{BMP}}) + \\beta_7 (\\text{dist} \\times \\text{bio}_{\\text{ECTZ}}) + \\epsilon \\label{mod2}\n\\end{align}\nWhere:\n\n\nY: The response variable, the mean Sørensen dissimilarity.\n\n\\alpha: The intercept term.\n\n\\text{dist}: The continuous predictor variable representing distance.\n\n\\text{bio}: The categorical predictor variable representing bioregional classification with four levels: AMP (reference category), B-ATZ, BMP, and ECTZ.\n\n\\text{bio}_\\text{B-ATZ}, \\text{bio}_\\text{BMP}, \\text{bio}_\\text{ECTZ}: Dummy variables for the bioregional classification, where:\n\n\n\\text{bio}_\\text{B-ATZ} = 1 if bio = B-ATZ, and 0 otherwise,\n\n\\text{bio}_\\text{BMP} = 1 if bio = BMP, and 0 otherwise, and\n\n\\text{bio}_\\text{ECTZ} = 1 if bio = ECTZ, and 0 otherwise.\n\n\n\n\\text{dist} \\times \\text{bio}_\\text{B-ATZ}, \\text{dist} \\times \\text{bio}_\\text{BMP}, \\text{dist} \\times \\text{bio}_\\text{ECTZ}: Interaction terms between distance and the bioregional classification dummy variables.\n\n\\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5, \\beta_6, \\beta_7: The coefficients to be estimated for the main effects and interactions.\n\n\\epsilon: The error term.\n\nIf this seems tricky, it is because of the dummy variable coding used to represent interactions in multiple linear regression. The bio variable is a categorical variable with four levels, so we need to create three dummy variables to represent the bioregional classification. The dist variable is then interacted with each of these dummy variables to create the interaction terms. The lm() function in R takes care of this for us in a far less complicated model statement. I’ll explain the details around the interpretation of dummy variable coding when we look at the output of the model with the summary() function.\nState the Hypotheses for a Multiple Linear Regression with Interaction Terms\nEquation expands into the following series of hypotheses that concern the main effects, the interactions between the main effects, and the overall hypothesis:\nMain effects hypotheses\nIn the main effects hypotheses we are concerned with the effect of each predictor variable on the response variable. For the main effect of distance we have the null:\n\nH_0: \\beta_1 = 0\n\nvs. the alternative:\n\nH_A: \\beta_1 \\neq 0\n\nFor the main effect of bioregional classification, the nulls are:\n\nH_0: \\beta_2 = 0 \\quad (\\text{bio}_{\\text{B-ATZ}})\nH_0: \\beta_3 = 0 \\quad (\\text{bio}_{\\text{BMP}})\nH_0: \\beta_4 = 0 \\quad (\\text{bio}_{\\text{ECTZ}})\n\nvs. the alternatives:\n\nH_A: \\beta_2 \\neq 0 \\quad (\\text{bio}_{\\text{B-ATZ}})\nH_A: \\beta_3 \\neq 0 \\quad (\\text{bio}_{\\text{BMP}})\nH_A: \\beta_4 \\neq 0 \\quad (\\text{bio}_{\\text{ECTZ}})\n\nHypotheses about interactions\nThis is where the hypothesis tests whether the effect of distance on the response variable is different for each bioregional classification. The null hypotheses are:\n\nH_0: \\beta_5 = 0 \\quad (\\text{dist} \\times \\text{bio}_{\\text{B-ATZ}})\nH_0: \\beta_6 = 0 \\quad (\\text{dist} \\times \\text{bio}_{\\text{BMP}})\nH_0: \\beta_7 = 0 \\quad (\\text{dist} \\times \\text{bio}_{\\text{ECTZ}})\n\nvs. the alternatives:\n\nH_A: \\beta_5 \\neq 0 \\quad (\\text{dist} \\times \\text{bio}_{\\text{B-ATZ}})\nH_A: \\beta_6 \\neq 0 \\quad (\\text{dist} \\times \\text{bio}_{\\text{BMP}})\nH_A: \\beta_7 \\neq 0 \\quad (\\text{dist} \\times \\text{bio}_{\\text{ECTZ}})\n\nOverall hypothesis\nThe overall hypothesis states that all coefficients associated with the predictors (distance, bioregional categories, and their interactions) are equal to zero, therefore indicating no relationship between these predictors and the response variable, the Sørensen index. The null hypothesis is:\n\nH_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = \\beta_6 = \\beta_7 = 0\n\nvs. the alternative:\n\nH_A: \\exists \\, \\beta_i \\neq 0 \\text{ for at least one } i\n\n\n\n\n\n\n\n\nFigure 5.6: Plot of main effects of A) distance along the coast and B) bioregional classification on the Sørensen dissimilarity index.\n\n\n\n\nVisualise the Main Effects\nTo facilitate the interpretation of the main effects hypotheses and make an argument for why an interaction term might be necessary, I’ve visualised the main effects (Figure 5.6). I see this as part of my exploratory data analysis ensemble of tests. We see that fitting a straight line to the Y vs. distance relationship seems unsatisfactory as there is too much scatter around that single line to adequately capture all the structure in the variability of the points. Colouring the points by bioregion reveals the hidden structure. The model could benefit from including an additional level of complexity: see how points in the same bioregion show less scatter compared to points in different bioregions.\nNow look at the boxplots of the Sørensen dissimilarity index for each bioregional classification. It shows that the median values of the Sørensen dissimilarity index are different for each bioregion. Taken together, Figure 5.6 (A, B) provide a good indication that adding the bioregional classification might be an important predictor of the Sørensen dissimilarity index as a function of distance between pairs of sites along the coast.\nNext, we will move ahead and fit the model inclusive of the distance along the coast and bioregion as per Equation .\nFit and Assess Nested Models\nI have a suspicion that the full model (mod2; see below) with the interaction terms will be a better fit than reduced models with only the effect due to distance (seen independently). How can we have greater certainty that we should indeed favour a slightly more complex model (with two predictors) over a simpler one with only (distance only)?\nOne way to do this is to use a nested model comparison. We will fit a reduced model (one slope for all bioregions) and compare this model to the full model (slopes are allowed to vary among bioregions).\n\n# Fit the linear regression model with only distance\nmod2a &lt;- lm(Y ~ dist, data = sw)\n\n# Fit the multiple linear regression model with interaction terms\nmod2 &lt;- lm(Y ~ dist * bio, data = sw)\n\nThis is a nested model where mod2a is nested within mod2. ‘Nested’ means that the reduced model is a subset of the full model. Nested models can be used to test hypotheses about the significance of the predictors in the full model—does adding more predictors to the model improve the fit? Comparing a nested model with a full model can be done with a sequential ANOVA, which is what the anova() function also does (in addition to its use in Section 5.6.8).\nSo, comparing mod2a to mod2 with an F-test tests the significance of adding the bio and using it together with dist. The interaction is built into mod2 but we are not yet testing the significance of the interaction terms. We will do that later.\n\nanova(mod2a, mod2, test = \"F\")\n\nAnalysis of Variance Table\n\nModel 1: Y ~ dist\nModel 2: Y ~ dist * bio\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    968 7.7388                                  \n2    962 2.2507  6    5.4881 390.95 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe sequential ANOVA shows that there is significant merit to consider an interaction term in the model. This model would then allow us to have a separate slope for the Sørensen index as function of distance for each bioregion. The residual sum of squares (RSS) decreases from 7.7388 in Model 1 to 2.2507 in Model 2, which indicates that Model 2 explains a significantly larger proportion of the variance in the response variable. The F-test for comparing the two models yields an F-value of 390.95 with a highly significant p-value (&lt; 0.0001). The improvement in model fit due to the inclusion of the interaction term is therefore statistically significant.\nThe above analyses skirted around the questions stated in the beginning of Section 5.7. I’ve provided statistical evidence that full model is a better fit than the reduced model (the sequential F-test tested this), so we should use both dist and bio in the model. I have not looked explicitly at the main effects of the predictors. However, we can easily address questions (1) and (2):\n\nQuestion 1: looking at the summary of mod2a tells us that the main effect of dist is a significant (p &lt; 0.0001) predictor of the Sørensen dissimilarity index.\nQuestion 2: the main effect of bio is also significant (p &lt; 0.0001), which is what we’d see if we fit the model mod2b &lt;- lm(Y ~ bio, data = sw).\n\nQuestion 3 warrants deeper investigation. Next, we will look at the interaction terms in the full model mod2 to see if the effect of dist on Y is different for each level of bio.\nInterpret the Full Model\nThe model summary\n\n# Summary of the model\nsummary(mod2)\n\n\nCall:\nlm(formula = Y ~ dist * bio, data = sw)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.112117 -0.030176 -0.004195  0.023698  0.233520 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    5.341e-03  4.177e-03   1.279   0.2013    \ndist           3.530e-04  1.140e-05  30.958  &lt; 2e-16 ***\nbioB-ATZ      -6.140e-03  1.659e-02  -0.370   0.7114    \nbioBMP         3.820e-02  6.659e-03   5.737 1.29e-08 ***\nbioECTZ        1.629e-02  6.447e-03   2.527   0.0117 *  \ndist:bioB-ATZ  7.976e-04  1.875e-04   4.255 2.30e-05 ***\ndist:bioBMP   -1.285e-04  2.065e-05  -6.222 7.31e-10 ***\ndist:bioECTZ   4.213e-04  1.801e-05  23.392  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04837 on 962 degrees of freedom\nMultiple R-squared:  0.8607,    Adjusted R-squared:  0.8597 \nF-statistic: 849.2 on 7 and 962 DF,  p-value: &lt; 2.2e-16\n\n\nIn the output returned by summary(mod2), we need to pay special attention to the use of dummy variable encoding for the categorical predictor. The Coefficients section is similar to that of mod1 (see Section 5.6.8), but now it includes the categorical predictor bio* and the interaction terms dist:bio* (* indicating the levels of the categorical variable). The bio variable has four levels, BMP, B-ATZ, AMP, and ECTZ, and AMP is selected as reference level. This decision to selected AMP as reference is entirely arbitrary, and alphabetical sorting offers a convenient approach to selecting the reference. The coefficients for the other levels of bio are interpreted as the sum of the response variable and the reference level.\nThe following are the key coefficients in the model summary:\n\n\n(Intercept): This is the estimated average value of Y when dist is zero and bio is the reference category (AMP). Its p-value (&gt; 0.05) suggests it’s not significantly different from zero.\nMain Effects:\n\n\ndist: This represents the estimated change in Y for a one-unit increase in dist when the bioregion is the reference category, AMP. The highly significant p-value (&lt; 0.0001) indicates a strong effect of distance in the AMP.\n\nbioB-ATZ, bioBMP, bioECTZ: These are dummy variables representing different bioregions. Their coefficients indicate the difference in the average value of Y between each of these bioregions and the reference bioregion when dist is zero. Only bioBMP and bioECTZ are significantly different from the reference bioregion, AMP.\n\n\nInteraction Effects:\n\n\ndist:bioB-ATZ, dist:bioBMP, dist:bioECTZ: These interaction terms capture how the effect of dist on Y varies across different bioregions. For instance, dist:bioB-ATZ indicates the additional change in the effect of dist in the B-ATZ bioregion compared to the reference bioregion, AMP. All interaction terms are highly significant, suggesting the effect of distance is different across bioregions.\n\n\n\nGiven this explanation, we can now interpret the coefficients of, for example, the bioB-ATZ main effect and dist:bioB-ATZ interaction. Since AMP is the reference bioregion, its effect is absorbed into the intercept term. Therefore, the coefficient for bioB-ATZ directly reflects the difference we are interested in. The coefficient for bioB-ATZ is -0.0061 \\pm 0.0166 lower than that of the reference, but the associated p-value (&gt; 0.05) indicates that the average value of Y in the B-ATZ bioregion is not significantly different from the reference bioregion, AMP.\nIf we’d want to report the actual coefficient for B-ATZ, we’d calculate the sum of the coefficients for (Intercept) and bioB-ATZ. This would give us the estimated average value of Y in the B-ATZ bioregion when dist is zero. The associated SE is calculated as the square root of the sum of the squared SEs of the two coefficients. Therefore, the coefficient for B-ATZ is -8^{-4} \\pm 0.0171.\nThe coefficient of 8^{-4} for dist:bioB-ATZ indicates that the effect of distance on Y is 8^{-4} units greater in the B-ATZ bioregion compared to the AMP bioregion. The SE of 2^{-4} suggests a high level of precision in this estimate, and the p-value (&lt; 0.0001) indicates that this difference is statistically significant.\nAs before, to calculate the actual coefficient for dist in the B-ATZ bioregion, we’d sum the coefficients for dist and dist:bioB-ATZ. The associated SE of this sum is calculated as the square root of the sum of the squared SEs of the two coefficients. Therefore, the coefficient for dist in the B-ATZ bioregion is 0.0012 \\pm 2^{-4}.\nConcerning the overall hypothesis, the Adjusted R-squared value of 0.8597 indicates that the model explains 85.97% of the variance in the response variable Y. The F-statistic and associated p-value (&lt; 0.0001) indicate that the model as a whole is highly significant, meaning at least one of the predictors (including interactions) has a significant effect on Y.\nThe ANOVA table\n\n# The ANOVA table\nanova(mod2)\n\nAnalysis of Variance Table\n\nResponse: Y\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndist        1 8.4199  8.4199 3598.79 &lt; 2.2e-16 ***\nbio         3 3.6232  1.2077  516.21 &lt; 2.2e-16 ***\ndist:bio    3 1.8648  0.6216  265.69 &lt; 2.2e-16 ***\nResiduals 962 2.2507  0.0023                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table’s interpretation is intuitive and simple: the Pr(&gt;F) column shows the p-value for each predictor in the model. The dist predictor has a highly significant effect on Y (&lt; 0.0001), as do all the bioregions and their interactions with dist. This confirms the results we obtained from the coefficients. We don’t need to overthink this result.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "multiple_linear_regression.html#example-3-the-final-model",
    "href": "multiple_linear_regression.html#example-3-the-final-model",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.8 Example 3: The Final Model",
    "text": "5.8 Example 3: The Final Model\nI’ll now expand mod1 to include bio as a predictor alongside augMean, febSD, and augSD (mod1 was applied only to data pertaining to ECTZ, one of the four levels in bio).\n\\begin{align}\nY &= \\alpha + \\beta_1 \\text{augMean} + \\beta_2 \\text{febSD} + \\beta_3 \\text{augSD} \\nonumber \\\\\n  &\\quad + \\beta_4 \\text{bio}_{\\text{B-ATZ}} + \\beta_5 \\text{bio}_{\\text{BMP}} + \\beta_6 \\text{bio}_{\\text{ECTZ}} \\nonumber \\\\\n  &\\quad + \\beta_7 (\\text{augMean} \\times \\text{bio}_{\\text{B-ATZ}}) + \\beta_8 (\\text{augMean} \\times \\text{bio}_{\\text{BMP}}) \\nonumber \\\\\n  &\\quad + \\beta_9 (\\text{augMean} \\times \\text{bio}_{\\text{ECTZ}}) + \\beta_{10} (\\text{febSD} \\times \\text{bio}_{\\text{B-ATZ}}) \\nonumber \\\\\n  &\\quad + \\beta_{11} (\\text{febSD} \\times \\text{bio}_{\\text{BMP}}) + \\beta_{12} (\\text{febSD} \\times \\text{bio}_{\\text{ECTZ}}) \\nonumber \\\\\n  &\\quad + \\beta_{13} (\\text{augSD} \\times \\text{bio}_{\\text{B-ATZ}}) + \\beta_{14} (\\text{augSD} \\times \\text{bio}_{\\text{BMP}}) \\nonumber \\\\\n  &\\quad + \\beta_{15} (\\text{augSD} \\times \\text{bio}_{\\text{ECTZ}}) + \\epsilon \\label{mod3}\n\\end{align}\nWhere:\n\n\nY: The response variable (mean Sørensen dissimilarity).\n\n\\alpha: The intercept term, representing the expected value of Y when all predictors are zero and bio is at the reference level AMP).\n\n\\beta_1: The coefficient for the main effect of augMean.\n\n\n\\beta_2: The coefficient for the main effect of febSD.\n\n\n\\beta_3: The coefficient for the main effect of augSD.\n\n\n\\beta_4, \\beta_5, \\beta_6: The coefficients for the main effects of the categorical predictor bio (for levels B-ATZ, BMP, and ECTZ respectively, with AMP as the reference category).\n\n\\beta_7, \\beta_8, \\beta_9: The coefficients for the interaction effects between augMean and bio (for levels B-ATZ, BMP, and ECTZ respectively).\n\n\\beta_{10}, \\beta_{11}, \\beta_{12}: The coefficients for the interaction effects between febSD and bio (for levels B-ATZ, BMP, and ECTZ respectively).\n\n\\beta_{13}, \\beta_{14}, \\beta_{15}: The coefficients for the interaction effects between augSD and bio (for levels B-ATZ, BMP, and ECTZ respectively).\n\n\\epsilon: The error term, representing the unexplained variability in the response variable.\n\nIn this multiple regression model, we aim to understand the complex and interacting relationships between the response variables and the set of predictors. It allows us to investigate not only the individual effects of the continuous predictors on Y, but also how these effects might vary across the different bioregions.\nThe model therefore incorporates interaction terms between each continuous predictor (augMean, febSD, and augSD) and the categorical variable bio. This allows us to assess whether the relationships between augMean, febSD, or augSD and Y change depending on the specific bioregion. Essentially, we are testing whether the slopes of these relationships are different in different bioregions.\nAdditionally, the model examines the main effects of the bioregions themselves on Y. This means we’re testing whether the average value of Y differs significantly across bioregions, after accounting for the influence of the continuous predictors.\nThis is how these different insights pertain to the model components:\n\nMain Effects: The coefficients for the main effects of augMean, febSD, and augSD represent the effect of each predictor when bio is at its reference level.\nCoefficients for bio: The coefficients for bio (e.g., \\beta_4 \\text{bio}_{\\text{B-ATZ}}) represent the difference in the intercept for the corresponding level of bio compared to the reference level.\nInteraction Terms: The interaction terms allow the slopes of augMean, febSD, and augSD to vary across the different levels of bio. For example, \\beta_7 (\\text{augMean} \\times \\text{bio}_{\\text{B-ATZ}}) represents how the effect of augMean on Y changes when bio is B-ATZ compared to AMP.\n\nState the Hypotheses\nOverall hypothesis\nI’ll only state the overall hypothesis for this model as the expansion of the individual hypotheses for each predictor and interactions (all the \\beta-coefficients in Equation ) is quite voluminous.\nThe null is that there is no relationship between the response variable Y and the predictors (including their interactions):\n\nH_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = \\beta_6 = \\beta_7 = \\beta_8 = \\beta_9 = \\beta_{10} = \\beta_{11} = \\beta_{12} = \\beta_{13} = \\beta_{14} = \\beta_{15} = 0\n\nThe alternative is that at least one predictor or interaction term has a significant relationship with the response variable Y:\n\nH_A: \\text{At least one } \\beta_i \\neq 0 \\text{ for } i \\in \\{1, 2, ..., 15\\}\nFit the Model\nIn Section 5.6 I included the ECTZ seaweed flora in my analysis, but here I expand it to the full dataset. To assure myself that there is not a high degree of multicollinearity between the predictors, I have calculated the variance inflation factors (VIFs) for the full model (not shown). This allowed me to retain the same three predictors used in mod1, i.e. augMean, febSD, and augSD. This is the point of departure for mod3.\nNow I fit the model with those three continuous predictors and their interactions with the categorical variable bio.\n\n# Make a dataframe with only the relevant columns\nsw_sub2 &lt;- sw |&gt;\n  dplyr::select(Y, augMean, febSD, augSD, bio)\n\n# Fit the multiple linear regression model with interaction terms\nfull_mod3 &lt;- lm(Y ~ (augMean + febSD + augSD) * bio, data = sw_sub2)\nfull_mod3a &lt;- lm(Y ~ augMean + febSD + augSD, data = sw_sub2)\nnull_mod3 &lt;- lm(Y ~ 1, data = sw_sub2)\n\nModel full_mod3a is similar to full_mod3 but without the interaction terms. This will allow me to compare the two models and assess the importance of the interactions.\n\n# Compare the models\nanova(full_mod3, full_mod3a)\n\nAnalysis of Variance Table\n\nModel 1: Y ~ (augMean + febSD + augSD) * bio\nModel 2: Y ~ augMean + febSD + augSD\n  Res.Df    RSS  Df Sum of Sq      F    Pr(&gt;F)    \n1    954 3.5603                                   \n2    966 5.6890 -12   -2.1288 47.535 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAIC(full_mod3, full_mod3a)\n\n           df       AIC\nfull_mod3  17 -2652.498\nfull_mod3a  5 -2221.852\n\n\nThe AIC value for full_mod3 is lower than that of full_mod3a, indicating that including the interaction with bio is necessary. Likewise, the ANOVA test also shows that the full model (lower residual sum of squares) is significantly better than the reduced model.\nI therefore use full_mod3 going forward. This is a complex model so I have used the stepwise selection function, stepAIC(), to identify the most important predictors and interactions (code and output not shown). I hoped that this might have simplified the model somewhat, but the simplification I had hoped for did not materialise.\nInterpret the Model\nThe model summary\nThe model summary provides a detailed look at the individual predictors and their interactions in the model.\n\n# Summary of the model\nsummary(mod3) # full_mod3 renamed to mod3 during stepAIC()\n\n\nCall:\nlm(formula = Y ~ augMean + bio + augSD + febSD + augMean:bio + \n    bio:augSD + bio:febSD, data = sw_sub2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.15399 -0.03841 -0.01475  0.03464  0.24051 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.0299094  0.0062756   4.766 2.17e-06 ***\naugMean           0.3441099  0.0158575  21.700  &lt; 2e-16 ***\nbioB-ATZ         -0.0459611  0.0242519  -1.895 0.058374 .  \nbioBMP            0.0160756  0.0100749   1.596 0.110906    \nbioECTZ          -0.0015444  0.0090275  -0.171 0.864197    \naugSD            -0.0059012  0.0034011  -1.735 0.083044 .  \nfebSD            -0.0006481  0.0027954  -0.232 0.816706    \naugMean:bioB-ATZ -0.0461775  0.0874044  -0.528 0.597400    \naugMean:bioBMP   -0.2406297  0.0211404 -11.382  &lt; 2e-16 ***\naugMean:bioECTZ  -0.0607745  0.0189030  -3.215 0.001348 ** \nbioB-ATZ:augSD    0.0655983  0.0371033   1.768 0.077382 .  \nbioBMP:augSD      0.0410220  0.0114706   3.576 0.000366 ***\nbioECTZ:augSD     0.0280513  0.0053752   5.219 2.21e-07 ***\nbioB-ATZ:febSD    0.0409425  0.0818927   0.500 0.617223    \nbioBMP:febSD      0.0056433  0.0150126   0.376 0.707070    \nbioECTZ:febSD     0.0502867  0.0082266   6.113 1.43e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06109 on 954 degrees of freedom\nMultiple R-squared:  0.7797,    Adjusted R-squared:  0.7762 \nF-statistic: 225.1 on 15 and 954 DF,  p-value: &lt; 2.2e-16\n\n\nThe first thing to notice is that the model function has been rewritten in the forward selection process (but none of the variables were deemed insignificant and removed):\n\nInitial specification: Y ~ (augMean + febSD + augSD) * bio\n\nSpecification after stepAIC(): Y ~ augMean + bio + augSD + febSD + augMean:bio + bio:augSD + bio:febSD\n\n\nFunctionally, these two are identical, but the order in which the terms are presented differs. Although this has affected the order in which the coefficients are presented in the summary output, the coefficients are the same. The coefficients are:\n\n\n(Intercept): This is the estimated average value of Y when all predictor variables are zero and the observation is in the reference bioregion (AMP).\nMain Effects:\n\n\naugMean: For every one-unit increase in augMean, Y increases by 0.3441, on average, assuming all other predictors are held constant. This effect is highly significant.\n\naugSD and febSD: The main effects of these variables are not statistically significant, suggesting they might not have a direct impact on Y when averaged across all bioregions.\n\nbioB-ATZ, bioBMP, bioECTZ: These coefficients represent the average difference in Y between each of these bioregions and the reference bioregion, when the continuous predictors are held at zero.\n\n\nInteraction Effects:\n\n\naugMean interactions: The significant interactions of augMean with bioregion indicate that the effect of augMean on Y varies across bioregions. Notably, the interaction with bioBMP has a strong, significant negative effect, suggesting that the positive effect of augMean is much weaker in this bioregion compared to the reference.\n\naugSD and febSD interactions: These interactions with bioregions are sometimes significant, providing good support for the alternative hypothesis that the effects of augSD and febSD on Y depend on the specific bioregion.\n\n\n\nSince dummy coding returns differences with respect to reference levels, how would we calculate the actual coefficients for, say, augMean? Since there are significant interaction effects, we must consider the main effect of augMean in conjunction with bioregion.\nFor bio = B-ATZ:\n\n\\beta_{\\text{augMean}} + \\beta_{\\text{augMean:bioB-ATZ}} = 0.3441099 + (-0.0461775) = 0.2979324\n\nFor bio = BMP:\n\n\\beta_{\\text{augMean}} + \\beta_{\\text{augMean:bioBMP}} = 0.3441099 + (-0.2406297) = 0.1034802\n\nFor bio = ECTZ:\n\\beta_{\\text{augMean}} + \\beta_{\\text{augMean:bioECTZ}} = 0.3441099 + (-0.0607745) = 0.2833354\nThe respective SEs for these coefficients can be calculated using the formula for the standard error of the sum of two variables. For example:\n\nSE_{\\text{augMean}} = \\sqrt{SE_{\\text{augMean}}^2 + SE_{\\text{augMean:bio}}^2}\n\nThe ANOVA table\nThe ANOVA table assesses the overall significance of groups of predictors or the sequential addition of predictors to the model.\n\nanova(mod3)\n\nAnalysis of Variance Table\n\nResponse: Y\n             Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \naugMean       1 9.9900  9.9900 2676.902 &lt; 2.2e-16 ***\nbio           3 1.1901  0.3967  106.296 &lt; 2.2e-16 ***\naugSD         1 0.1393  0.1393   37.331 1.451e-09 ***\nfebSD         1 0.0053  0.0053    1.422    0.2334    \naugMean:bio   3 0.7910  0.2637   70.647 &lt; 2.2e-16 ***\nbio:augSD     3 0.3426  0.1142   30.602 &lt; 2.2e-16 ***\nbio:febSD     3 0.1401  0.0467   12.517 4.953e-08 ***\nResiduals   954 3.5603  0.0037                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows that the model is highly significant, with very low p-values throughout (&lt; 0.0001). This indicates that the model as a whole is a good fit for the data.\nReporting\nHere is what the reporting of the findings could look like in the Results section in your favourite journal.\nResults\nA multiple linear regression model examining the effects of the August climatological mean temperature (augMean), the August and February climatological SD of temperature (augSD and febSD, respectively), and the bioregion classification (bio) on the response variable, the Sørensen dissimilarity (Y), including their interaction terms, revealed several significant findings (Table 5.1). This model allows a separate regression slope for each predictor within the bioregions (Figure 5.7). The model explains a substantial portion of the variance in Y (R^2 = 0.780, adjusted R^2 = 0.776), and the overall model fit is highly significant (F(15, 954) = 225.1, p &lt; 0.0001).\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nt value\nP-value\n\n\n\n(Intercept)\n0.0299\n0.0063\n4.766\n&lt; 0.0001 ***\n\n\naugMean\n0.3441\n0.0159\n21.700\n&lt; 0.0001 ***\n\n\nbioB-ATZ\n-0.0460\n0.0243\n-1.895\n&gt; 0.05\n\n\nbioBMP\n0.0161\n0.0101\n1.596\n&gt; 0.05\n\n\nbioECTZ\n-0.0015\n0.0090\n-0.171\n&gt; 0.05\n\n\naugSD\n-0.0059\n0.0034\n-1.735\n&gt; 0.05\n\n\nfebSD\n-0.0006\n0.0028\n-0.232\n&gt; 0.05\n\n\naugMean:bioB-ATZ\n-0.0462\n0.0874\n-0.528\n&gt; 0.05\n\n\naugMean:bioBMP\n-0.2406\n0.0211\n-11.382\n&lt; 0.0005 ***\n\n\naugMean:bioECTZ\n-0.0608\n0.0189\n-3.215\n&lt; 0.005 **\n\n\nbioB-ATZ:augSD\n0.0656\n0.0371\n1.768\n&gt; 0.05\n\n\nbioBMP:augSD\n0.0410\n0.0115\n3.576\n&lt; 0.0005 ***\n\n\nbioECTZ:augSD\n0.0281\n0.0054\n5.219\n&lt; 0.0005 ***\n\n\nbioB-ATZ:febSD\n0.0409\n0.0819\n0.500\n&gt; 0.05\n\n\nbioBMP:febSD\n0.0056\n0.0150\n0.376\n&gt; 0.05\n\n\nbioECTZ:febSD\n0.0503\n0.0082\n6.113\n&lt; 0.0005 ***\n\n\n\n\n\nTable 5.1: Summary of the multiple linear regression model examining the effects of augMean, augSD, febSD, and bio on Y.\n\n\nThe main effect of augMean was highly significant (Estimate = 0.3441, p &lt; 0.0001), indicating a strong positive relationship with Y. The interaction term augMean:bioBMP (Estimate = -0.2406, p &lt; 0.0001) and augMean:bioECTZ (Estimate = -0.0608, p &lt; 0.005) were also significant, suggesting that the effect of augMean on Y varies significantly for BMP and ECTZ bioregions compared to the reference category (AMP). The bioBMP (Estimate = 0.0161, p &gt; 0.05) and bioECTZ (Estimate = -0.0015, p &gt; 0.05) terms were not significant, indicating no significant difference from AMP.\nFor augSD, the main effect was not significant (Estimate = -0.0059, p &gt; 0.05). Significant interaction terms for bioBMP:augSD (Estimate = 0.0410, p &lt; 0.001) and bioECTZ:augSD (Estimate = 0.0281, p &lt; 0.0001) indicate that the effect of augSD on Y varies by bioregion.\nThe main effect of febSD was not significant (Estimate = -0.0006, p &gt; 0.05), suggesting no direct relationship with Y. However, the interaction term bioECTZ:febSD (Estimate = 0.0503, p = 0.0001) was significant, indicating that the effect of febSD on Y differs for the ECTZ bioregion.\nThe ANOVA further highlights the overall significance of each predictor. augMean had a highly significant contribution to the model (F = 2676.902, p &lt; 0.0001), as did bio (F = 106.296, p &lt; 0.0001), and their interactions (augMean:bio, F = 70.647, p &lt; 0.0001; bio:augSD, F = 30.602, p &lt; 0.0001; bio:febSD, F = 12.517, p = 4.953 \\times 10^{-8}). The main effect of augSD was also significant (F = 37.331, p = 1.451 \\times 10^{-9}), while febSD did not significantly contribute to the model on its own (F = 1.422, p = 0.2334).\nThese findings suggest that the effects of augMean, augSD, and febSD on Y are influenced by the bioregional classification, with significant variations in the relationships depending on the specific bioregion.\n\n\n\n\n\n\n\nFigure 5.7: Individual linear regression fit to the variables augMean, febSD, and augSD for each bioregion as predictors of the seaweed species composition.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "multiple_linear_regression.html#sec-contrasts",
    "href": "multiple_linear_regression.html#sec-contrasts",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.9 Alternative Categorical Variable Coding Schemes (Contrasts)",
    "text": "5.9 Alternative Categorical Variable Coding Schemes (Contrasts)\nThroughout the book, we have used dummy variable coding the specify the categorical variables in the multiple linear regression models. But, should dummy variable coding not be to your liking, there are other coding schemes that can be used to represent the categorical variables. These alternative coding schemes are known as contrasts. The choice of contrast coding can affect the interpretation of the regression coefficients.\nI’ll provide some synthetic data to illustrate a few different contrasts. The data consist of a continuous variable x, a categorical variable cat_var with four levels, and a response variable y that has some relationship with x and cat_var. I’ll use dummy variable coding as the reference (haha!).\n\nhead(data)\n\n           y           x cat_var\n1  0.6667876 -0.56047565       B\n2  1.3086873 -0.23017749       B\n3  0.4496192  1.55870831       D\n4  2.1326402  0.07050839       A\n5 -2.8608771  0.12928774       D\n6  0.1497346  1.71506499       D\n\n\nCategorical variable coding (any scheme) only affects the interpretation of the categorical variable main effects and their interactions, so I’ll not discuss the coefficient associated with the continuous variable x (the slope) in the model throughout the explanations offered below.\nDummy Variable Coding (Treatment Contrasts)\nThis is the most commonly used coding scheme, and lm()’s default. One level is the reference category (A) and the other levels are compared against it. Contrast matrices can be assigned and/or inspected using the contrasts() function. For the dummy coding, the reference level A will remain 0 and the other levels will be independently coded as 1 in three columns. You’ll now understand why, when we have four levels within a categorical variable, we only need three dummy variables to represent them.\n\n# Dummy coding (treatment coding) ... default\ncontrasts(data$cat_var)\n\n  B C D\nA 0 0 0\nB 1 0 0\nC 0 1 0\nD 0 0 1\n\n\nWhen we have four levels in a categorical variable, there are three dummy variable columns in the contrast matrix. The first row, consisting of all zeros (0, 0, 0), represents the reference level, which in this case is A. The other rows represent the different levels of the categorical variable, with a 1 in the respective column indicating that level. For example, level A is represented by (0, 0, 0), B by (1, 0, 0), C by (0, 1, 0), and D by (0, 0, 1). In the regression model, these contrasts are used to estimate the differences between each level and the reference level. Specifically, the first contrast column indicates that the coefficient for this column will represent the difference between the mean of the response variable for level B and the mean for the reference level A, holding all other variables constant. Similarly, the second and third columns represent the differences between levels C and A, and D and A, respectively. This coding allows for a straightforward interpretation of how each level of the categorical variable affects the response variable relative to the reference level.\n\nmodel_dummy &lt;- lm(y ~ x + cat_var, data = data)\nsummary(model_dummy)\n\n\nCall:\nlm(formula = y ~ x + cat_var, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6615 -0.6297 -0.1494  0.4978  2.9305 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.8176     0.1635  17.232  &lt; 2e-16 ***\nx             1.8274     0.1040  17.572  &lt; 2e-16 ***\ncat_varB     -1.7201     0.2499  -6.883 6.24e-10 ***\ncat_varC     -3.9056     0.2678 -14.586  &lt; 2e-16 ***\ncat_varD     -5.4880     0.2512 -21.850  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9246 on 95 degrees of freedom\nMultiple R-squared:  0.887, Adjusted R-squared:  0.8822 \nF-statistic: 186.4 on 4 and 95 DF,  p-value: &lt; 2.2e-16\n\n\nThe model summary shows that the coefficients for cat_varB, cat_varC, and cat_varD represent the differences in the mean of the response variable y between the reference category A and categories B, C, and D, respectively, while controlling for the effect of the continuous variable x.\nInterpretation:\n\n\n(Intercept) (2.8176): The intercept represents the estimated mean value of the response (y) when x is zero and the categorical variable is at the reference level A. This is the baseline from which other categories are compared.\n\nx (1.8274): For each one-unit increase in x, y is expected to increase by 1.8274 units, holding the categorical variable constant. This effect is consistent across all levels of the categorical variable because the model does not have an interaction effect present.\n\ncat_varB (-1.7201): On average, the value of y for level B is 1.7201 units lower than that for the reference level A, when x is held constant. This corresponds to the (1, 0, 0) row in the contrast matrix.\n\ncat_varC (-3.9056): Similarly, on average, the value of y for level C is 3.9056 units lower than that for the reference level, when x is held constant. This corresponds to the (0, 1, 0) row in the contrast matrix.\n\ncat_varD (-5.4880): Lastly, on average, the value of y for level D is 5.4880 units lower compared to the reference , when x is held constant. This is row (0, 0, 1) row in the contrast matrix.\n\nAll these coefficients are highly significant (p &lt; 0.0001), indicating strong evidence for differences between each category and the reference category A.\nThe model explains a large proportion of the variance in y (Adjusted R-squared: 0.8822), suggesting a good fit. The F-statistic (186.4) with a very low p-value (&lt; 0.0001) indicates that the model as a whole is statistically significant.\nIf you want to change the reference level, you can use the relevel() function. For example, to change the reference level of cat_var variable to C_2, you can use:\n\n# Set \"C\" as the reference level for cat_var\ndata$cat_var &lt;- relevel(data$cat_var, ref = \"C\")\ncontrasts(data$cat_var)\n\n  A B D\nC 0 0 0\nA 1 0 0\nB 0 1 0\nD 0 0 1\n\n\nThis may be useful when you want to compare the other levels to a different reference level.\nEffect Coding (Sum Contrasts)\nThis coding method compares the levels of a categorical variable to the overall mean of the dependent variable. The coefficients represent the difference between each level and the grand mean. Instead of using 0 and 1 as we did with dummy variable coding, effect coding uses -1, 0, and 1 to represent the different levels of the categorical variable.\n\n# Reset the reference level to \"A\"\ndata &lt;- data.frame(y, x, cat_var)\n\n# Effect coding\ncontrasts(data$cat_var) &lt;- contr.sum(4)\ncontrasts(data$cat_var)\n\n  [,1] [,2] [,3]\nA    1    0    0\nB    0    1    0\nC    0    0    1\nD   -1   -1   -1\n\n\nIn effect coding (sum contrasts), each level of the categorical variable is compared to the overall mean rather than a specific reference category. This contrast matrix with four levels (A, B, C, D) and three columns can be interpreted as follows:\n\nLevel A (1, 0, 0): The first row indicates that level A is included in the first contrast (cat_var1), which means the mean of level A is being compared to the overall mean. Since the other columns are zero, level A does not contribute to the other contrasts.\nLevel B (0, 1, 0): The second row indicates that level B is included in the second contrast (cat_var2). The mean of level B is being compared to the overall mean, and it does not contribute to the other contrasts.\nLevel C (0, 0, 1): The third row indicates that level C is included in the third contrast (cat_var3). The mean of level C is being compared to the overall mean, and it does not contribute to the other contrasts.\nLevel D (-1, -1, -1): The fourth row is a balancing row, ensuring that the sum of the contrasts for each level equals zero. This indicates that level D is being compared to the overall mean indirectly by balancing the contributions of levels A, B, and C.\n\n\nmodel_effect &lt;- lm(y ~ x + cat_var, data = data)\nsummary(model_effect)\n\n\nCall:\nlm(formula = y ~ x + cat_var, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6615 -0.6297 -0.1494  0.4978  2.9305 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.03921    0.09452   0.415    0.679    \nx            1.82741    0.10400  17.572  &lt; 2e-16 ***\ncat_var1     2.77844    0.14968  18.563  &lt; 2e-16 ***\ncat_var2     1.05832    0.16329   6.481 4.04e-09 ***\ncat_var3    -1.12720    0.17765  -6.345 7.53e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9246 on 95 degrees of freedom\nMultiple R-squared:  0.887, Adjusted R-squared:  0.8822 \nF-statistic: 186.4 on 4 and 95 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation:\n\n\n(Intercept) 0.03921: The intercept represents the grand mean of the response variable (y). Since the intercept is not statistically significant (p &gt; 0.05), it indicates that the overall mean is not significantly different from zero when considering the average effect of all levels of the categorical variable.\n\nx (1.82741): For each one-unit increase in (x), the response (y) increases by approximately 1.82741 units. This effect is highly significant (p &lt; 0.0001).\n\ncat_var1 (2.77844): Level A has a mean (y) that is 2.77844 units higher than the grand mean. This effect is highly significant (p &lt; 0.0001).\n\ncat_var2 (1.05832): Level B has a mean (y) that is 1.05832 units higher than the grand mean. This effect is also highly significant (p &lt; 0.0001).\n\ncat_var3 (-1.12720): Level C has a mean (y) that is 1.12720 units lower than the grand mean. This effect is highly significant (p &lt; 0.0001).\n\nAll these coefficients are highly significant (p &lt; 0.0001), indicating strong evidence for differences between each category and the overall mean of all levels.\nThe model explains a large proportion of the variance in y (Adjusted R-squared: 0.8822), suggesting a good fit. The F-statistic (186.4) with a very low p-value (&lt; 0.0001) indicates that the model as a whole is statistically significant.\nHelmert Coding\nHelmert coding compares each level of a categorical variable to the mean of the subsequent levels. It is useful for testing ordered differences.\n\n# Helmert coding\ncontrasts(data$cat_var) &lt;- contr.helmert(4)\ncontrasts(data$cat_var)\n\n  [,1] [,2] [,3]\nA   -1   -1   -1\nB    1   -1   -1\nC    0    2   -1\nD    0    0    3\n\n\nThe contrast matrix for a categorical variable with four levels (A, B, C, D) and three columns can be interpreted as follows:\n\nLevel A (-1, -1, -1): Level A is compared to the mean of levels B, C, and D. The negative values indicate that level A is being subtracted in these comparisons.\nLevel B (1, -1, -1): Level B is compared to the mean of levels C and D. The positive value in the first column indicates that level B is being added in this comparison.\nLevel C (0, 2, -1): Level C is compared to the mean of level D. The positive value in the second column indicates that level C is being added in this comparison, while the negative value in the third column is part of the comparison for subsequent levels.\nLevel D (0, 0, 3): Level D is compared on its own in the final contrast. The positive value in the third column indicates that level D is being added in this comparison.\n\n\nmodel_helmert &lt;- lm(y ~ x + cat_var, data = data)\nsummary(model_helmert)\n\n\nCall:\nlm(formula = y ~ x + cat_var, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6615 -0.6297 -0.1494  0.4978  2.9305 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.03921    0.09452   0.415    0.679    \nx            1.82741    0.10400  17.572  &lt; 2e-16 ***\ncat_var1    -0.86006    0.12495  -6.883 6.24e-10 ***\ncat_var2    -1.01519    0.08206 -12.371  &lt; 2e-16 ***\ncat_var3    -0.90319    0.05477 -16.491  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9246 on 95 degrees of freedom\nMultiple R-squared:  0.887, Adjusted R-squared:  0.8822 \nF-statistic: 186.4 on 4 and 95 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation:\n\n\n(Intercept) (0.03921): The grand mean of y when x is zero.\n\nx (1.82741): For each unit increase in x , y increases by 1.82741 units.\n\ncat_var1 (-0.86006): The mean of level A is 0.86006 units lower than the combined mean of levels B, C, and D.\n\ncat_var2 (-1.01519): The mean of level B is 1.01519 units lower than the combined mean of levels C and D.\n\ncat_var3 (-0.90319): The mean of level C is 0.90319 units lower than the mean of level D.\n\nThe interpretation of the overall model remains more-or-less similar to before:\nAll these coefficients are highly significant (p &lt; 0.0001), indicating strong evidence for differences between each level and the overall mean of all subsequent levels.\nThe model explains a large proportion of the variance in y (Adjusted R-squared: 0.8822), suggesting a good fit. The F-statistic (186.4) with a very low p-value (&lt; 0.0001) indicates that the model as a whole is statistically significant.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "multiple_linear_regression.html#exercises",
    "href": "multiple_linear_regression.html#exercises",
    "title": "5  Multiple Linear Regression",
    "section": "\n5.10 Exercises",
    "text": "5.10 Exercises\n\n\n\n\n\n\nTask G\n\n\n\nUse the data loaded at the start of this chapter for this task.\nIn this task you will develop data analysis, undertake model building, and provide an interpretation of the findings. Your goal is to explore the species composition and assembly processes of the seaweed flora around the coast of South Africa. See Smit et al. (2017) for more information about the data and the analysis.\n\n\nAnalysis: Please develop multiple linear regression models for the seaweed species composition (\\beta_\\text{sim} and \\beta_\\text{sne}, i.e. columns called Y1 and Y2, respectively) using the all the predictors in this dataset. At the end, the final model(s) that best describe(s) the species assembly processes operating along the South African coast should be presented. The final model may/may not contain all the predictors in the dataset, and it is your goal to justify the variable and model selection.\n\nAccomplishing a) will require that you work through the whole model-building process as outlined in the chapter. This includes the following steps:\n\nData exploration and visualisation (EDA)\nModel building (providing hypothesis statements, variable selection using VIF and forward selection, comparisons of nested models, justifications for model selection)\nModel diagnostics\nExplanation of summary() and anova() outputs\nProducing the Results section\n[60%]\n\n\n\n\n\nInterpretation: Once you have arrived at the best model, discuss your findings in the light of the appropriate ecological hypotheses that explain the relationships between the predictors and the seaweed species composition. Include insights drawn from the analysis of \\beta_\\text{sør} that I developed in this chapter, and also rely on the theory you have developed for the lecture material the class presented in Task A2.\n\nAccomplishing b) is thus all about model interpretation and discussing the ecological relevance of the results.\n[40%]\n\n\n\nThe format of this task is a Quarto file that will be converted to an HTML file. The HTML file will contain the graphs, all calculations, and the text sections. The task should be written up as a publication (i.e. use appropriate headings) using a journal style of your choice. Aside from this, there are no limitations.\n\n\n\n\n\n\n\n\nGraham MH (2003) Confronting multicollinearity in ecological multiple regression. Ecology 84:2809–2815.\n\n\nSmit AJ, Bolton JJ, Anderson RJ (2017) Seaweeds in two oceans: Beta-diversity. Frontiers in Marine Science 4:404.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "generalised_linear_models.html",
    "href": "generalised_linear_models.html",
    "title": "6  Generalised Linear Models (GLM)",
    "section": "",
    "text": "6.1 Logistic Regression\nA logistic regression model is used when the dependent variable is binary (e.g., 0 or 1, yes or no). The logistic regression model is expressed as:\n\\log\\left(\\frac{p}{1-p}\\right) = \\alpha + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_k X_{ik} \\tag{6.1}\nWhere:",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalised Linear Models (GLM)</span>"
    ]
  },
  {
    "objectID": "generalised_linear_models.html#logistic-regression",
    "href": "generalised_linear_models.html#logistic-regression",
    "title": "6  Generalised Linear Models (GLM)",
    "section": "",
    "text": "p is the probability of the dependent variable being 1,\nX_{i1}, X_{i2}, \\ldots, X_{ik} are the k predictor variables for the i-th observation,\n\\alpha is the intercept,\n\\beta_1, \\beta_2, \\ldots, \\beta_k are the coefficients for the k predictor variables.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalised Linear Models (GLM)</span>"
    ]
  },
  {
    "objectID": "non-linear_regression.html",
    "href": "non-linear_regression.html",
    "title": "7  Nonlinear Models",
    "section": "",
    "text": "7.1 Extension of Nonlinear Models\nLike linear models, nonlinear models have also been extended to include multiple predictors, interactions, and other terms to capture complex relationships between the variables. The first type of more complex nonlinear models accommodates a wider range of data distributions by generalising to non-normal error distributions through link functions. These models are called generalised nonlinear models (GNLMs). The examples of GLMs in Chapter 6 should prepare you sufficiently to handle nonlinear models too. The other type deals with hierarchical data structures and incorporates fixed and random effects. As such, you can also correctly model repeated measures and longitudinal, and nested (grouped) designs. These hierarchical models are called nonlinear mixed models (NLMMs). Examples of NLMMs are provided in Section 7.5.2.3 and Section 7.5.3.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonlinear Models</span>"
    ]
  },
  {
    "objectID": "non-linear_regression.html#considerations-for-model-selection",
    "href": "non-linear_regression.html#considerations-for-model-selection",
    "title": "7  Nonlinear Models",
    "section": "7.2 Considerations for Model Selection",
    "text": "7.2 Considerations for Model Selection\n\n\n\n\n\n\n\n\nFigure 7.2: Plot of growth rate data fitted with a von Bertalanffy model, a first- (straight line), second- and third-order polynomial, and a GAM.\n\n\n\n\n\nThere are a few practical considerations to keep in mind when choosing a suitable nonlinear (in shape) model. Sometimes different models can provide similar fits to the same data, but they may have different implications for the interpretation of the relationship between the variables. See for example Figure 7.2. The plot shows growth rate data fitted with a first-, second- and third-order polynomial, a GAM, and a NLS von Bertalanffy model. To the untrained eye and inexperienced biologist, all models seem to provide a good fit to the data, but they do differ subtly in the shape of the fitted curve. The von Bertalanffy model is a saturating growth model (it reaches a plateau), while the polynomial models and the GAM are more flexible and can capture a wider range of shapes. The choice of model should be guided by the underlying biological or physical processes that generated the data and the research question you are trying to answer.\nSince you will often have to decide among polynomial regressions, nonlinear models, and GAMs, I’ll outline some general guidelines to help you make an informed decision.\n\nLinearity vs. Nonlinearity: If the relationship between the variables is linear or can be adequately approximated by a polynomial function, polynomial regression is a suitable choice. Nonlinear models or GAMs may be more appropriate if the relationship is nonlinear and does not follow a specific polynomial form. In Figure 7.2, it is obvious that the straight line model is not a good fit for the data, but the second- and third-order polynomial models, the GAM, and the von Bertalanffy model all provide better fits.\nComplexity of the Relationship: Polynomial regression is limited in its ability to capture complex nonlinear relationships, especially those with more bends, peaks, or valleys than a polynomial of order &lt;3 (or even 4 at a push) can capture. Another consideration is the process the data represent: if it is inherently nonlinear according to a known function such as exponential growth or decay, seasonal sinusoidal patterns, or logistic growth, then nonlinear models or GAMs are more flexible and can capture a wider range of nonlinear responses. In Figure 7.2, the von Bertalanffy model is a saturating growth model, which is a known biological process that can be captured by a nonlinear model. The 3rd-order polynomial model also seems to capture a saturating growth pattern, but it also somewhat influenced by the dip in the raw data around 12.5 years (in addition to some other nuances), but this is likely due to some random variation and is not part of the growth response.\nInterpretability vs. Flexibility: Polynomial regression provides coefficients that relate to the powers of the predictor variables, but the interpretation of the \\beta parameters is not as intuitive as in a linear model of order 1. In contrast, nonlinear models and GAMs offer greater flexibility in capturing complex patterns. GAMs may lack direct interpretability of the coefficients, but the nonlinear model offers coefficients that can be interpreted in the context of the model’s structure. In Figure 7.2, the von Bertalanffy model has a clear biological interpretation (see Section 7.6), while the 3rd-order polynomial model and the GAM are more flexible and can capture a wider range of shapes (it follows the dips and peaks in the raw data closer). The 2nd-order polynomial does not fit the data as well at very low ages at 20 year, but it is still a better fit than the linear model.\nOverfitting Concerns: Polynomial regression with high-degree polynomials can lead to overfitting, especially when the model complexity exceeds the underlying data patterns. Nonlinear models and GAMs can also overfit if not properly regularised or constrained. These insights can be seen when we examine the summaries of the regression fits, and can be formally assessed using cross-validation or information criteria. In Figure 7.2, the 3rd-order polynomial model seems to capture some of the random variation in the data, which may be an indication of overfitting. The GAM also seems to capture some of the random variation, but it is less pronounced than in the 3rd-order polynomial model.\nData Size and Complexity: For small to moderate-sized datasets with complex nonlinear relationships, GAMs may be more suitable due to their flexibility and ability to capture intricate patterns. For simpler relationships or when interpretability is important, nonlinear regression (with mechanistically-informed parameters) may be preferred. These are not of concern in Figure 7.2.\nModel Complexity and Assumptions: Polynomial regression assumes a specific polynomial form for the relationship, which may not hold in practice. Nonlinear models and GAMs are more flexible and do not always impose strict parametric assumptions (see Section 7.3), making them more robust to deviations from the assumed form. A detailed assessment of the model assumptions and the complexity of the relationship can help guide the choice of model. We need to add to this our biologist specialist knowledge to make the best choice.\nComputational Considerations: Polynomial regression is relatively simple to implement and computationally efficient, especially for low-degree polynomials. Nonlinear models and GAMs may require more computational resources, especially for large datasets or complex models. Not a concern for the models represented in Figure 7.2.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonlinear Models</span>"
    ]
  },
  {
    "objectID": "non-linear_regression.html#sec-assumptions",
    "href": "non-linear_regression.html#sec-assumptions",
    "title": "7  Nonlinear Models",
    "section": "7.3 Requirements and Assumptions",
    "text": "7.3 Requirements and Assumptions\nPolynomial regression, nonlinear regression, and GAMs are built upon the principles of linear regression; therefore, the fundamental assumptions of normality and homoscedasticity of residuals usually still apply. Specifically, these models assume that the residuals are independent and identically distributed (i.i.d.), which implies that they are normally distributed with a constant variance (homoscedasticity). However, the specifics can vary depending on the model and the distribution of the response variable. Of course, there is also the requirement for the response variable to be continuous and independent. These assumptions help ensure that the error terms (residuals) in the model are well-behaved so that reliable inference and predictions can be obtained.\nNuances:\n\nPolynomial Regression: While a type of nonlinear regression, polynomial models are still linear in their parameters. This means that they are more bound to the classic regression assumptions and can be more sensitive to violations.\nGAMs: Offer more flexibility in handling nonlinear relationships. Depending on the distributions used for the outcome variable and the link functions employed, GAMs can potentially relax some of the strict normality assumptions.\nNonlinear Models in General: Some truly nonlinear models (like those based on exponential or logarithmic functions) may have inherently different error structures and may not strictly require the same assumptions of normality and homoscedasticity. However, these models come with their own set of assumptions and considerations.\n\nImportant considerations:\n\nDiagnostic Checks: Regardless of the model type, it’s essential to perform residual diagnostics to assess if assumptions are met. Visualisations (e.g., histograms, Q-Q plots, residuals vs. fitted plots) are well-known tools.\nTransformations: If violations of assumptions are found, data transformation techniques (e.g., Box-Cox, log) could be considered to improve model validity.\nGeneralised Linear Models (GLMs): An important class of models designed to handle various non-normal responses (e.g., count, binary) while extending the linear modeling framework. GLMs are good alternative to both polynomial regression and GAMs in certain contexts.\nMixed models: Linear Mixed Models (LLMs), Generalised Linear Mixed Models (GLMMs), and Generalised Nonlinear Models (GNLMs) can be used to account for dependencies in the data, such as repeated measures or hierarchical structures. GAMs also accommodate mixed data structures.\n\nThe rest of this chapter will focus on the practical aspects of fitting polynomial regression models and nonlinear regressions in R. GAMs will be covered in a separate chapter due to their unique characteristics and implementation details.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonlinear Models</span>"
    ]
  },
  {
    "objectID": "non-linear_regression.html#r-functions-and-packages",
    "href": "non-linear_regression.html#r-functions-and-packages",
    "title": "7  Nonlinear Models",
    "section": "7.4 R Functions and Packages",
    "text": "7.4 R Functions and Packages\n\nPolynomial Regression\nTo fit a polynomial model in R, use the simple linear regression function lm() to fit the model. The purpose of poly() is to generate polynomial terms of a specified degree. The basic form is:\n\npoly_model &lt;- lm(y ~ poly(x, degree = 2), data = data)\n\nGLMs are a generalisation of ordinary linear regression that allows for the response variable to have non-Gaussian error distributions such as one of the exponential family distributions (e.g., binomial, Poisson, gamma). These distributions are accommodated via so-called link functions within the GLM framework. The most common R function for fitting GLMs is glm().\nMixed models that include random and fixed effects (see box ‘Fixed and Random Effects’) are also available. These are necessary for the analysis of data that have correlations within groups or hierarchies (e.g., repeated measures1 or the inclusion of grouped variables). Commonly used are lmer() for LLMs and glmer() for GLMMs. Both functions are in the lme4 package. Another package that accommodates LLMs is nlme and its lme() function. It has somewhat different capabilities and syntax compared to lme4.\n1 Repeated measures are multiple observations taken on the same subject or unit over time or under different conditions. Sometimes this is called longitudinal data.\n\n\n\n\n\nFixed and Random Effects\n\n\n\nRandom effects and fixed effects are used in regression models to account for different sources of variation in the data.\nFixed effects are variables or factors that represent sources of variation that are of primary interest in the study or that have a finite and fixed number of levels or categories. These effects are assumed to have an influence on the mean response. Examples of fixed effects include:\n\nTreatment groups in an experiment (e.g., fertiliser A, fertiliser B, control)\nCategorical variables (e.g., sex, age group, species)\nContinuous variables (e.g., time, temperature, concentration)\n\nThe coefficients associated with fixed effects are estimated and interpreted as the primary effects of interest in the model.\nRandom effects are variables or factors that represent sources of variation that are not of primary interest but need to be accounted for in the model. These effects are assumed to be randomly sampled from a larger population, and their levels are theoretically infinite or too numerous to be modeled as fixed effects. Examples of random effects include:\n\nSubjects or individuals in a study (e.g., individual plants or animals)\nClusters or groups (e.g., plots, aquaria, transects)\nRepeated measures or time points within subjects\n\nRandom effects are used to model the correlation or dependence among observations within the same cluster, subject, or time series. They allow for subject-specific or cluster-specific adjustments to the overall model, accounting for the fact that observations within the same group are more similar than observations from different groups.\nIn LMMs and GLMMs, both fixed and random effects are included. The fixed effects represent the primary effects of interest and the random effects account for the correlation or dependence within clusters or subjects.\n\n\n\n\nNonlinear Regression\nIn R, nonlinear regressions can be performed using the nls() function in the base package. It uses iterative algorithms to minimise the residual sum of squares and find the best-fit parameters for the user-specified nonlinear model.\nThe nls() function is most frequently used to fit user-specified nonlinear functions. The basic syntax is:\n\nnls_model &lt;- nls(y ~ f(x, theta1, theta2, ...), data = data,\n                 start = list(theta1 = value1, theta2 = value2, ...))\n\nGNLMs extend nonlinear models by allowing the response variable to follow one of the exponential family distributions, such as binomial, Poisson, or gamma, etc. This is done through a link function that relates the mean of the distribution to the predictors through the nonlinear model. GNLMs are fit using maximum likelihood estimation, which is flexible enough to handle various types of error distribution and link functions. The gnm package rovides the gnm() function designed for this purpose.\nFor data with dependencies within groups or hierarchies (such as in longitudinal studies), NLMMs are available within nlme(). NLMMs incorporate fixed effects (associated with the nonlinear terms) and random effects (to account for correlation and variation within groups).",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonlinear Models</span>"
    ]
  },
  {
    "objectID": "non-linear_regression.html#sec-ex1",
    "href": "non-linear_regression.html#sec-ex1",
    "title": "7  Nonlinear Models",
    "section": "7.5 Example: Algal Nutrient Uptake Kinetcis",
    "text": "7.5 Example: Algal Nutrient Uptake Kinetcis\nWe can measure algal nutrient uptake rates using two types of experiments: multiple flask experiments and perturbation experiments. The fundamental concept underlying both methods is to introduce a known quantity of nutrients (termed the substrate) into a flask or a series of flasks and then measure the rate of nutrient uptake (V) at different substrate concentrations ([S]). We calculate the nutrient uptake rate as the change in nutrient concentration in the flask over a predefined time interval (V = \\Delta [S]/\\Delta t). Consequently, both experiments generate data that relate the nutrient uptake rate to the corresponding substrate concentration. The primary difference between the two methods lies in the experimental setup and the data analysis.\nIn the multiple flask method, we prepare a series of flasks, each containing a different initial concentration of the substrate nutrient to span the range typically encountered by the specimen in its natural environment. We then measure the nutrient uptake rate in each individual flask over a specific time period, for example by taking measurements at the start (t=0) and end (t=30 minutes) of the incubation. We calculate the change in substrate concentration over this time interval in each flask to determine the corresponding nutrient uptake rate. The resulting data from this method therefore consists of the different initial substrate concentrations used in each flask, paired with their respective measured nutrient uptake rates over the incubation period.\n\n\n\n\n\n\n\nThe perturbation method uses a single flask to which we add a high initial concentration of the substrate nutrient, set at a level that is ecologically meaningful and relevant to the study system. Instead of using multiple flasks, we measure the change in the remaining substrate concentration at multiple time points within this same flask, for example by taking samples every 10 or 20 minutes until all the substrate is depleted, say at 120 minutes. We calculate the change in substrate concentration between each successive time point to determine the corresponding nutrient uptake rate over that time interval. The resulting data, therefore, consist of a time series of substrate concentrations at each measurement time point, paired with the nutrient uptake rates calculated over the periods between those time points.\nThe important differences between the multiple flask and perturbation experiments are summarised in Table 7.1.\n\n\n\n\n\n\n\n\n\n\n\nFeature\nMultiple Flask Experiments\nPerturbation Experiments\n\n\n\n\nExperimental Setup\nMultiple flasks, each with different [S]\nSingle flask with initial high [S]\n\n\nData Independence\nData points are independent\nData points are correlated (repeated measures)\n\n\nAnalysis\nNonlinear least squares regression (NLS)\nNonlinear mixed model (NLMM)\n\n\nR Function\nnls()\nnlme::nlme()\n\n\n\n\n\nTable 7.1: Key differences between multiple flask and perturbation experiments.\n\n\n\nOur choice between multiple flask and perturbation experiments depends on our research questions and experimental constraints. In both methods, we must consider all sources of error and variability, such as measurement error, the type of nutrient, the physiological state of the alga, the light intensity, the experimental temperature, and other variables that might affect the uptake response.\nWe apply the Michaelis-Menten model (Equation 7.3) to data from multiple flask and perturbation experiments to characterise nutrient uptake. Applied to algae, this model assumes an irreversible uptake process that saturates at high substrate concentrations. It effectively quantifies key characteristics of the nutrient uptake system, including the maximum uptake rate and the algae’s affinity for the nutrient.\nWe use the nls() function to fit the Michaelis-Menten model to the data from multiple flask experiments. For the perturbation experiment, things are a bit more complicated. This method includes dependent data points because the measurements are taken from the same flask at different times, introducing a correlation between observations. This violates the independence assumption required for standard regression models. To accurately analyse these data, I recommend a nonlinear mixed-effects model implemented in the nlme() function. Mixed-effects models account for fixed effects (overall trends across all observations) and random effects (variations specific to individual experimental units, in this case, time points within the same flask). This helps handle the correlation between repeated measures and produces reliable estimates of the uptake dynamics within the flask.\nThe Michaelis-Menten equation is given by:\nV_i = \\frac{V_{max} \\cdot [S_i]}{K_m + [S_i]} + \\epsilon_i \\tag{7.3}\nWhere:\n\nV_i is the uptake rate at the i-th observation,\nV_{max} is the maximum nutrient uptake rate achieved,\n[S_i] is the substrate concentration at the i-th observation,\nK_m is the Michaelis constant, which represents the substrate concentration at which the uptake rate is half of V_{max}, and\n\\epsilon_i is the error term at the i-th observation. and\n\nThe two parameters of the Michaelis-Menten model are rooted in theory and have ecophysiological interpretations. K_m is a measure of the alga’s affinity for the nutrient and is determined by the kinetic constants governing the formation and dissociation of the enzyme-substrate complex responsible for taking up the nutrient; lower values indicate a higher affinity. V_{max} represents the maximum capacity of the alga to utilise the nutrient.\n\nHypothesis Testing and the Michaelis-Menten Model\n\nLinear vs. Michaelis-Menten Model\nOften, we aim to understand the relationship between two variables but we may not yet know which model best describes this relationship. For instance, in algal nutrient uptake kinetics, both a linear model and a nonlinear Michaelis-Menten model can be used to describe the relationship between nutrient uptake rate and substrate concentration. Both models are valid but they have different interpretations and unique ecophysiological implications. The choice between the two models depends on the biological system.\n\nLinear models indicate that the uptake process is inherently unsaturated, such as with the uptake of ammonium. In this case, the uptake rate continues to increase linearly with substrate concentration.\nThe Michaelis-Menten model suggests that the uptake rate eventually saturates as the substrate concentration increases, which is often the case with nitrate.\n\nThe key question is: How do we decide which model fits our data best?\nThe simplest way is to visually inspect the scatter of points on a plot of the V vs. [S] data, which would be part of any exploratory data analysis. If the data exhibit a clear saturation pattern, where the uptake rate levels off at high substrate concentrations, the Michaelis-Menten model is likely to provide a better fit. Conversely, if the data show a linear relationship over the observed range of substrate concentrations, the linear model may be more appropriate.\nIt is also important to consider the biological plausibility of the models. If there is prior knowledge or theoretical reasons to expect a saturating relationship between the uptake rate and substrate concentration, the Michaelis-Menten model may be more appropriate, even if both models provide a similar fit to the data.\nConfirmation can be obtained by fitting both models to our data and comparing their performance using statistical measures such as the sum of squared residuals (SSR), Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), or log-likelihood test.\nTo proceed with the statistical approach, we must first set hypotheses such as these to compare the models:\nH_0: The Michaelis-Menten model does not provide a better fit to the data than a simple linear model.\nIn other words, we suggest with the null hypothesis that the relationship between nutrient uptake rate and the substrate concentration is adequately described by a linear model rather than the Michaelis-Menten nonlinear model. The implication is that the uptake rate increases linearly with substrate concentration, without saturation.\nH_a: The Michaelis-Menten model provides a significantly better fit to the data than a simple linear model.\nWith the alternative hypothesis we propose that the relationship between the nutrient uptake rate and the substrate concentration is best described by the nonlinear Michaelis-Menten model, so the uptake rate initially increases with substrate concentration but eventually levels off, indicating saturation.\nTo test these hypotheses, we can:\n\nFit both the Michaelis-Menten model and a linear model to the data.\nCompare the goodness-of-fit of both models using statistical measures such as the SSR, AIC, or BIC.\nPerform a model comparison test (such as an F-test or likelihood ratio test) to determine if the improvement in fit provided by the Michaelis-Menten model is statistically significant compared to the linear model.\n\nIn the above scenario, which is to decide among the linear and Michaelis-Menten models, hypotheses concerning the parameters of the models are not directly tested as they are not really of interest (except for estimating their magnitude, perhaps). Instead, the focus is on the overall goodness-of-fit of the models to the data.\n\n\nComparing Two Michaelis-Menten Models\nHere, we may be interested in testing whether the parameters V_\\text{max} and K_m differ from some hypothesised values or across different experimental conditions.\nIn the first instance, we can set up the hypotheses as follows:\nH_0: V_\\text{max} = V_\\text{max}^* and K_m = K_m^*\nwhere V_\\text{max}^* and K_m^* are the hypothesised values (or values from a reference condition) for the maximum uptake rate and Michaelis constant, respectively.\nH_a: V_\\text{max} \\neq V_\\text{max}^* or K_m \\neq K_m^*\nThis alternative hypothesis states that at least one of the parameters (V_\\text{max} or K_m) differs from the hypothesised value.\nIf the experiment involves different experimental conditions or treatments, we can modify the hypotheses accordingly. For example, if we want to test whether the parameters differ between two experimental conditions (A and B), the hypotheses could be:\nH_0: V_\\text{max}^A = V_\\text{max}^B and K_m^A = K_m^B\nH_a: V_\\text{max}^A \\neq V_\\text{max}^B or K_m^A \\neq K_m^B\nIn this case, the null hypothesis states that the maximum uptake rate and Michaelis constant are the same for both experimental conditions, while the alternative hypothesis states that at least one of the parameters differs between the two conditions.\nAfter fitting the Michaelis-Menten model to the data using the nls() or nlme() functions in R, appropriate statistical tests (e.g., likelihood ratio tests, Wald tests, or other model comparison techniques) can be performed to evaluate the hypotheses and determine whether the parameter estimates significantly differ from the hypothesised values or across experimental conditions.\n\n\n\nMultiple Flask Experiment\n\nFitting a single model (NLS)\nTo demonstrate fitting a nonlinear model to V vs [S] data produced from a multiple flask experiment, I simulate data across a range of substrate concentrations. We then fit the model to the data using the nls() function in R. The dataset consists of five replicate flask sets (n=5) for each of 13 substrate concentrations. Each set therefore results in independently estimated uptake rates for the initial nutrient concentrations. The dataset is shown in Table 7.2, and a plot of V as a function of [S] is shown in Figure 7.3.\n\n\n\n\n\n\n\n\n\n\n\nReplicate flask\n[S]\nV\n\n\n\n\n1\n0\n0.00\n\n\n2\n0\n0.00\n\n\n3\n0\n0.00\n\n\n3\n30\n37.64\n\n\n4\n30\n37.97\n\n\n5\n30\n35.95\n\n\n\n\n\n\n\n\nTable 7.2: Simulated data for a multiple flask experiment on an alga (showing only the top and bottom three rows).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.3: Plot of V as a function of [S] for a multiple flask experiment involving seven replicate flask sets.\n\n\n\n\n\nIn Figure 7.3, there is a clear indication that the uptake rates plateau at higher substrate concentrations, suggesting that fitting a Michaelis-Menten model is advisable. Later, I will compare this with a linear model for completeness. A central feature of this dataset is that the data were collected independently, with each flask set representing a separate experimental unit. There is no correlation between flasks within a set, and no correlation across the initial substrate concentrations. Consequently, the assumption of independence is fully met, allowing the simplest expression of the nls() function to be used to fit the Michaelis-Menten model to the data.\nThe Michaelis-Menten model is fit to the data using the nls() function in R. It is specified as:\n\n# Define the model function\nmm_fun &lt;- function(S, Vmax, Km) {\n  Vmax * S / (Km + S)\n}\n\n# Fit the nonlinear model Michaelis-Menten model\n1nls_mod &lt;- nls(V ~ mm_fun(S, Vmax, Km),\n            data = mf_data,\n2            start = c(Vmax = 30, Km = 5))\n\n\n1\n\nThe model formula specifies the Michaelis-Menten equation, with V as the dependent variable on the left-hand side and S as the independent variable on the right. The model parameters Vmax and Km will be estimated when fitting the model.\n\n2\n\nThe start argument provides initial values for the model parameters. The Vmax and Km parameters are estimated by minimising the sum of squared residuals between the observed and predicted values of V. The nls() function uses an iterative process to find the best-fitting values for these parameters, and the starting values improve the success of model convergence.\n\n\n\n\nHere is the model summary:\n\nsummary(nls_mod)\n\n\nFormula: V ~ mm_fun(S, Vmax, Km)\n\nParameters:\n     Estimate Std. Error t value Pr(&gt;|t|)    \nVmax  49.2444     0.8924   55.18   &lt;2e-16 ***\nKm     9.4953     0.4474   21.22   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.092 on 63 degrees of freedom\n\nNumber of iterations to convergence: 4 \nAchieved convergence tolerance: 4.705e-07\n\n\nThe above output provides the estimates for V_{\\max} and K_m, along with their standard errors, t-values, and p-values:\n\nThe estimated maximum uptake rate (V_{\\max}) is approximately 49.24 \\mu M N g^{-1} hr^{-1} and the small standard error associated with this parameter (0.89) indicates a precise estimate. The t-value (55.18) is very high, and the corresponding p-value is extremely small (&lt;0.0001), indicating that V_{\\max} is highly significantly different from zero.\nThe estimated Michaelis constant (K_m) is approximately 9.50 \\mu M and its standard error (0.45) is also small, suggesting a precise estimate. The t-value (21.22) and the very small p-value (&lt;0.0001) indicate that K_m is also highly significantly different from zero.\nThe residual standard error is 1.10 on 63 degrees of freedom, indicating the average deviation of the observed uptake rates from the fitted model values.\nThe model converged in 4 iterations with a very small convergence tolerance, indicating a good fit and stability of the model.\n\n\n\n\n\n\n\nResults\n\n\n\nThe Michaelis-Menten parameters, maximum uptake rate (V_{\\max}) and half-saturation constant (K_m), were estimated using nonlinear regression (Figure 7.4). The estimated V_{\\max} was 49.24 \\muM N g^{-1} hr^{-1} (SE = 0.89, t = 55.18, p &lt; 0.0001), and the estimated K_m was 9.50 \\muM (SE = 0.45, t = 21.22, p &lt; 0.0001). Both parameters were significantly different from zero. The model fit was good, converging in 3 iterations with a residual standard error of 1.10 (63 degrees of freedom).\n\n\n\n\n\n\n\n\n\n\nFigure 7.4: Plot of the Michaelis-Menten model fitted to the data in Figure 7.3. The vertical and horizontal dashed lines indicate the estimated K_m and V_{max} values, respectively.\n\n\n\n\n\nThe text is clear and concise, but here are a few minor changes for improved readability and precision:\nAssumption tests Since these data are simulated and drawn from a normal distribution with equal variances across the range of substrate concentrations, the assumptions of homoscedasticity and normality of residuals are inherently met. In this example, we fit the model solely to obtain estimates of the Michaelis-Menten parameters, rather than to make predictions, inferences, or calculate confidence intervals. Therefore, assumption tests are not critical at this stage. We will formally test assumptions in Section 7.5.2.3 when comparing the effects of experimental treatments on kinetic parameters.\n\n\nIs the Michaelis-Menten model a better fit than a linear model?\nIn Section 7.5.1.1, we pose a hypothesis that requires comparing a linear model to a Michaelis-Menten model fitted to the same data. Figure 7.4 indicates the nonlinear model indeed provides a very good fit but in some situations this distinction may be less clear and require verification. Let us fit a linear model to the above data and compare it to the Michaelis-Menten model.\n\n# Fit the linear model\nlm_mod &lt;- lm(V ~ S, data = mf_data)\n\nsummary(lm_mod)\n\n\nCall:\nlm(formula = V ~ S, data = mf_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.354 -4.791  0.580  4.948  8.293 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.46005    0.98044   6.589 1.03e-08 ***\nS            1.29488    0.06683  19.376  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.13 on 63 degrees of freedom\nMultiple R-squared:  0.8563,    Adjusted R-squared:  0.854 \nF-statistic: 375.4 on 1 and 63 DF,  p-value: &lt; 2.2e-16\n\n\nThe linear model summary shows that the slope and intercept are significantly different from zero, indicating a good fit. The R^2 value is 0.86, which is very high, suggesting that the linear model explains 86% of the variance in the data. The residual standard error is 5.13, which is higher than the Michaelis-Menten model, indicating a worse fit. We can test the difference between the models formally by examining the AIC, BIC, or SSR, and the likelihood ratio test.\n\nAIC(lm_mod, nls_mod)\n\n        df      AIC\nlm_mod   3 400.9933\nnls_mod  3 199.8814\n\n\n\nBIC(lm_mod, nls_mod)\n\n        df      BIC\nlm_mod   3 407.5164\nnls_mod  3 206.4046\n\n\n\n# Calculate the sum of squared residuals (SSR)\nsum(residuals(lm_mod)^2)\n\n[1] 1657.938\n\nsum(residuals(nls_mod)^2)\n\n[1] 75.13611\n\n\n\nanova(lm_mod, nls_mod)\n\nAnalysis of Variance Table\n\nResponse: V\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nS          1 9879.9  9879.9  375.43 &lt; 2.2e-16 ***\nResiduals 63 1657.9    26.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe AIC, BIC, and SSR values for the Michaelis-Menten model are lower than those for the linear model. Low is good, and we conclude that the Michaelis-Menten model is a better fit. The likelihood ratio test also shows that the Michaelis-Menten model is significantly better than the linear model (d.f. = 1, F=375.43, p&lt;0.0001). Therefore, we can conclude that the Michaelis-Menten model is the most appropriate model for these data and that the rate of nutrient uptake by the seaweed (in this example) is saturated at high nutrient concentrations.\n\n\nComparing treatment effects (NLS and NLMM)\nExperiments are seldom as simple as the one above. To develop our example further, consider an experiment designed to assess whether an experimental treatment, such as light intensity or seawater temperature, affects the nutrient uptake rate of a seaweed. It is biologically plausible to expect that each treatment will result in unique V_{max} and/or K_m values. For example, we know that the uptake rate of nitrate () might increase at higher light intensities and higher temperatures. Therefore, our hypothesis for this experiment is that the nutrient uptake kinetics of the seaweed is influenced by the treatment, as more formally stated in Section 7.5.1.2. To test this hypothesis, we fit a Michaelis-Menten model so that it allows estimates of V_{max} and K_m to vary among treatment groups.\nThe data for a multiple flask experiment with a treatment effect comprised of three levels are provided in Table 7.3. Except for a new variable (treatment), the data are in all other respects identical to those in Section 7.5.2.1.\n\n\n\n\n\n\n\n\n\n\n\nTreatment\nReplicate flask\n[S]\nV\n\n\n\n\nTreatment 1\n1\n0\n0.00\n\n\nTreatment 1\n2\n0\n0.00\n\n\nTreatment 1\n3\n0\n0.00\n\n\nTreatment 3\n3\n30\n17.19\n\n\nTreatment 3\n4\n30\n16.66\n\n\nTreatment 3\n5\n30\n16.00\n\n\n\n\n\n\n\n\nTable 7.3: Simulated data with three treatment levels for a multiple flask experiment on a seaweed species.\n\n\n\n\nOption 1 The nls() function in R does not handle factor variables directly, which means we cannot include the treatment variable as a factor in the model formula. To address this limitation, we fit the nls() model separately for each treatment group. This approach allows each treatment to have its own V_{\\max} and K_m values, effectively accommodating the variability in the Michaelis-Menten parameters across treatments.\nIn addition to fitting separate models for each treatment, we also fit a global model (a null model) to all the data. The global model assumes that the effect of the experimental treatment is negligible, meaning that all treatments share the same V_{\\max} and K_m. This global fit serves as a baseline for comparison.\nTo determine whether the Michaelis-Menten parameters significantly differ among the treatment groups, we perform a likelihood ratio test. The likelihood ratio test compares the fit of the global model (where parameters are shared across treatments) to the combined fit of the separate models (where parameters vary by treatment). The test statistic is the difference in the log-likelihoods of the two models, which follows a \\chi^2 distribution with degrees of freedom equal to the difference in the number of parameters between the two models.\n\n# Fit separate models\nseparate_models &lt;- mf_data2 |&gt;\n  group_by(trt) |&gt;\n  nest() |&gt;\n  mutate(model = map(data, ~nls(V ~ mm_fun(S, Vmax, Km),\n                                data = .x,\n                                start = list(Vmax = 40, Km = 10))))\n\n# Extract model summaries of separate models\nmodel_summaries &lt;- separate_models|&gt;\n  mutate(summary = map(model, broom::tidy))\n\n# Display summaries of separate models\nmodel_summaries |&gt;\n  select(trt, summary) |&gt;\n  unnest(summary)\n\n# A tibble: 6 × 6\n# Groups:   trt [3]\n  trt         term  estimate std.error statistic  p.value\n  &lt;fct&gt;       &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 Treatment 1 Vmax     49.2      0.958      51.4 3.94e-53\n2 Treatment 1 Km        9.55     0.482      19.8 9.50e-29\n3 Treatment 2 Vmax     39.4      0.865      45.5 6.66e-50\n4 Treatment 2 Km        7.54     0.481      15.7 2.14e-23\n5 Treatment 3 Vmax     19.2      0.558      34.5 1.34e-42\n6 Treatment 3 Km        5.87     0.560      10.5 1.97e-15\n\n# Fit the global model\nglobal_model &lt;- nls(V ~ mm_fun(S, Vmax, Km),\n                    data = mf_data2,\n                    start = list(Vmax = 45, Km = 9))\n\n# Extract log-likelihoods and degrees of freedom\nlogLik_global &lt;- logLik(global_model)\ndf_global &lt;- attr(logLik_global, \"df\")\n\n# Combined log-likelihoods and degrees of freedom\nlogLik_separate &lt;- sum(sapply(separate_models$model, logLik))\ndf_separate &lt;- sum(sapply(separate_models$model,\n                          function(m) attr(logLik(m), \"df\")))\n\n# Perform the likelihood ratio test\nlrt_stat &lt;- 2 * (logLik_separate - logLik_global)\np_value &lt;- pchisq(lrt_stat, df = df_separate - df_global,\n                  lower.tail = FALSE)\n\n# Display results\ncat(\"Global model log-likelihood:\", logLik_global, \"\\n\")\n\nGlobal model log-likelihood: -620.5374 \n\ncat(\"Separate models log-likelihood:\", logLik_separate, \"\\n\")\n\nSeparate models log-likelihood: -300.2111 \n\ncat(\"Degree of freedom:\", df_separate - df_global, \"\\n\")\n\nDegree of freedom: 6 \n\ncat(\"Likelihood ratio test statistic:\", lrt_stat, \"\\n\")\n\nLikelihood ratio test statistic: 640.6525 \n\ncat(\"p-value:\", p_value, \"\\n\")\n\np-value: 3.953134e-135 \n\n\nThe results of the likelihood ratio test indicate whether the variation in V_{\\max} and K_m among the treatments is statistically significant. If the test is significant, it suggests that the Michaelis-Menten parameters differ across treatments. We interpret the results as follows:\n\nThe log-likelihood value (-620.7498) for the global model, indicating the fit of the model with shared parameters.\nThe combined log-likelihood value (-313.1862) for the separate models, indicating the fit of the models with parameters varying by treatment.\nThe calculated test statistic (615.1273) for the likelihood ratio test on 6 degrees of freedom.\nThe p-value of the test is less than 0.0001 and provides strong evidence that V_{\\max} and K_m differ significantly among the treatment groups.\n\n\n\n\n\n\n\nResults\n\n\n\nThe analysis aimed to determine if the Michaelis-Menten parameters V_{\\max} and K_m significantly differed among the three experimental treatments. This was evaluated by fitting a global model with shared V_{\\max} and K_m values across all treatments and comparing it to a model allowing separate V_{\\max} and K_m estimates for each treatment. The log-likelihood value for the global model, which assumes shared V_{\\max} and K_m values across all treatments, was -620.75, indicating the fit of the model with common parameters. In contrast, the combined log-likelihood value for the separate models, which allow V_{\\max} and K_m to vary by treatment, was -313.19, indicating the fit of the models with treatment-specific parameters. The calculated test statistic for the likelihood ratio test was 615.13 (d.f. = 6, p &lt; 0.001), providing strong evidence that the Michaelis-Menten parameters V_{\\max} and K_m differ significantly among the treatment groups. Consequently we estimate a V_{max} of 49.2 ± 0.96, 39.4 ± 0.87 \\muM N g^{-1} hr^{-1} and 18.9 ± 0.65 and a K_m of 9.55 ± 0.48, 7.54 ± 0.48 and 5.50 ± 0.64 \\muM for treatments 1, 2 and 3 respectively.\n\n\nOption 2 If Option 1 seems cumbersome, we can fit a NLMM using the nlme package instead. This package allows us to fit a mixed model with random effects for each treatment group. In this model, the fixed effects are the Michaelis-Menten parameters V_{\\max} and K_m, which vary by treatment, while the random effects are the replicate-specific intercepts. Thus, the cumbersome nls() formulation is replaced by the compact but more fiddly nlme() model specification. Pick your poison. The model is specified as follows:\n\n# Fit the model with the same parameters for both treatments\n# Starting values for Vmax and Km\nstart_vals &lt;- c(Vmax = 50, Km = 10)\nglobal_model &lt;- nlme(\n  V ~ mm_fun(S, Vmax, Km),\n  data = mf_data2,\n1  fixed = Vmax + Km ~ 1,\n2  random = Vmax ~ 1 | trt/rep,\n  start = start_vals\n)\n\n# Fit the model with parameters varying by treatment\n# Starting values for Vmax and Km for each treatment\nstart_vals &lt;- c(Vmax1 = 50, Vmax2 = 40, Vmax3 = 30,\n3                Km1 = 10, Km2 = 10, Km3 = 5)\nseparate_models &lt;- nlme(\n  V ~ mm_fun(S, Vmax, Km),\n  data = mf_data2,\n4  fixed = list(Vmax ~ trt, Km ~ trt),\n  random = Vmax ~ 1 | trt/rep,\n  start = start_vals\n)\n\n\n1\n\nThe fixed effects indicate that both V_{\\max} and K_m are fixed (do not vary) across treatments.\n\n2\n\nThe random effects indicate that the V_{\\max} parameter varies by treatment and replicate.\n\n3\n\nThe starting values for the V_{\\max} and K_m parameters are specified for each treatment group. Because we are now fitting a separate model for each treatment, we need to provide starting values for each treatment.\n\n4\n\nThe fixed effects now indicate that both V_{\\max} and K_m vary by treatment.\n\n\n\n\nThe estimated parameters for the global model and the separate models can be extracted using the summary() function:\n\n# Extract the estimated parameters (abbreviated output)\n# summary(global_model) # for verbose output\nsummary(global_model)$tTable\n\n         Value Std.Error  DF   t-value      p-value\nVmax 36.248519  6.287216 179  5.765432 3.504878e-08\nKm    8.271727  0.304345 179 27.178780 1.952829e-65\n\n\n\n# Extract the estimated parameters (abbreviated output)\n# summary(separate_models) # for verbose output\nsummary(separate_models)$tTable\n\n                         Value Std.Error  DF    t-value       p-value\nVmax.(Intercept)     49.199643 0.9498953 175  51.794808 4.546825e-108\nVmax.trtTreatment 2  -9.879910 1.2312719 175  -8.024150  1.422499e-13\nVmax.trtTreatment 3 -29.971535 1.1529098 175 -25.996427  5.425758e-62\nKm.(Intercept)        9.542071 0.4707903 175  20.268197  8.782004e-48\nKm.trtTreatment 2    -2.027313 0.6350017 175  -3.192611  1.671900e-03\nKm.trtTreatment 3    -3.689268 0.7898830 175  -4.670651  5.961284e-06\n\n\nThe log-likelihood ratio test can then easily be performed using the anova() function, which compares the global model with the separate models:\n\nanova(global_model, separate_models)\n\n                Model df      AIC      BIC    logLik   Test  L.Ratio p-value\nglobal_model        1  5 657.9763 674.3413 -323.9882                        \nseparate_models     2  9 621.5038 650.9608 -301.7519 1 vs 2 44.47252  &lt;.0001\n\n\nAgain, the results of the likelihood ratio test indicate that the variation in V_{\\max} and K_m among the treatments is statistically significant (log-likelihood = 45.20, p &lt; 0.0001). The AIC values can also be used to compare the models, with lower AIC values indicating a better fit. In this case, the separate models have a lower AIC value (644.28), suggesting that they provide a better fit to the data than the global model (681.479). The data fitted with the global and separate models is presented in Figure 7.6.\nAssumption tests To complete our example comparing the Michaelis-Menten parameters among treatments, let’s confirm the assumptions by examining the residuals. Residuals in nonlinear regression models have the same interpretation as in linear models, and therefore, the assumption tests available for linear models can be applied here as well. For instance, we can use the shapiro.test() function to check the normality of residuals, as shown below, and the hist() and plot() functions for diagnostic plots. In real-world data, it is advised to verify these assumptions before accepting the analysis and drawing conclusions from the nonlinear regression model. Let’s check the normality of residuals for each treatment and plot the residuals to check for normality and homoscedasticity (Figure 7.5).\n\n# Add residuals and fitted information to the data frame\nmf_data2$residuals_separate &lt;- residuals(separate_models)\nmf_data2$fitted_values_separate &lt;- fitted(separate_models)\n\n# Perform the Shapiro-Wilk test for each treatment\nshapiro.test(mf_data2$residuals_separate[mf_data2$trt == \"Treatment 1\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  mf_data2$residuals_separate[mf_data2$trt == \"Treatment 1\"]\nW = 0.976, p-value = 0.2374\n\nshapiro.test(mf_data2$residuals_separate[mf_data2$trt == \"Treatment 2\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  mf_data2$residuals_separate[mf_data2$trt == \"Treatment 2\"]\nW = 0.97125, p-value = 0.1344\n\nshapiro.test(mf_data2$residuals_separate[mf_data2$trt == \"Treatment 3\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  mf_data2$residuals_separate[mf_data2$trt == \"Treatment 3\"]\nW = 0.95091, p-value = 0.01177\n\n\n\n\n\n\n\n\n\n\nFigure 7.5: Histograms (A) of residuals and plots of residuals vs. the fitted values (B) for residuals for the three treatments in the multiple-flask experiment.\n\n\n\n\n\nThe Shapiro-Wilk test results indicate that the residuals are normally distributed for Treatments 1 and 2 (p &gt; 0.05) but not for Treatment 3 (p &lt; 0.05). However, the histograms in Figure 7.5 show that the residuals are approximately normally distributed for all treatment groups, with the median roughly in the middle of the distribution in each case. This apparent discrepancy can be explained by the sensitivity of the Shapiro-Wilk test to sample size. With large sample sizes, even minor deviations from normality can be detected as statistically significant. In situations such as this one, I suggest that it is important to consider the sample size and visual inspection of the data when interpreting the results of normality tests. Here, given the relatively large sample size and the visual assessment of the histograms, we can reasonably conclude that the residuals are approximately normally distributed for all treatment groups.\nAnother normality tests such as the Kolmogorov-Smirnov (K-S) test might be less sensitive to sample size and could be considered for comparison. The K-S test is a non-parametric statistical test that is used to determine if a sample comes from a specific probability distribution. Here I use it to test if a sample follows a normal distribution (pnorm), but it can also be used to test against other theoretical distributions or to compare two empirical distributions. The K-S test can be performed using the ks.test(), as shown below.\n\nperform_ks_test &lt;- function(data, treatment) {\n  ks.test(data$residuals_separate[data$trt == treatment], \"pnorm\", \n          mean = mean(data$residuals_separate[data$trt == treatment]), \n          sd = sd(data$residuals_separate[data$trt == treatment]))\n}\n\n# Perform the test for each treatment group\nperform_ks_test(mf_data2, \"Treatment 1\")\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  data$residuals_separate[data$trt == treatment]\nD = 0.10658, p-value = 0.4513\nalternative hypothesis: two-sided\n\nperform_ks_test(mf_data2, \"Treatment 2\")\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  data$residuals_separate[data$trt == treatment]\nD = 0.1246, p-value = 0.2652\nalternative hypothesis: two-sided\n\nperform_ks_test(mf_data2, \"Treatment 3\")\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  data$residuals_separate[data$trt == treatment]\nD = 0.14151, p-value = 0.148\nalternative hypothesis: two-sided\n\n\nWe see that the K-S test indicates that the residuals are normally distributed for all treatment groups (p &gt; 0.05). As already noted, this test is less sensitive to sample size than the Shapiro-Wilk test, and the results are consistent with the visual assessment of the histograms.\nWe should also check for homoscedasticity (here I use the Levene test) and a plot of residuals versus fitted values.\n\n# Perform the Levene test\ncar::leveneTest(residuals_separate ~ trt, data = mf_data2)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   2  1.4933 0.2272\n      192               \n\n\nThe Levene test shows that the variances are the same across the three treatments and this is confirmed by the plot of residuals against the fitted values in Figure 7.5.\n\n\n\n\n\n\nResults\n\n\n\nMichaelis-Menten models were fitted to nutrient uptake data across three experimental treatments to investigate the effects of the treatments on seaweed nutrient kinetics. A global model, assuming shared kinetic parameters (V_{max} and K_m) across all treatments, was compared to a model with separate parameters for each treatment. The model allowing treatment-specific parameters (AIC = 644.3) provided a significantly better fit to the data than the global model (AIC = 681.5), a finding confirmed by the log-likelihood test (log-likelihood ratio = 45.20, d.f. = 4, p &lt; 0.0001). As the assumption tests do not indicate any cause for concern regarding the distribution of residuals, we conclude that the experimental treatments significantly influenced the nutrient uptake kinetics of the seaweed (Figure 7.6).\nSpecifically, all three treatments exhibited unique combinations of V_{max} and K_m values (Treatment 1: V_{max} = 49.2, K_m = 9.5; Treatment 2: V_{max} = 39.3, K_m = 7.5; Treatment 3: V_{max} = 19.0, K_m = 5.5). These findings support the hypothesis that nutrient uptake kinetics in this seaweed species are sensitive to environmental perturbations.\n\n\n\n\n\n\n\n\n\n\nFigure 7.6: Plot of the Michaelis-Menten model fitted to the data in Table 7.3. Fits are provided for the separate models and the global model.\n\n\n\n\n\n\n\n\nThe Perturbation Method (NLMM)\nThe data for this example is by Smit (2002). A perturbation experiment was conducted to determine the nutrient uptake rate versus nutrient concentration of the red seaweed, sp. The experiment involved flasks, initially enriched to approximately 55 μM nitrate, sampled 16 times over approximately 2.5 hours. The uptake rates were measured under three rates of water movement (treatments): low, medium, and high. Each treatment had three replicate flasks (Table 7.4). The primary objective was to determine if the Michaelis-Menten parameters significantly differ among the three levels of water movement, and we must state a hypothesis similar to those in Section 7.5.1.2.\n\n\n\n\n\n\n\n\n\n\n\nReplicate flask\nTreatment\nV\n[S]\n\n\n\n\n1\nlow\n10.8\n60.2\n\n\n2\nlow\n10.0\n61.1\n\n\n3\nlow\n14.1\n60.8\n\n\n1\nhigh\n0.0\n0.1\n\n\n2\nhigh\n0.0\n0.1\n\n\n3\nhigh\n0.0\n0.1\n\n\n\n\n\n\n\n\nTable 7.4: Simulated data for a multiple flask experiment on an alga (showing only the top and bottom three rows).\n\n\n\n\nFor the reasons discussed in Section 7.5, we will use a nonlinear mixed effects model, nlme(), to analyse these data. Models such as these can be quite challenging to fit. There are several things we have to deal with. First and most obviously is the fact that the data are repeated measures, and the residuals may be correlated. Second, the flasks are nested within the treatment levels, and we need to account for this in the model. Finally, we need to account for the possibility that the Michaelis-Menten parameters may vary among the treatment levels—in fact, we want to test this! Here is the model:\n\n# Determine the number of levels in the factor 'trt'\nnum_levels &lt;- length(levels(mm_data$trt))\n\n# Starting values for the fixed parameters\n# (one set for each level of 'trt')\nstart_vals &lt;- list(fixed = c(Vmax = rep(max(mm_data$V), num_levels),\n                             Km = rep(median(mm_data$S), num_levels)))\n\nnlme_mod2 &lt;- nlme(V ~ mm_fun(S, Vmax, Km),\n                  data = mm_data,\n1                  fixed = Vmax + Km ~ trt,\n2                  random = Vmax + Km ~ 1 | flask,\n                  start = start_vals,\n                  method = \"REML\")\n\n\n1\n\nThe fixed argument specifies that the Michaelis-Menten parameters Vmax and Km are fixed effects that vary among the treatment levels, and a grouping variable (trt) is used to specify the levels of the treatment factor.\n\n2\n\nThe random argument specifies that the Michaelis-Menten parameters Vmax and Km are random effects that vary among the replicate flasks.\n\n\n\n\nThis model brings us closer to our goal, but there are some notable omissions. The specification allows the Michaelis-Menten parameters to vary among the treatment levels, which is central to our hypothesis. We have also accounted for the replication structure of the data, recognising that random variations may arise not due to the treatment levels but due to the replicate flasks.\nHowever, we have not accounted for the central feature of a perturbation experiment, which is the correlation structure of the residuals. We must deal with the fact that the residuals may be correlated due to the repeated measures nature of the data. Additionally, we have omitted the nesting of the flasks within the treatment levels.\nLet’s update our model accordingly:\n\nnlme_mod3 &lt;- nlme(V ~ mm_fun(S, Vmax, Km),\n                  data = mm_data,\n                  fixed = list(Vmax ~ trt, Km ~ trt),\n1                  random = Vmax ~ 1 | trt/flask,\n2                  groups = ~ trt/flask,\n3                  correlation = corAR1(form = ~ 1 | trt/flask),\n                  start = start_vals,\n                  method = \"REML\")\n\n\n1\n\nThe random argument specifies that the Michaelis-Menten parameter Vmax is a random effect that varies among the replicate flasks nested within the treatment levels.\n\n2\n\nThe groups argument specifies that the replicate flasks are nested within the treatment levels.\n\n3\n\nThe correlation argument specifies that the residuals have a first-order autoregressive correlation structure. This structure assumes that the correlation between residuals decreases exponentially with the time lag between observations.\n\n\n\n\nIf we are not convinced that nlme_mod3 is the best model, we can compare it to nlme_mod2 using a likelihood ratio test. It is used to compare the fit of two models, where one model is a special case of the other. The test statistic is the difference in the log-likelihoods of the two models, and the null hypothesis is that the simpler model is the best fit.\n\nanova(nlme_mod2, nlme_mod3)\n\n          Model df      AIC      BIC    logLik\nnlme_mod2     1 10 637.2782 665.6411 -308.6391\nnlme_mod3     2 10 632.1053 660.4681 -306.0527\n\n# Likelihood ratio test\nlrt_stat &lt;- -2 * (logLik(nlme_mod2) - logLik(nlme_mod3))\n\n# Determine degrees of freedom and p-value\ndf_diff &lt;- attr(logLik(nlme_mod3), \"df\") - attr(logLik(nlme_mod2), \"df\") \np_value &lt;- pchisq(lrt_stat, df = df_diff, lower.tail = FALSE) \n\nprint(paste(\"LRT statistic:\", lrt_stat))\n\n[1] \"LRT statistic: 5.17293584867423\"\n\nprint(paste(\"Degrees of freedom:\", df_diff))\n\n[1] \"Degrees of freedom: 0\"\n\nprint(paste(\"P-value:\", p_value))\n\n[1] \"P-value: 0\"\n\n\nThe likelihood ratio test indicates that nlme_mod3 is a better fit than nlme_mod2 (p &lt; 0.001). This result suggests that the Michaelis-Menten parameters vary among the treatment levels, and the residuals have a first-order autoregressive correlation structure.\n\nsummary(nlme_mod3)\n\nNonlinear mixed-effects model fit by REML\n  Model: V ~ mm_fun(S, Vmax, Km) \n  Data: mm_data \n       AIC      BIC    logLik\n  632.1053 660.4681 -306.0527\n\nRandom effects:\n Formula: Vmax ~ 1 | trt\n        Vmax.(Intercept)\nStdDev:       0.00837941\n\n Formula: Vmax ~ 1 | flask %in% trt\n        Vmax.(Intercept) Residual\nStdDev:     0.0002584018 2.731378\n\nCorrelation Structure: AR(1)\n Formula: ~1 | trt/flask \n Parameter estimate(s):\n      Phi \n0.2048944 \nFixed effects:  list(Vmax ~ trt, Km ~ trt) \n                     Value Std.Error  DF   t-value p-value\nVmax.(Intercept) 15.394469  1.082697 118 14.218627  0.0000\nVmax.trtlow      -1.660245  2.381505 118 -0.697141  0.4871\nVmax.trtmed      -3.555246  1.503682 118 -2.364361  0.0197\nKm.(Intercept)    5.381378  1.873000 118  2.873133  0.0048\nKm.trtlow        11.448682  8.044641 118  1.423144  0.1573\nKm.trtmed        -0.381246  3.147606 118 -0.121123  0.9038\n Correlation: \n               Vm.(I) Vmx.trtl Vmx.trtm Km.(I) Km.trtl\nVmax.trtlow    -0.455                                 \nVmax.trtmed    -0.720  0.327                          \nKm.(Intercept)  0.726 -0.330   -0.523                 \nKm.trtlow      -0.169  0.876    0.122   -0.233        \nKm.trtmed      -0.432  0.196    0.734   -0.595  0.139 \n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.0222398 -0.7529003 -0.2362146  0.4364407  3.2055101 \n\nNumber of Observations: 132\nNumber of Groups: \n           trt flask %in% trt \n             3              9",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonlinear Models</span>"
    ]
  },
  {
    "objectID": "non-linear_regression.html#sec-vonbertalanffy",
    "href": "non-linear_regression.html#sec-vonbertalanffy",
    "title": "7  Nonlinear Models",
    "section": "7.6 Example: The Growth Rate of Fish (NLMM)",
    "text": "7.6 Example: The Growth Rate of Fish (NLMM)\nThe von Bertalanffy model (Equation 7.4) is used to describe the growth patterns of animals over time. For example, in a fish growth study, we measure the length of individual fish at regular intervals as the fish age. By fitting the von Bertalanffy model to these length-at-age data, we can estimate growth parameters specific to the fish species.\nThe model is given by:\nL(t) = L_{\\infty} \\left(1 - e^{-k(t-t_0)}\\right) \\tag{7.4}\nWhere:\n\nL(t) is the length of the fish at time t.\nL_{\\infty} is the asymptotic length, representing the theoretical maximum length that the individual would reach if it grew indefinitely.\nk is the growth coefficient, indicating the rate at which the growth of the fish approaches its maximum size. A higher k value means it reaches its asymptotic length more quickly.\nt_0 is the hypothetical age at which the individual’s length would be zero according to the model.\n\nL_{\\infty} (the asymptotic length) represents the length towards which the individual grows as time (t) approaches infinity. The concept behind L_{\\infty} is that as the fish ages, its growth rate slows down and eventually approaches zero, with its length nearing the asymptotic value L_{\\infty}. k (the growth rate coefficient) determines how quickly the fish reaches its asymptotic length. Physiologically, k reflects the metabolic rates and general fitness of the fish, while ecologically, it can be influenced by environmental factors such as food availability and temperature. Lastly, t_0 (the theoretical age at zero length) is not directly observable in practice but provides a useful way to shift the growth curve along the time axis to provide a better fit to the data, especially in the early developmental stages.\nConsider a study where the lengths of 30 Atlantic Cod, Gadus morua, in captivity are measured twice a year from hatching to 15 years. This creates a longitudinal dataset with repeated length measurements for each fish over time. In this experiment, we will focus on the growth patterns of individual fish, assuming they were raised under identical conditions. This allows us to attribute any growth differences to inherent biological variation among the fish. Apart from the repeated measures on individual fish, we will assume that the data are independent in all other respects.\nThe longitudinal nature of the data requires that we use appropriate statistical methods that account for the correlation among the repeated measures. We will use a nonlinear mixed-effects regression for the data in Table 1.\n\n\n\n\n\n\n\n\n\nFish ID\nAge (yr)\nLength (cm)\n\n\n\n\n1\n0.0\n5.3\n\n\n1\n0.5\n16.8\n\n\n1\n1.0\n27.2\n\n\n30\n14.0\n115.4\n\n\n30\n14.5\n116.0\n\n\n30\n15.0\n116.5\n\n\n\n\nThe Atlantic Cod data set with 30 fish and 15 years of growth data (showing only the top and bottom three rows).\n\n\nA plot of the data is shown in Figure 7.7; here, each line represents the growth trajectory of an individual fish over time.\n\n\nList of 1\n $ legend.position: chr \"none\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\n\n\n\n\nFigure 7.7: Plot of growth data measured in 30 Atlantic cod, Gadus morua.\n\n\n\n\n\nWe will fit the von Bertalanffy growth model to the data using nlme::nlme() as follows, and the output is provided:\n\n# von Bertalanffy growth function\nvb_growth &lt;- function(age, L_inf, k, t0) {\n  L_inf * (1 - exp(-k * (age - t0)))\n}\n\n# Define the nonlinear mixed-effects model\nnlme_model &lt;- nlme(Length ~ vb_growth(Age, L_inf, k, t0),\n                   data = vb_data,\n1                   fixed = L_inf + k + t0 ~ 1,\n2                   random = L_inf + k ~ 1 | Fish_ID,\n3                   groups = ~ Fish_ID,\n4                   correlation = corAR1(form = ~ 1),\n                   start = c(L_inf = 100, k = 0.2, t0 = -0.5))\n\n# Print the summary of the model\nsummary(nlme_model)\n\n\n1\n\nThe fixed effects are the parameters of the von Bertalanffy growth model which are invariant among fish.\n\n2\n\nThe random effects are the asymptotic length and growth rate to account for the intrinsic differences among fish.\n\n3\n\nThe grouping variable is the fish ID.\n\n4\n\nThe correlation structure is autoregressive of order 1 to account for the correlation among repeated measures within the same fish, the ~ 1 indicates that the order of the observations in the data must be used along which measurements are serially correlated, and since no grouping variable is provided, all fish will have the same correlation structure.\n\n\n\n\nNonlinear mixed-effects model fit by maximum likelihood\n  Model: Length ~ vb_growth(Age, L_inf, k, t0) \n  Data: vb_data \n        AIC       BIC  logLik\n  -2833.361 -2794.679 1424.68\n\nRandom effects:\n Formula: list(L_inf ~ 1, k ~ 1)\n Level: Fish_ID\n Structure: General positive-definite, Log-Cholesky parametrization\n         StdDev      Corr  \nL_inf    1.857032547 L_inf \nk        0.008341198 -0.139\nResidual 0.555742464       \n\nCorrelation Structure: AR(1)\n Formula: ~1 | Fish_ID \n Parameter estimate(s):\n      Phi \n0.9972623 \nFixed effects:  L_inf + k + t0 ~ 1 \n          Value Std.Error  DF  t-value p-value\nL_inf 124.80230 0.3551041 898 351.4527       0\nk       0.20042 0.0015299 898 131.0050       0\nt0     -0.20415 0.0040574 898 -50.3164       0\n Correlation: \n   L_inf  k     \nk  -0.137       \nt0 -0.260 -0.007\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.2053004 -0.7552048  0.2402975  0.4902483  1.9150860 \n\nNumber of Observations: 930\nNumber of Groups: 30 \n\n\n\n\n\n\n\n\n\n\nFigure 7.8: Fit of the von Bertalanffy model to experimental data obtained from 30 Atlantic Cod individuals.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonlinear Models</span>"
    ]
  },
  {
    "objectID": "non-linear_regression.html#scrathpad",
    "href": "non-linear_regression.html#scrathpad",
    "title": "7  Nonlinear Models",
    "section": "7.7 Scrathpad",
    "text": "7.7 Scrathpad\n\nTo include in the article\n\nUse Cases: Mentioning use cases like trend analysis in time-series data or other applications where the relationship between variables might be expected to follow a polynomial form (e.g., growth rates, trajectories) can provide practical context.\nAssumptions: Not necessary for simply estimating model parameters, but if the model is used for prediction or inference, it is important to state the assumptions of the model (e.g., linearity, homoscedasticity, independence of residuals) and test them.\ni.i.d.: The residuals are assumed to be independent and identically distributed (i.i.d.), which is a common assumption in linear regression models.\nHomoscedasticity:\ni.i.d: The residuals are assumed to be independent and identically distributed (i.i.d.), which is a common assumption in linear regression models. For a normal distribution, this is written as \\epsilon_i \\sim N(0, \\sigma^2), where \\sigma^2 is the variance of the residuals.\n\n\n\nContuinuing the MM model\n\n\n\n\n\n\nSmit A (2002) Nitrogen uptake by Gracilaria gracilis (Rhodophyta): Adaptations to a temporally variable nitrogen environment. Bot Mar 45:196–209.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Nonlinear Models</span>"
    ]
  },
  {
    "objectID": "regularisation.html",
    "href": "regularisation.html",
    "title": "8  Regularisation Techniques",
    "section": "",
    "text": "8.1 Ridge Regression (L2 Regularisation)\nRidge regression mathematically ‘tames’ the wildness of linear regression when faced with multicollinearity. It achieves this by adding a penalty term to the linear regression loss function—a term proportional to the square of the coefficients (the L2 norm). This penalty nudges the coefficients towards zero, effectively shrinking them without forcing them to be exactly zero.\nIn linear regression, the loss function is typically the Mean Squared Error (MSE), which is the average of the squared residuals (also known as the residual sum of squares, RSS). The optimisation objective is to minimise this loss function. In other words, the linear regression model aims to find the coefficients that minimise the average squared difference between the observed values and the predicted values. The RSS is expressed in Equation 8.1:\nRSS(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 \\tag{8.1}\nAnd the MSE, which is the loss function to be minimised, is in Equation 8.2:\nMSE(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 \\tag{8.2}\nWhere:\nThe notation RSS(\\beta) and MSE(\\beta) indicates that these are functions of the coefficients \\beta. The optimisation objective for linear regression is to find the coefficients \\beta_0 and \\beta_1 to \\beta_p that minimise the MSE. This can be expressed in Equation Equation 8.3:\n\\min_{\\beta} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij})^2 \\right\\} \\tag{8.3}\nRidge regression extends the optimisation of the least squares regression by introducing a penalty term to the loss function. This penalty term is proportional to the square of the L2 norm of the coefficient vector, penalising large coefficient values. Ridge regression is specifically designed to handle multicollinearity and mitigate issues caused by correlated predictors. It also helps prevent overfitting when there are many predictors relative to the sample size, providing a more stable estimation process.\nThe penalty term is controlled by a hyperparameter1 called lambda (\\lambda) that determines the strength of the penalty. Larger values of \\lambda lead to more shrinkage of the coefficients. When \\lambda = 0, Ridge Regression is equivalent to ordinary least squares regression. As \\lambda approaches infinity, all coefficients (except the intercept) approach zero. To find the optimal \\lambda, you might have to use techniques like cross-validation. Cross-validation will be discussed later in Section Section 8.4.\nThe loss function in Ridge Regression is given by Equation 8.4:\nL_{ridge}(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\tag{8.4}\nWhere \\lambda is the regularisation parameter controlling the penalty’s strength. Note that typically, the intercept \\beta_0 is not included in the penalty term.\nIn Equation Equation 8.4, L_{ridge}(\\beta) is the Ridge Regression loss function. This loss function includes the residual sum of squares (RSS) plus a penalty term \\lambda \\sum_{j=1}^p \\beta_j^2. The optimisation objective in Ridge Regression is to find the values of the coefficients \\beta_1 through \\beta_p that minimise this penalised loss function, while also finding the optimal value for the intercept \\beta_0.\nRidge regression introduces a bias-variance trade-off. By shrinking the coefficients, it introduces a slight bias, as the model’s predictions may not perfectly match the training data. However, this bias is often offset by a significant reduction in variance. The reduced variance means the model’s predictions are more stable and less sensitive to small changes in the input data. This trade-off often results in improved overall predictive performance, especially on new, unseen data.\nSo, Ridge Regression sacrifices a bit of bias (accuracy on the sample data) to gain a lot in terms of reduced variance (generalisation to new data). This is a typical example of the bias-variance trade-off in statistical modelling and machine learning, where we often find that a bit of bias can lead to a much more robust and reliable model.\nUnlike some other regularisation methods, such as principal component regression, Ridge Regression maintains the interpretability of the coefficients in terms of their relationship with the outcome. It is also versatile and can be applied to various types of regression models, including linear and logistic regression.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regularisation Techniques</span>"
    ]
  },
  {
    "objectID": "regularisation.html#ridge-regression-l2-regularisation",
    "href": "regularisation.html#ridge-regression-l2-regularisation",
    "title": "8  Regularisation Techniques",
    "section": "",
    "text": "y_i is the observed value for the i-th observation.\n\n\\beta_0 is the intercept.\n\n\\beta_j are the coefficients for the predictors.\n\nx_{ij} is the value of the j-th predictor variable for the i-th observation.\n\nn is the number of observations.\n\np is the number of predictors.\n\n\n\n\n\n1 Hyperparameters are configuration settings that are external to your model and not learned from the data itself.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regularisation Techniques</span>"
    ]
  },
  {
    "objectID": "regularisation.html#lasso-regression-l1-regularisation",
    "href": "regularisation.html#lasso-regression-l1-regularisation",
    "title": "8  Regularisation Techniques",
    "section": "\n8.2 Lasso Regression (L1 Regularisation)",
    "text": "8.2 Lasso Regression (L1 Regularisation)\nLasso (Least Absolute Shrinkage and Selection Operator) regression employs a different penalty term compared to Ridge Regression. Instead of squaring the coefficients, Lasso Regression takes their absolute values. The cost function in Lasso Regression is given in Equation Equation 8.5:\nL_{lasso}(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\tag{8.5}\nIn Equation Equation 8.5, L_{lasso}(\\beta) is the Lasso Regression loss function. It includes the residual sum of squares (RSS) plus a penalty term \\lambda \\sum_{j=1}^{p} |\\beta_j| (L1 norm). This penalty term is the sum of the absolute values of the coefficients, scaled by the regularisation parameter \\lambda (similar to Ridge Regression). Lasso regression seeks the values of \\beta_0 through \\beta_p that minimise L_{lasso}(\\beta). As with Ridge Regression, the intercept \\beta_0 is typically not included in the penalty term.\nThe strength of Lasso Regression lies in its ability to shrink some coefficients all the way to zero, effectively eliminating those variables from the model. This automatic variable selection makes Lasso Regression well-suited for creating sparse models where only the most influential variables are retained. This simplification aids in interpretation and can enhance model performance by reducing noise and overfitting.\nLasso Regression still applies a degree of shrinkage for the coefficients that are not shrunk to zero. Shrinkage reduces their variance and provide more stable models that are less sensitive to fluctuations in the data. Similar to Ridge Regression, Lasso involves a trade-off between bias and variance. The shrinkage introduces a small bias but can greatly reduce variance and result in better overall predictions.\nLasso regression is useful when dealing with datasets that have a large number of potential predictor variables. It helps identify the most relevant predictors. The end results is a simpler and more interpretable model. If you suspect redundancy among your predictor variables, Lasso can prune them and retain only those that provide the best predictive value. As always, the optimal value for \\lambda should be determined through techniques like cross-validation.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regularisation Techniques</span>"
    ]
  },
  {
    "objectID": "regularisation.html#elastic-net-regression",
    "href": "regularisation.html#elastic-net-regression",
    "title": "8  Regularisation Techniques",
    "section": "\n8.3 Elastic Net Regression",
    "text": "8.3 Elastic Net Regression\nElastic net regression is a hybrid regularisation technique that combines the penalties of Ridge and Lasso Regression. It tries to provide the advantages of both methods and mitigate their drawbacks.\nHere, the penalty term is the weighted average of the L1 (Lasso) and L2 (Ridge) penalties. A mixing parameter called alpha (\\alpha) controls the weighting between the two penalties. When \\alpha = 0, Elastic Net is equivalent to Ridge Regression and when \\alpha = 1 it is equivalent to Lasso Regression. For values of \\alpha between 0 and 1, Elastic Net blends the properties of both methods and provides some flexibility to regularisation.\nThe cost function in Elastic Net Regression is given in Equation 8.6:\nL_{e_net}(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\left( \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1 - \\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right) \\tag{8.6}\nWhere \\alpha is the mixing parameter, with 0 \\leq \\alpha \\leq 1.\nIn Equation 8.6 there is the familiar RSS plus the combined penalty term that is a weighted sum of the L1 and L2 norms. The objective of Elastic Net Regression is again to minimise L_{e_net}(\\beta) by seeking optimal values of \\beta_1 through \\beta_p.\nLike the other regularisation techniques, Elastic Net is also used when you have highly correlated predictors. While Lasso Regression might arbitrarily select one variable from a group and ignore the rest, Elastic Net tends to select groups of correlated features together and so provide a more comprehensive understanding of variable importance. The flexibility of adjusting the \\alpha parameter allows you to fine-tune the regularisation to best suit your specific dataset and modelling goals. It balances variable selection (Lasso) and shrinkage (Ridge). Also, Elastic Net can outperform Lasso and Ridge Regression in terms of prediction accuracy when dealing with high-dimensional datasets where the number of predictors exceeds the number of observations.\nElastic net is a good option if you have a dataset with many potential predictor variables and suspect strong correlations among them. Use it when you are uncertain whether pure variable selection (Lasso) or pure shrinkage (Ridge) is the best approach. The challenge is that now we also have to tune the \\alpha parameter in addition to the regularisation parameter \\lambda. A caveat is that Elastic Net retains the interpretability of individual coefficients but the interpretation becomes slightly more nuanced due to the mixed penalty term. This requires a thoughtful approach to understanding the model outputs.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regularisation Techniques</span>"
    ]
  },
  {
    "objectID": "regularisation.html#sec-cross-validation",
    "href": "regularisation.html#sec-cross-validation",
    "title": "8  Regularisation Techniques",
    "section": "\n8.4 Cross-Validation",
    "text": "8.4 Cross-Validation\nThe values of the hyperparameters (\\lambda or \\alpha) significantly affect the model’s performance and generalisation ability and so it necessitates careful optimisation. The cv.glmnet() function (see Section 8.5) automates this process by performing both hyperparameter tuning2 and cross-validation. It systematically evaluates different combinations of \\lambda or \\alpha values across multiple subsets of our data, using cross-validation to estimate their out-of-sample performance. This allows for the selection of the hyperparameter combination that yields the best performance and thus avoids the risk of overfitting and improves model generalisation.\n2 The goal of hyperparameter tuning is to find the optimal combination of hyperparameters that leads to the best model performance on your specific dataset. This is done by systematically evaluating different hyperparameter values and selecting the combination that yields the best results.The most widely used cross-validation method is \\text{k}-fold cross-validation. The dataset is divided into \\text{k} equally sized subsets (specified by the user). The subsets are called ‘folds’. The model is then trained \\text{k} times, each time using \\text{k}-1 folds for training and the remaining fold for validation. It provides a robust estimate of model performance by utilising all data points for both training and validation. It balances computational cost and bias reduction. But, the choice of k can influence results, and there’s a trade-off between bias and variance: lower \\text{k} values may lead to higher bias but lower variance, whilst higher \\text{k} values do the opposite.\nThe general approach taken in \\text{k}-fold cross validation is that, for each combination of hyperparameter values, we:\n\nPerform \\text{k}-fold cross-validation on the training data.\nCalculate the average performance metric (e.g., mean squared error) across all folds.\nSelect the hyperparameter values that produced the best average performance.\n\nThis ensures that the hyperparameters we select are robust and generalissable to unseen data, rather than being overly influenced by the peculiarities of a single training set.\nK-fold cross-validation is the most frequently-used form of cross-validation, but several other types exist. Some of them are:\nLeave-one-out cross-validation (LOOCV) is an extreme case of \\text{k}-fold cross-validation where \\text{k} equals the number of data points. This method trains the model on all but one data point and validates on the left-out point, repeating this process for each data point. LOOCV provides an nearly unbiased estimate of model performance but can be computationally expensive for large datasets. It’s most often used for small datasets where maximising training data is important. The downside is that LOOCV can suffer from high variance, especially for noisy datasets.\nStratified cross-validation ensures each fold maintains the same proportion of samples for each class as in the complete dataset. It useful for imbalanced datasets or when dealing with categorical outcomes. By preserving the class distribution in each fold, stratified cross-validation provides a more representative evaluation of model performance across all classes. Implementing stratification can be complex for multi-class problems or continuous outcomes.\nHoldout validation is the simplest form of cross-validation. The dataset is split into a training set and a test set. Typically, about 70-80% of the data is used for training and the balance is reserved for testing. The model is trained on the training set and then evaluated on the held-out test set. It is computationally efficient and provides a quick estimate of model performance but it has several limitations. Firstly, because it doesn’t make full use of the available data for training, it can be an issue for smaller datasets. Secondly, the results can be highly dependent on the particular split chosen, leading to high variance in performance estimates. This is especially true for smaller datasets or when the split doesn’t represent the overall data distribution well. But holdout validation remains useful for large datasets or as a quick initial assessment before applying more complex cross-validation techniques.\nThe examples will show \\text{k}-fold cross validation, but you can easily adapt the code to use other cross-validation methods.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regularisation Techniques</span>"
    ]
  },
  {
    "objectID": "regularisation.html#sec-r-function",
    "href": "regularisation.html#sec-r-function",
    "title": "8  Regularisation Techniques",
    "section": "\n8.5 R Function",
    "text": "8.5 R Function\nIn R, the glmnet package provides functions for fitting regularised linear models. The cv.glmnet() function performs cross-validated regularisation path selection for the Elastic Net, Lasso, and Ridge Regression models.\n\ncv.glmnet(x, y, alpha = 1, lambda = NULL, nfolds = 10,\n          standardize = TRUE)\n\nThe function takes the following arguments:\n\n\nx: A matrix of predictors.\n\ny: A matrix of response variables (but read the help file as this varies depending on the data type).\n\nalpha: The mixing parameter for the Elastic Net penalty. When alpha = 0, the model is a Ridge Regression. When alpha = 1, the model is a Lasso Regression. The default value is alpha = 1.\n\nlambda: A vector of regularisation parameters. The function fits a model for each value of lambda and selects the best one based on cross-validation. The default is lambda = NULL, which means the function will generate a sequence of 100 values between 10^-2 and 10^2.\n\nnfolds: The number of folds in the cross-validation. The default is nfolds = 10.\n\nstandardize: A logical value indicating whether the predictors should be standardised. The default is standardize = TRUE.\n\nIt is not clearly documented in the function’s help file, but the ‘glm’ in the function name indicates that the function fits a generalised linear model. This implies ‘gaussian,’ ‘binomial,’ ‘poisson,’ ‘multinomial,’ ‘cox,’ and ‘mgaussian’ families are supported, which can be supplied via the family argument to the function. The ‘net’ part of the name indicates that the function fits an Elastic Net, thus allowing choose between Lasso and Ridge by setting alpha to 1 or 0 (or something in-between). The ‘cv’ part of the name indicates that the function performs cross-validation.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regularisation Techniques</span>"
    ]
  },
  {
    "objectID": "regularisation.html#example-1-ridge-regression",
    "href": "regularisation.html#example-1-ridge-regression",
    "title": "8  Regularisation Techniques",
    "section": "\n8.6 Example 1: Ridge Regression",
    "text": "8.6 Example 1: Ridge Regression\nThe data I use here should be well-known by now. They are the same seaweed dataset used throughout Chapter 5. I will use Ridge Regression to predict the response variable Y using the predictors annMean, augMean, augSD, febSD, and febRange.\nFirst, I will read in the data and prepare them in the format required by cv.glmnet(). This involves standardising the response variable and predictors and converting them to matrices. I specify the range of \\lambda values to try and set up 10-fold cross-validation. I then fit the model and plot the results of the cross-validation.\n\n# Ridge Regression with Cross-Validation\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Load necessary libraries\nlibrary(glmnet)\nlibrary(tidyverse)\n\n# Read the data\nsw &lt;- read.csv(\"data/spp_df2.csv\")\n\n# Standardise the response variable and present as a matrix\ny &lt;- sw |&gt;\n  select(Y) |&gt;\n  scale(center = TRUE, scale = FALSE) |&gt;\n  as.matrix()\n\n# Provide the predictors as a matrix\nX &lt;- sw |&gt;\n  select(-X, -dist, -bio, -Y, -Y1, -Y2) |&gt;\n  as.matrix()\n\n# Set up lambda sequence\nlambdas_to_try &lt;- 10 ^ seq(-3, 5, length.out = 100)\n\n# Perform 10-fold cross-validation\nridge_cv &lt;- cv.glmnet(X, y, alpha = 0, lambda = lambdas_to_try,\n  standardize = TRUE, nfolds = 10)\n\n\n# Plot cross-validation results (ggplot shown)\nplot(ridge_cv)\n\n\n\n\n\n\n\n\nFigure 8.1: Cross-validation statistics for the Ridge Regression approach applied to the seaweed data.\n\n\n\n\nFigure 8.1, generated from the cv.glmnet() object, illustrates the relationship between the regularisation parameter \\lambda and the model’s cross-validation performance. The y-axis represents the mean squared error (MSE) from cross-validation, whilst the x-axis shows the log(\\lambda) values tested. Red dots indicate the mean MSE for each \\lambda, with error bars showing ±1 standard error. Two vertical dashed lines highlight important \\lambda values: \\lambda_{min}, which minimises the mean MSE, and \\lambda_{1se}, the largest \\lambda within one standard error of the minimum MSE. One may select the optimal \\lambda using either the \\lambda_{min} or the \\lambda_{1se} rule, accessible via cv.glmnet_object$lambda.min and cv.glmnet_object$lambda.1se, respectively. To utilise the chosen \\lambda, one refits the model using glmnet() and extract the coefficients.\nFor performance evaluation, one can calculate the sum of squared residuals (SSR) as the sum of squared differences between observed and predicted values, and the R-squared value as the square of the correlation between observed and predicted values, representing the proportion of variance in the dependent variable that is predictable from the independent variable(s).\nThe results show that the model explains 67.07% of the variance in the response variable:\n\n# Fit models and calculate performance metrics\nfit_model_and_calculate_metrics &lt;- function(X, y, lambda) {\n  model &lt;- glmnet(X, y, alpha = 0, lambda = lambda,\n                  standardize = TRUE)\n  y_hat &lt;- predict(model, X)\n  ssr &lt;- sum((y - y_hat) ^ 2)\n  rsq &lt;- cor(y, y_hat) ^ 2\n  list(model = model, ssr = ssr, rsq = rsq)\n}\n\n# Best cross-validated lambda\nlambda_cv &lt;- ridge_cv$lambda.min\nmod_cv &lt;- fit_model_and_calculate_metrics(X, y, lambda_cv)\n\n# Print results\nmod_cv\n\n$model\n\nCall:  glmnet(x = X, y = y, alpha = 0, lambda = lambda, standardize = TRUE) \n\n  Df  %Dev Lambda\n1  5 67.06  0.001\n\n$ssr\n[1] 5.321994\n\n$rsq\n         s0\nY 0.6706681\n\n\nAs already indicated, an alternative to using lambda.min for selecting the optimal \\lambda value is to use the 1 SE rule, which is contained in the attribute lambda.1se. This reduces the risk of overfitting as it tends to select a simpler model. We can use this value to refit the model and extract the coefficients, as before.\nAIC and BIC can also be used to select suitable models. These information criteria penalise the model for the number of parameters used, providing a balance between model complexity and goodness of fit. The calculate_ic() function below calculates the AIC and BIC for a given model and returns the results in a list. We can then use this function to calculate the AIC and BIC for each model fit with each \\lambda in lambdas_to_try:\n\n# Calculate AIC and BIC\ncalculate_ic &lt;- function(X, y, lambda) {\n  model &lt;- glmnet(X, y, alpha = 0, lambda = lambda,\n                  standardize = TRUE)\n  betas &lt;- as.vector(coef(model)[-1])\n  resid &lt;- y - (scale(X) %*% betas)\n  H &lt;- scale(X) %*%\n    solve(t(scale(X)) %*% scale(X) + lambda *\n            diag(ncol(X))) %*% t(scale(X))\n  df &lt;- sum(diag(H))\n  log_resid_ss &lt;- log(sum(resid ^ 2))\n  aic &lt;- nrow(X) * log_resid_ss + 2 * df\n  bic &lt;- nrow(X) * log_resid_ss + log(nrow(X)) * df\n  list(aic = aic, bic = bic)\n}\n\nic_results &lt;- map(lambdas_to_try, ~ calculate_ic(X, y, .x)) |&gt;\n  transpose()\n\nA plot of the change in the information criteria with log(\\lambda) is shown in Figure 8.2. The optimal \\lambda values according to both AIC and BIC can then be used to refit the model and arrive at the coefficients of interest.\n\n# Plot information criteria\nplot_ic &lt;- function(lambdas, ic_results) {\n  df &lt;- data.frame(lambda = log(lambdas),\n                   aic = unlist(ic_results$aic),\n                   bic = unlist(ic_results$bic))\n\n  df_long &lt;- pivot_longer(df, cols = c(aic, bic),\n                          names_to = \"criterion\",\n                          values_to = \"value\")\n\n  ggplot(df_long, aes(x = lambda, y = value, color = criterion)) +\n    geom_line() +\n    scale_color_manual(values = c(\"aic\" = \"orange\", \"bic\" = \"skyblue3\"),\n                       labels = c(\"aic\" = \"AIC\", \"bic\" = \"BIC\")) +\n    labs(x = \"log(lambda)\",\n         y = \"Information Criterion\", color = \"Criterion\") +\n    theme_minimal() +\n    theme(legend.position = \"top\",\n          legend.direction = \"horizontal\",\n          legend.box = \"horizontal\")\n}\n\nplot_ic(lambdas_to_try, ic_results)\n\n\n\n\n\n\nFigure 8.2: Plot of information criteria for best model fit selected through Ridge Regression.\n\n\n\n\nNow we find the \\lambda values that minimise the AIC and BIC, and refit the models using these values. It so happens that both AIC and BIC selects the same \\lambda values:\n\n# Optimal lambdas according to both criteria\nlambda_aic &lt;- lambdas_to_try[which.min(ic_results$aic)]\nlambda_bic &lt;- lambdas_to_try[which.min(ic_results$bic)]\n\n# Fit final models using the optimal lambdas\nmod_aic &lt;- fit_model_and_calculate_metrics(X, y, lambda_aic)\nmod_bic &lt;- fit_model_and_calculate_metrics(X, y, lambda_bic)\n\nFor interest sake, we may also produce a plot that traces the coefficients of the model as \\lambda changes. This can help us understand how the coefficients shrink as \\lambda increases, and which variables are most important in the model. The plot below shows the Ridge Regression coefficients path for each variable in the model (Figure 8.3).\n\n# Plot the Ridge Regression coefficients path\nres &lt;- glmnet(X, y, alpha = 0, lambda = lambdas_to_try,\n              standardize = FALSE)\nplot(res, xvar = \"lambda\")\nlegend(\"topright\", lwd = 1, col = 1:6,\n       legend = colnames(X), cex = 0.7)\n\n\n\n\n\n\nFigure 8.3: Plot of the Ridge Regression coefficients paths.\n\n\n\n\nSo, after having demonstrated the different methods for selecting the optimal \\lambda value, we can now summarise the results:\n\n\n[1] \"CV Lambda: 0.001\"\n\n\n[1] \"AIC Lambda: 0.3854\"\n\n\n[1] \"BIC Lambda: 0.3854\"\n\n\n[1] \"CV R-squared: 0.6707\"\n\n\n[1] \"AIC R-squared: 0.6025\"\n\n\n[1] \"BIC R-squared: 0.6025\"\n\n\nNow we can extract the coefficient produced from models selected via the AIC and CV methods.\n\nres_aic &lt;- glmnet(X, y, alpha = 0, lambda = lambda_aic,\n                  standardize = FALSE)\nres_aic\n\n\nCall:  glmnet(x = X, y = y, alpha = 0, lambda = lambda_aic, standardize = FALSE) \n\n  Df  %Dev Lambda\n1  5 13.46 0.3854\n\ncoef(res_aic)\n\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s0\n(Intercept) -0.021327121\naugMean      0.009856026\nfebRange     0.007118466\nfebSD       -0.001074341\naugSD        0.010696102\nannMean      0.008114467\n\n\n\nres_cv &lt;- glmnet(X, y, alpha = 0, lambda = lambda_cv,\n                 standardize = FALSE)\nres_cv\n\n\nCall:  glmnet(x = X, y = y, alpha = 0, lambda = lambda_cv, standardize = FALSE) \n\n  Df  %Dev Lambda\n1  5 66.77  0.001\n\ncoef(res_cv)\n\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) -0.12384440\naugMean      0.22200994\nfebRange     0.04287655\nfebSD       -0.03446642\naugSD        0.02699458\nannMean      0.04324177\n\n\nRidge regression adds a penalty to the size of the coefficients, resulting in their shrinkage towards zero. This penalty affects all coefficients simultaneously. Notably, there is a difference in the model fit obtained using \\lambda_{AIC} (which is larger) and \\lambda_{min} (which is smaller). The former model explains 55.69% of the variance, compared to \\lambda_{min}, which explains 63.37% of the variance.\nAlthough shrinkage affects the absolute magnitude of the coefficients (they are biased estimates of the true relationships between the predictors and the response variable), the coefficients in Ridge Regression retain their general meaning—they still represent the change in the response variable associated with a one-unit change in the predictor variable, holding other predictors constant. While the absolute values of the coefficients may be biased due to regularisation, the relative importance of the predictors can still be interpreted. The magnitude of the coefficients can indicate the relative influence of each predictor on the response variable, even if their exact values are reduced.\nImportantly, the predictive ability of the model can improve with shrunk coefficients because Ridge Regression reduces overfitting and enhances the model’s generalisability to new, unseen data. By stabilising the coefficient estimates, the model often achieves better performance on validation and test datasets, which is important should robust predictive analytics be the goal.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regularisation Techniques</span>"
    ]
  },
  {
    "objectID": "regularisation.html#example-2-lasso-regression",
    "href": "regularisation.html#example-2-lasso-regression",
    "title": "8  Regularisation Techniques",
    "section": "\n8.7 Example 2: Lasso Regression",
    "text": "8.7 Example 2: Lasso Regression\nDoing a Lasso Regression is easy. Simply change the alpha parameter to 1 in the glmnet function. The rest of the code remains the same. I’ll show only the final output of this analysis to avoid repetition.\n\n# Print results\nmod_cv\n\n$model\n\nCall:  glmnet(x = X, y = y, alpha = 1, lambda = lambda, standardize = TRUE) \n\n  Df %Dev Lambda\n1  5   67  0.001\n\n$ssr\n[1] 5.332835\n\n$rsq\n         s0\nY 0.6701255\n\ncoef(mod_cv$model)\n\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) -0.12886019\naugMean      0.26097296\nfebRange     0.03431981\nfebSD       -0.02497532\naugSD        0.02441380\nannMean      0.02021480\n\n# Print results\nprint(paste(\"CV Lambda:\", lambda_cv))\n\n[1] \"CV Lambda: 0.001\"\n\nprint(paste(\"CV R-squared:\", round(mod_cv$rsq, 4)))\n\n[1] \"CV R-squared: 0.6701\"\n\n\n\n\n\n\n\n\n\nFigure 8.4: Cross-validation statistics for Lasso Regression applied to the seaweed data.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.5: Plot of the Lasso Regression coefficients paths.\n\n\n\n\nLasso regression incorporates an L1 penalty term in its cost function, which shrinks some coefficient estimates to exactly zero. By reducing certain coefficients to zero, Lasso effectively eliminates those predictors from the model, which achieves automatic variable selection:\n\nWhen \\lambda is small, the penalty is minimal, and Lasso behaves similarly to ordinary least squares regression, retaining most coefficients.\nWhen \\lambda is large, the penalty increases, causing more coefficients to shrink to zero. This results in a sparser model where only the most significant predictors have non-zero coefficients.\n\nIn our example (Figure 8.4), we see at \\lambda_{min}, the number of non-zero coefficients is minimised—all five coefficients remain. At \\lambda_{1se}, the number of non-zero coefficients decreases to four. Consequently, for higher values of \\lambda, more predictors will have coefficients exactly equal to zero. This is also seen in Figure 8.4. In Figure 8.5 we can see that the first predictor to reach zero is annMean, then febSD, febRange, and so forth. The implication is that they are excluded from the model and the model is simplified. This leads to several benefits: reduced multicollinearity, improved interpretability, and better generalisation to new data.\nCoefficients that remain non-zero after Lasso regularisation are considered more important predictors. Those remaining coefficients can be interpreted similarly to standard linear regression: as the expected change in the response variable for a one-unit change in the predictor, holding other predictors constant.\nThe \\lambda parameter controls the amount of bias introduced. While Lasso can produce biased estimates, it reduces variance, often resulting in a model that performs better on new, unseen data. This trade-off enhances predictive accuracy but means that the exact coefficient values may not represent the true underlying relationships as closely as those in an unregularised model.\nDespite regularisation, the relative magnitudes of the non-zero coefficients provide a glimpse into predictor importance. Larger absolute values of coefficients indicate stronger relationships with the response variable. The exact numerical values are biased, but ranking predictors by their coefficients still offers useful insight into their relative importance.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regularisation Techniques</span>"
    ]
  },
  {
    "objectID": "regularisation.html#example-3-elastic-net-regression",
    "href": "regularisation.html#example-3-elastic-net-regression",
    "title": "8  Regularisation Techniques",
    "section": "\n8.8 Example 3: Elastic Net Regression",
    "text": "8.8 Example 3: Elastic Net Regression\nIn this last example we’ll look at Elastic Net Regression, which combines the L1 and L2 penalties of Lasso and Ridge Regression. There are now two parameters to optimise: \\alpha and \\lambda. The \\alpha parameter controls the mix between the L1 and L2 penalties, with \\alpha = 0 behaving like Ridge Regression and \\alpha = 1 behaving like Lasso Regression. For \\alpha values between 0 and 1, Elastic Net combines the strengths of both Lasso and Ridge Regression. Optimisation of \\alpha and \\lambda is also done using cross-validation. In practise, the steps are:\n\nSet up a grid of \\alpha values (from 0 to 1) and \\lambda values to try.\nPerforms cross-validation for each combination of \\alpha and \\lambda using cv.glmnet().\nSelect the best \\alpha and \\lambda combination based on the minimum mean cross-validated error.\nFit the final model using the best \\alpha and \\lambda.\nCalculate the performance metrics.\nFor the Elastic Net model with the best alphaCreate plots similar to those in the Ridge and Lasso examples.\n\n\n# Define the range of alpha values to try\nalphas_to_try &lt;- seq(0, 1, by = 0.1)\n\n# Define the range of lambda values to try\nlambdas_to_try &lt;- 10^seq(-3, 3, length.out = 100)\n\n# Perform grid search with cross-validation\ncv_results &lt;- lapply(alphas_to_try, function(a) {\n  cv.glmnet(X, y, alpha = a, lambda = lambdas_to_try,\n            standardize = TRUE, nfolds = 10)\n})\n\n# Find the best alpha and lambda\nbest_result &lt;- which.min(sapply(cv_results, function(x) min(x$cvm)))\nbest_alpha &lt;- alphas_to_try[best_result]\nbest_lambda &lt;- cv_results[[best_result]]$lambda.min\n\n# Fit the final model with the best parameters\nfinal_model &lt;- glmnet(X, y, alpha = best_alpha,\n                      lambda = best_lambda,\n                      standardize = TRUE)\n\n# Calculate performance metrics\ny_hat &lt;- predict(final_model, X)\nssr &lt;- sum((y - y_hat) ^ 2)\nrsq &lt;- cor(y, y_hat) ^ 2\n\n\n\n[1] \"Best Alpha: 0.3\"\n\n\n[1] \"Best Lambda: 0.001\"\n\n\n[1] \"R-squared: 0.6706\"\n\n\n\n\n\n\n\n\n\nFigure 8.6: Cross-validation statistics for Elastic Net Regression applied to the seaweed data.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.7: Plot of the Elastic Net Regression coefficients paths.\n\n\n\n\nThe model coefficients are:\n\ncoef(cv_results[[best_result]])\n\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept) -0.117063010\naugMean      0.240447330\nfebRange     0.015404286\nfebSD       -0.007224065\naugSD        0.014882111\nannMean      0.026504736\n\n\nThe interpretation of coefficients in Elastic Net is a blend of Ridge and Lasso. Some coefficients may be shrunk to zero (feature selection), while others are shrunk but remain non-zero (magnitude reduction). The non-zero coefficients retain their general meaning with an emphasis on their relative importance.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regularisation Techniques</span>"
    ]
  },
  {
    "objectID": "regularisation.html#theory-driven-and-data-driven-variable-selection",
    "href": "regularisation.html#theory-driven-and-data-driven-variable-selection",
    "title": "8  Regularisation Techniques",
    "section": "\n8.9 Theory-Driven and Data-Driven Variable Selection",
    "text": "8.9 Theory-Driven and Data-Driven Variable Selection\nThe choice between theory-driven and data- or statistics-driven variable selection represents an important consideration that can greatly influence model interpretation, its predictive power, and your value as an ecologist. This decision reflects a broader tension in scientific methodology between deductive and inductive reasoning. Each offers advantages and limitations that you should be aware of as an ecologist.\nTheory-driven variable selection is core to the scientific method. It relies on a priori knowledge and established ecological theories (as far as they exist in ecology!) to guide your choice of predictors in a model. This aligns closely with the hypothetico-deductive method, where we formulate hypotheses based on existing knowledge and subsequently test these against the data we collect. The strength of this method lies in its interpretability. Models built on theoretical foundations often contribute directly to testing and refining ecological hypotheses. By focusing on variables with known or hypothesised relationships (with mechanisms often rooted in ecophysiological or ecological inquiries), the theory-driven hypothetico-deductive method should lead to more parsimonious models that are less prone to overfitting and more reflecting of reality.\nTheory-driven selection is not without its drawbacks. It requires that we have a good grasp of the mechanism underlying our favourite ecological system. This is not always the case in complex systems where the underlying mechanisms are not well understood. Theory-driven selection can then lead to the exclusion of important variables that were not initially hypothesised and it can limit the scope of the analysis and potentially overlook significant relationships in the data.\nA naive young ecologist might place undue value on the notion that their hard work collecting diverse data and developing hypotheses should all be reflected in their final model. This can lead to confirmation bias, where one is more likely to select variables that support our hypotheses and ignore those that do not. This bias can compromise the objectivity of the model and lead to skewed results that do not accurately represent the underlying ecological processes.\nMoreover, the insistence on including all variables that were initially considered important can result in overly complex models. Such models can be difficult to interpret and may suffer from overfitting, where the model captures noise rather than the true signal in the data. Overfitted models perform well on the data we collected but poorly on new, unseen data. The consequence is a loss of predictive power and generalisability.\nAnother weakness of theory-driven variable selection is that the reliance on existing theories or the novel, promising hypothsis of the day may lead us to overlook important but unexpected relationships in the data. In complex ecological systems, where our theoretical understanding may be incomplete, some variables could be missed entirely—these might in fact hold the key to the real cause of the ecological patterns we observe. This limitation becomes concerning when studying ecosystems or phenomena that are not well understood or are undergoing rapid changes, such as those affected by climate change or novel anthropogenic pressures.\nOn the other hand, data-driven approaches, including regularisation techniques, VIF, and forward model variable selection (Chapter 5), allow the data itself to guide variable selection. These methods are increasingly used in today’s era of high-dimensional datasets common in modern ecological research. The primary advantage of data-driven selection lies in its potential for discovery—it can uncover unexpected relationships and generate new hypotheses, which is valuable in complex ecological systems where interactions may not be immediately apparent.\nData-driven methods are well-suited for handling the complexity often encountered in environmental and ecological datasets, where numerous potential predictors may co-occur and interact. They offer a degree of objectivity, reducing the potential for our personal biases in variable selection. But these approaches are not without risks. There’s a danger of identifying relationships that are statistically significant but ecologically meaningless—we refer to this as spurious correlations (e.g. the belief that consuming carrots significantly improves our night vision). Moreover, models with many variables can present significant interpretability challenges, especially when complex interactions are present. This can make it difficult to extract meaningful (plausible) insights from the model and to communicate results to a broader audience.\nIn practice, the most robust approach to selecting which of the multitude of variables to include in our model often involves a thoughtful combination of theory-driven and data-driven methods. Well-trained ecologists should start with theory-driven variable selection to identify the core predictors based on established ecological principles. We could then employ regularisation techniques to explore additional variables and potential interactions, and use the results to refine our models and generate new hypotheses for future research.\nThis hybrid approach combines the strengths of both methods. It allows for rigorous hypothesis testing while remaining open to unanticipated and new insights from the data. In ecology, where systems are often characterised by complex, non-linear relationships and interactions that may vary across spatial and temporal scales, this two-pronged approach offers distinct benefits.\nConsider how these methods complement theoretical knowledge. Use variable selection methods as tools for prediction, and to assit generating new insights and hypotheses about ecosystems. The choice between theory-driven and data-driven variable selection is not a binary one, but rather a spectrum of approaches.",
    "crumbs": [
      "Parametric Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regularisation Techniques</span>"
    ]
  },
  {
    "objectID": "assumption_tests.html",
    "href": "assumption_tests.html",
    "title": "9  Testing Assumptions",
    "section": "",
    "text": "9.1 Tests for Normality\nIn biological research, it is often important to determine whether a normal distribution adequately represents the underlying population distribution. This assessment is relevant when applying statistical procedures that rely on the assumption of normality, such as many of those discussed in earlier chapters. However, note that not all biological data conform to a normal distribution. In fact, many natural processes will result in non-normal data.\nAssessing normality allows us to make informed decisions about appropriate statistical methods. If the data reasonably approximates a normal distribution, we can confidently apply parametric tests using probability calculations based on the normal curve. Conversely, if the data significantly deviates from normality, alternative non-parametric approaches may be more suitable.\nBeyond simply validating statistical assumptions, examining the distribution of biological data can offer valuable insights into the underlying mechanisms and processes shaping the population. Identifying deviations from normality can challenge existing hypotheses, reveal hidden patterns, or suggest the influence of unanticipated factors. Therefore, normality tests are not only a technical requirement but they may also offer a tool for understanding the biological phenomena under investigation.\nIn this section, we will explore a range of graphical methods (e.g., histograms, Q-Q plots) and statistical tests (e.g., Shapiro-Wilk test, Kolmogorov-Smirnov test) to assess the goodness-of-fit of a normal distribution to our data.",
    "crumbs": [
      "Non-Parametric Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Testing Assumptions</span>"
    ]
  },
  {
    "objectID": "assumption_tests.html#tests-for-normality",
    "href": "assumption_tests.html#tests-for-normality",
    "title": "9  Testing Assumptions",
    "section": "",
    "text": "Shapiro-Wilk Test\n\n\nKolmogorov-Smirnov Test\n\n\nAnderson-Darling Test\n\n\nLilliefors Test\n\n\nJarque-Bera Test",
    "crumbs": [
      "Non-Parametric Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Testing Assumptions</span>"
    ]
  },
  {
    "objectID": "assumption_tests.html#tests-for-homoscedasticity",
    "href": "assumption_tests.html#tests-for-homoscedasticity",
    "title": "9  Testing Assumptions",
    "section": "9.2 Tests for Homoscedasticity",
    "text": "9.2 Tests for Homoscedasticity\n\nBreusch-Pagan Test\n\n\nWhite’s Test\n\n\nLevene’s Test\n\n\nBartlett’s Test\n\n\nFligner-Killeen Test",
    "crumbs": [
      "Non-Parametric Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Testing Assumptions</span>"
    ]
  },
  {
    "objectID": "quantile_regression.html",
    "href": "quantile_regression.html",
    "title": "\n10  Quantile Regression\n",
    "section": "",
    "text": "10.1 Understanding the Challenge",
    "crumbs": [
      "Non-Parametric Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantile Regression</span>"
    ]
  },
  {
    "objectID": "quantile_regression.html#understanding-the-challenge",
    "href": "quantile_regression.html#understanding-the-challenge",
    "title": "\n10  Quantile Regression\n",
    "section": "",
    "text": "Standard Quantile Regression: Models the relationship between predictor variables and specific quantiles of the response variable.\n\nYour Objective: To understand how different quantiles of the predictor variable influence the response variable.",
    "crumbs": [
      "Non-Parametric Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantile Regression</span>"
    ]
  },
  {
    "objectID": "quantile_regression.html#possible-approaches",
    "href": "quantile_regression.html#possible-approaches",
    "title": "\n10  Quantile Regression\n",
    "section": "\n10.2 Possible Approaches",
    "text": "10.2 Possible Approaches\n\n\nQuantile-on-Quantile Regression (QQR):\n\n\nConcept:\n\nQQR extends traditional quantile regression by examining how quantiles of the predictor variable affect quantiles of the response variable.\nIt provides a more comprehensive picture of the dependence structure between the two variables across their entire distributions.\n\n\n\nImplementation Steps:\n\n\nEstimate Conditional Quantiles of the Predictor Variable:\n\nFor each quantile \\tau of the predictor variable (wind stress curl), calculate the quantile values.\n\n\n\nModel Response Quantiles Conditional on Predictor Quantiles:\n\nFor each quantile \\theta of the response variable (SST), model it as a function of the predictor variable’s quantiles.\nThe model can be specified as: Q_{\\theta}(SST \\mid Q_{\\tau}(\\text{Wind Stress Curl})) = \\beta_0(\\theta, \\tau) + \\beta_1(\\theta, \\tau) \\times Q_{\\tau}(\\text{Wind Stress Curl})\n\n\n\n\nInterpretation:\n\nThe coefficients \\beta_1(\\theta, \\tau) show how the \\tau-th quantile of the wind stress curl affects the \\theta-th quantile of SST.\nBy varying \\theta and \\tau, you can map out the entire dependence structure.\n\n\n\n\n\nAdvantages:\n\nCaptures non-linear and asymmetric relationships between the variables.\nAllows for interactions between different parts of the distributions.\n\n\n\nConsiderations:\n\n\nComputational Complexity: Estimating the model for all combinations of \\theta and \\tau can be computationally intensive.\n\nData Requirements: Requires a large dataset to obtain reliable estimates across quantiles.\n\n\n\n\n\nBinning the Predictor Variable:\n\n\nConcept:\n\nDivide the predictor variable into bins based on its quantiles (e.g., quartiles or deciles).\nWithin each bin, analyse the relationship between the predictor and response variable.\n\n\n\nImplementation Steps:\n\n\nQuantile Binning:\n\nDivide wind stress curl data into quantile-based bins.\n\n\n\nWithin-Bin Analysis:\n\nFor each bin, perform regression analysis to see how variations within that bin affect SST.\n\n\n\nComparative Analysis:\n\nCompare the regression coefficients across bins to see if the effect of wind stress curl on SST changes across its distribution.\n\n\n\n\n\nAdvantages:\n\nSimpler to implement and interpret.\nHighlights non-linear relationships and threshold effects.\n\n\n\nConsiderations:\n\n\nLoss of Information: Binning can lead to loss of information due to grouping continuous data.\n\nBoundary Issues: Care must be taken at the edges of bins to ensure continuity.\n\n\n\n\n\nInteraction Terms and Non-Linear Models:\n\n\nConcept:\n\nIntroduce interaction terms or non-linear transformations to allow the effect of wind stress curl to vary across its own distribution.\n\n\n\nImplementation Steps:\n\n\nCreate Interaction Terms:\n\nInclude terms like \\text{Wind Stress Curl} \\times I(\\text{Wind Stress Curl} &gt; Q_{\\tau}), where I is an indicator function, and Q_{\\tau} is the \\tau-th quantile.\n\n\n\nNon-Linear Models:\n\nUse models like Generalised Additive Models (GAMs) to allow for non-linear relationships.\n\n\n\nQuantile Regression with Interactions:\n\nCombine quantile regression with interaction terms to see how the effect changes at different levels of the predictor variable.\n\n\n\n\n\nAdvantages:\n\nFlexible modelling of relationships.\nCan capture threshold effects and non-linearities.\n\n\n\nConsiderations:\n\n\nModel Complexity: More complex models require careful interpretation and validation.\n\nOverfitting Risk: Including too many interactions can lead to overfitting, especially with limited data.\n\n\n\n\n\nCopula-Based Approaches:\n\n\nConcept:\n\nUse copulas to model the joint distribution of the predictor and response variables, allowing for dependence in their marginal distributions.\n\n\n\nImplementation Steps:\n\n\nEstimate Marginal Distributions:\n\nDetermine the marginal distributions of wind stress curl and SST.\n\n\n\nSelect Appropriate Copula:\n\nChoose a copula function that captures the dependence structure (e.g., Clayton, Gumbel).\n\n\n\nModel the Joint Distribution:\n\nUse the copula to model the joint behaviour, focusing on the tails of the distributions.\n\n\n\n\n\nAdvantages:\n\nCaptures complex dependence structures.\nParticularly useful for modelling tail dependencies.\n\n\n\nConsiderations:\n\n\nStatistical Expertise Required: Copula models can be mathematically complex.\n\nData Demands: Requires large datasets for reliable estimation.",
    "crumbs": [
      "Non-Parametric Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantile Regression</span>"
    ]
  },
  {
    "objectID": "quantile_regression.html#applying-quantile-on-quantile-regression-to-the-upwelling-problem",
    "href": "quantile_regression.html#applying-quantile-on-quantile-regression-to-the-upwelling-problem",
    "title": "\n10  Quantile Regression\n",
    "section": "\n10.3 Applying Quantile-on-Quantile Regression to the Upwelling Problem",
    "text": "10.3 Applying Quantile-on-Quantile Regression to the Upwelling Problem\nLet’s focus on Quantile-on-Quantile Regression as it seems most relevant to our situation and requirement.\n\n\nModel Estimation:\n\nFor each chosen quantile \\tau of wind stress curl (e.g., 10th, 25th, 50th, 75th, 90th percentiles), extract the corresponding values.\nFor each chosen quantile \\theta of SST, perform quantile regression using the extracted wind stress curl quantile as the predictor.\n\n\n\nAnalysis:\n\nExamine the estimated coefficients \\beta_1(\\theta, \\tau) across different combinations of \\theta and \\tau.\nIdentify patterns where certain quantiles of wind stress curl have a stronger or weaker effect on specific quantiles of SST.\nFor example, you may find that extreme high values (e.g., 90th percentile) of wind stress curl significantly affect the lower quantiles (e.g., 10th percentile) of SST, indicating strong upwelling events.\n\n\n\nVisualisation:\n\nCreate heatmaps or surface plots to visualize the coefficient values across the (\\theta, \\tau) grid.\nPlot the estimated relationships to interpret the effects intuitively.\n\n\n\nRobustness Checks:\n\nPerform bootstrapping to assess the stability of your estimates.\nTest for statistical significance of the coefficients.",
    "crumbs": [
      "Non-Parametric Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantile Regression</span>"
    ]
  },
  {
    "objectID": "quantile_regression.html#alternative-approaches",
    "href": "quantile_regression.html#alternative-approaches",
    "title": "\n10  Quantile Regression\n",
    "section": "\n10.4 Alternative Approaches",
    "text": "10.4 Alternative Approaches\nIf quantile-on-quantile regression proves too complex or data-intensive, consider the following simplified methods:\n\n\nConditional Mean Regression with Binned Predictors:\n\nBin wind stress curl into quantiles and use these as categorical predictors in a standard regression model.\nThis approach simplifies the analysis while still providing insights into how different levels of wind stress curl affect SST.\n\n\n\nThreshold Regression Models:\n\nUse models that allow for different regression regimes based on the value of the predictor variable.\nFor example, a piecewise linear model where the slope changes when wind stress curl exceeds certain quantiles.",
    "crumbs": [
      "Non-Parametric Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantile Regression</span>"
    ]
  },
  {
    "objectID": "quantile_regression.html#recommendations",
    "href": "quantile_regression.html#recommendations",
    "title": "\n10  Quantile Regression\n",
    "section": "\n10.5 Recommendations",
    "text": "10.5 Recommendations\n\n\nData Exploration:\n\nBefore modeling, thoroughly explore your data to understand the distributions and potential relationships.\nUse scatter plots, quantile plots, and correlation analyses.\n\n\n\nModel Selection:\n\nStart with simpler models to establish baseline relationships.\nGradually incorporate complexity as needed, based on initial findings.\n\n\n\nValidation:\n\nUse cross-validation techniques to assess the predictive performance of your models.\nCompare models using appropriate metrics (e.g., Akaike Information Criterion for model selection).\n\n\n\nExpert Consultation:\n\nCollaborate with a statistician experienced in advanced regression techniques.\nThis can help ensure that your models are correctly specified and interpreted.",
    "crumbs": [
      "Non-Parametric Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantile Regression</span>"
    ]
  },
  {
    "objectID": "quantile_regression.html#conclusion",
    "href": "quantile_regression.html#conclusion",
    "title": "\n10  Quantile Regression\n",
    "section": "\n10.6 Conclusion",
    "text": "10.6 Conclusion\nStudying which quantiles of the predictor variable affect the response variable adds a layer of complexity but can yield valuable insights into the dynamics of upwelling events. Quantile-on-Quantile Regression offers a direct method to explore these relationships comprehensively. However, it’s essential to balance methodological rigour with practical considerations like data availability and computational feasibility.\nBy carefully selecting your approach and thoroughly validating your models, you can enhance your understanding of how wind stress curl influences SST and upwelling metrics across different conditions. This, in turn, can contribute significantly to the field by providing a more detailed characterization of upwelling dynamics and their drivers.",
    "crumbs": [
      "Non-Parametric Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantile Regression</span>"
    ]
  },
  {
    "objectID": "quantile_regression.html#additional-resources",
    "href": "quantile_regression.html#additional-resources",
    "title": "\n10  Quantile Regression\n",
    "section": "\n10.7 Additional Resources",
    "text": "10.7 Additional Resources\n\n\nLiterature on Quantile-on-Quantile Regression:\n\nHao, L., & Naiman, D. Q. (2007). Quantile Regression. Sage Publications.\nSim, N., Zhou, H., & Goh, T. (2019). Quantile-on-Quantile Regression Approach to Analyzing the Impact of Oil Price Changes on Stock Returns. Energy Economics, 80, 297-309.\n\n\n\nStatistical Software:\n\n\nR Packages:\n\n\nquantreg: For quantile regression.\n\nqgam: For quantile generalized additive models.\nCustom scripts may be needed for quantile-on-quantile regression.",
    "crumbs": [
      "Non-Parametric Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantile Regression</span>"
    ]
  },
  {
    "objectID": "quantile_regression.html#final-thoughts",
    "href": "quantile_regression.html#final-thoughts",
    "title": "\n10  Quantile Regression\n",
    "section": "\n10.8 Final Thoughts",
    "text": "10.8 Final Thoughts\nYour inquiry demonstrates a deep engagement with the methodological aspects of your research. By extending your analysis to consider how different quantiles of wind stress curl affect SST and upwelling metrics, you are likely to uncover nuanced relationships that can advance understanding in this area. Don’t hesitate to reach out to experts in statistical modeling to support this aspect of your work.",
    "crumbs": [
      "Non-Parametric Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quantile Regression</span>"
    ]
  },
  {
    "objectID": "generalised_additive_models.html",
    "href": "generalised_additive_models.html",
    "title": "11  Generalised Additive Models",
    "section": "",
    "text": "GAMs utilise a sum of smooth functions, each of which may depend on different subsets of the predictors. This additive structure, with smooth functions modelling the nonlinear effects of different predictor variables, allows GAMs to capture complex, nonlinear relationships without the need for a single, global parametric form. GAMs are useful in areas where the data exhibit complex patterns that are not easily described by traditional parametric or even non-parametric models.\nUnlike polynomial regressions or specific nonlinear models that capture functional relationships with parameters directly linked to the system’s mechanics, GAMs do not necessarily provide parameters that correspond to a mechanistic understanding. Instead, they offer flexibility and robustness in modelling, making them suitable for a wide range of applications where the relationship dynamics are complex and not well-defined by simpler models.\nIn GAMs, the smooth functions are typically represented using regression splines (Figure A). Splines are piecewise polynomial functions that are flexible and can approximate complex nonlinear relationships. In GAMs, the smooth functions are estimated using various types of regression splines, such as Thin Plate Regression Splines, Cubic Regression Splines, and P-Splines (B-Splines). These spline functions are used to model the nonlinear effects of the predictor variables in a flexible and data-driven manner, without assuming any specific parametric form. A GAM can be expressed as:\nY_i = \\alpha + f_1(X_{i1}) + f_2(X_{i2}) + \\ldots + f_p(X_{ip}) + \\epsilon_i \\tag{11.1}\nWhere:\n\nY_i is the response variable for the i-th observation,\n\\alpha is the intercept,\nf_j(X_{ij}) are smooth functions of the predictor variables X_{ij} (for j = 1, 2, \\ldots, p), and\n\\epsilon_i is the error term for the i-th observation.\n\nThe degree of smoothness of the smooth functions f_j is typically chosen based on the data and the modelling objectives, often using cross-validation or other model selection techniques.",
    "crumbs": [
      "Semi-Parametric Methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalised Additive Models</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "\n12  Summary\n",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Graham MH (2003) Confronting multicollinearity in ecological multiple\nregression. Ecology 84:2809–2815.\n\n\nSmit A (2002) Nitrogen uptake by Gracilaria\ngracilis (Rhodophyta): Adaptations to a temporally\nvariable nitrogen environment. Bot Mar 45:196–209.\n\n\nSmit AJ, Bolton JJ, Anderson RJ (2017) Seaweeds in two oceans:\nBeta-diversity. Frontiers in Marine Science 4:404.\n\n\nUnderwood AJ (1997) Experiments in ecology: Their logical design and\ninterpretation using analysis of variance. Cambridge university press",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "part_A.html",
    "href": "part_A.html",
    "title": "Parametric Methods",
    "section": "",
    "text": "If the research question does not involve exploring the relationship between a response variable and predictor variables, then non-regression inferential statistical methods would be more appropriate. These include tests of means/medians, tests of proportions, correlation analysis, and nonparametric tests. These methods are suitable when the goal is to compare groups, assess central tendencies, test for differences, or measure the strength of association between two variables without explicitly modeling the relationship.",
    "crumbs": [
      "Parametric Methods"
    ]
  }
]