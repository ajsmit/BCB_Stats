# Multiple Linear Regression {#sec-multiple-linear-regression}

In @sec-simple-linear-regression we have seen how to model the relationship between two variables using simple linear regression (SLR). However, in ecosystems, the relationship between the response variable and the explanatory variables is more complex and in many cases cannot be adequately captured by a single driver (i.e. influential or predictor variable). In such cases, multiple linear regression (MLR) can be used to model the relationship between the response variable and multiple explanatory variables. 

## Multiple Linear Regression

Multiple linear regression helps us answer questions such as:

* How do various environmental factors influence the population size of a species? Factors like average temperature, precipitation levels, and habitat area can be used to predict the population size of a species in a given region. Which of these factors are most important in determining the population size?
* What are the determinants of plant growth in different ecosystems? Variables such as soil nutrient content, water availability, and light exposure can help predict the growth rate of plants in various ecosystems. How do these factors interact to influence plant growth?
* How do genetic and environmental factors affect the spread of a disease in a population? The incidence of a disease might depend on factors like genetic susceptibility, exposure to pathogens, and environmental conditions (e.g., humidity and temperature). What is the relative importance of these factors in determining the spread of the disease?

Multiple linear regression extends the simple linear regression model to include several independent variables. The model is expressed as:
$$Y_i = \alpha + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_k X_{ik} + \epsilon_i$$ {#eq-mlr1}
Where:

* $Y_i$ is the response variable for the $i$-th observation,
* $X_{i1}, X_{i2}, \ldots, X_{ik}$ are the $k$ predictor variables for the $i$-th observation,
* $\alpha$ is the intercept,
* $\beta_1, \beta_2, \ldots, \beta_k$ are the coefficients for the $k$ predictor variables, and
* $\epsilon_i$ is the error term for the $i$-th observation (the residuals).

When including a categorical variable in a multiple linear regression model, dummy (indicator) variables are used to represent the different levels of the categorical variable. Let's assume we have a categorical variable $C$ with three levels: $C_1$, $C_2$, and $C_3$. We can represent this categorical variable using two dummy variables:

* $D_1$: Equals 1 if $C = C_2$, 0 otherwise.
* $D_2$: Equals 1 if $C = C_3$, 0 otherwise.

$C_1$ is considered the reference category and does not get a dummy variable. This way, we avoid multicollinearity (see @sec-multicollinearity). R's `lm()` function will automatically convert the categorical variables to dummy variables (sometimes called treatment coding). The first level of the alphabetically sorted categorical variable is taken as the reference level. See @sec-r-function for more information about how to include categorical variables in a multiple linear regression model. At the end of the chapter you'll find alternative ways to assess categorical variables in a multiple linear regression model (@sec-contrasts).

Assume we also have $k$ continuous predictors $X_{1}, X_{2}, \ldots, X_{k}$. The multiple linear regression model with these predictors and the categorical variable can be expressed as:
$$Y_i = \alpha + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_k X_{ik} + \gamma_1 D_{i1} + \gamma_2 D_{i2} + \epsilon_i$$ {#eq-mlr2}
Where:

* $Y_i$ is the dependent variable for observation $i$.
* $\alpha$ is the intercept term.
* $\beta_1, \beta_2, \ldots, \beta_k$ are the coefficients for the continuous independent variables $X_{i1}, X_{i2}, \ldots, X_{ik}$.
* $D_{i1}$ and $D_{i2}$ are the dummy variables for the categorical predictor $C$.
* $\gamma_1$ and $\gamma_2$ are the coefficients for the dummy variables, representing the effect of levels $C_2$ and $C_3$ relative to the reference level $C_1$.
* $\epsilon_i$ is the error term for observation $i$.

## Nature of the Data

You are referred to the discussion in simple linear regression (@sec-simple-linear-regression). The only added consideration is that the data should be multivariate, i.e., it should contain more than one predictor variable. The predictor variables are generally continuous, but there may also be categorical variables.

## Assumptions

Basically, this is as already discussed in simple linear regression (@sec-simple-linear-regression)---in multiple linear regression, the same assumptions apply to the response relative to each of the predictor variables. In @sec-diagnostics I will assess the assumptions in an example dataset. An additional consideration is that the predictors must not be highly correlated with each other (multicollinearity) (see @sec-multicollinearity).

## Outliers

Again, this is as discussed in simple linear regression (@sec-simple-linear-regression). In multiple linear regression, the same considerations apply to the response relative to each of the predictor variables.

## R Function {#sec-r-function}

The `lm()` function in R is used to fit a multiple linear regression model. The syntax is similar to that of the `lm()` function used for simple linear regression, but with multiple predictor variables. The function takes the basic form:

```r
lm(formula, data)
```

For a multiple linear regression with only continuous predictor variables (as in @eq-mlr1), the formula is:

```{r}
#| eval: false
lm(response ~ predictor1 + predictor2 + ... + predictorN,
   data = dataset)
```

Interaction effects are implemented by including the product of two variables in the formula. For example, to include an interaction between `predictor1` and `predictor2`, we can use:

```{r}
#| eval: false
lm(response ~ predictor1 * predictor2, data = dataset)
```

When we have both continuous and categorical predictor variables (@eq-mlr2), the formula is:

```{r}
#| eval: false
lm(response ~ continuous_predictor1 + continuous_predictor2 + ...
   + continuous_predictorN + factor(categorical_predictor1) +
     factor(categorical_predictor2) + ...
   + factor(categorical_predictorM),
   data = dataset)
```


## Example 1: The Seaweed Dataset {#sec-example1}

```{r}
#| echo: FALSE

library(tidyverse)
library(vegan)
library(ggpubr)
library(viridis)
library(MASS) # for the stepAIC() function
library(car) # for the vif() function, among others
library(ggfortify) # for autoplot()
palette(viridis(8))
```

Load some [data](https://tangledbank.netlify.app/data/seaweed/spp_df2.csv) produced in the analysis by @smit2017seaweeds. Please refer to the chapter [Deep Dive into Gradients](https://tangledbank.netlify.app/BCB743/06-deep_dive.html) on Tangled Bank for the data description.

This dataset is suitable for a multiple linear regression because it has continuous response variables ($\beta_\text{sør}$, $\beta_\text{sim}$, and $\beta_\text{sne}$, the Sørenesen dissimilarity, the turnover component of $\beta$-diversity, and the nestedness-resultant component of $\beta$-diversity, respectively), continuous predictor variables (the mean climatological temperature for August, the mean climatological temperature for the year, the temperature range for February and August, and the SD of February and August), and a categorical variable (the bioregional classification of the samples).

```{r}
sw <- read.csv("data/spp_df2.csv")
rbind(head(sw, 3), tail(sw, 3))[,-1]
```

We will do a multiple linear regression analysis to understand the relationship between some of the environmental variables and the seaweed species. Specifically, we will consider only the variables `augMean`, `febRange`, `febSD`, `augSD`, and `annMean` as predictors of the species composition as measured by $\beta_\text{sør}$ (`Y` in the data file).

The model, which we will call `full_mod1` below, can be stated formally as @eq-full:
$$Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + \beta_5 X_5 + \epsilon$$ {#eq-full}
Where:

* $Y$ is the response variable, the mean Sørensen dissimilarity,
* the predictors $X_1$, $X_2$, $X_3$, $X_4$, and $X_5$ correspond to `augMean`, `febRange`, `febSD`, `augSD`, and `annMean`, respectively, and 
* $\epsilon$ is the error term. 

But before we jump into multiple linear regression, let's warm up by first fitting some simple linear regressions.

### Simple Linear Models

For interest sake, let's fit simple linear models for each of the predictors against the response variable. Let's look at relationships between the continuous predictors and the response in the East Coast Transition Zone (`ECTZ`), ignoring the other bioregions for now. We will first fit the simple linear models and then create scatter plots of the response variable $\beta_\text{sør}$ against each of the predictor variables. To these plots, we will add a best fit (regression) lines.

```{r}
sw_ectz <- sw |> filter(bio == "ECTZ")

predictors <- c("augMean", "febRange", "febSD", "augSD", "annMean")

# Fit models using purrr::map and store in a list
models <- map(predictors, ~ lm(as.formula(paste("Y ~", .x)),
                               data = sw_ectz))

names(models) <- predictors

model_summaries <- map(models, summary)
model_summaries
```

The individual models show that, for each predictor, the estimate of the coefficients (for slope) and the test for the overall hypothesis are both significant ($p < 0.05$ in all cases; refer to the model output). All the predictor variables are therefore good predictors of the structure of seaweed species composition along.

```{r}
#| fig-width: 10
#| fig-height: 7.5
#| fig.cap: "Individual simple linear regressions fitted to the variables `augMean`, `febRange`, `febSD`, `augSD`, and `annMean` as predictors of the seaweed species composition as measured by the Sørensen dissimilarity, `Y`."
#| label: fig-slr1
# Create individual plots for each predictor
plts1 <- map(predictors, function(predictor) {
  ggplot(sw_ectz, aes_string(x = predictor, y = "Y")) +
    geom_point(shape = 1, colour = "dodgerblue4") +
    geom_smooth(method = "lm", col = "magenta", fill = "pink") +
    labs(title = paste("Y vs", predictor),
         x = predictor,
         y = "Y") +
    theme_bw()
})

# Name the list elements for easy reference
names(plts1) <- predictors

ggpubr::ggarrange(plotlist = plts1, ncol = 2,
                  nrow = 3, labels = "AUTO")
```

@fig-slr1 is a series of scatter plots showing the relationship between the response variable $\beta_\text{sør}$ and each of the predictor variables. The blue line represents the linear regression fitted to the data. We see that the relationship between the response variable and each of the predictors is positive and linear. Each of the models are significant, as indicated by the $p$-values in the model summaries. These simple models do not tell us how some predictors might act together to influence the response variable.

To consider combined effects and interactions between predictor variables, we must explore multiple linear regression models that include all the predictors. Multiple regression will give us a more integrated understanding of how various environmental variables jointly influence species composition along the coast. In doing so, we can control for confounding variables, improve model fit, deal with multicollinearity, test for interaction effects, and enhance predictive power.

We will fit this multiple regression model next. 

### State the Hypotheses for a Multiple Linear Regression

As with all inferential statistics, we need to consider the hypotheses when performing multiple linear regression.

The null hypothesis ($H_0$) states that there is no significant relationship between the Sørensen diversity index and any of the the climatological variables entered into the model, implying that the coefficients for all predictors are equal to zero. The alternative hypothesis ($H_A$), on the other hand, states that there is a significant relationship between the Sørensen diversity index and the climatological variables, positing that at least one of the coefficients is not equal to zero.

The hypotheses can be divided into two kinds: those dealing with the main effects and the one assessing the overall model stated in @eq-full.

**Main effects hypotheses**

The main effects hypotheses test, for each predictor, $X_i$, if the predictor has a significant effect on the response variable $Y$. 

$H_0$: There is no linear relationship between the environmental variables (`augMean`, `febRange`, `febSD`, `augSD`, and `annMean`) and the community composition as measured by $\beta_\text{sør}$ (in `Y`). Formally, for each predictor variable $X_i$:

* $H_0: \beta_i = 0 \text{ for } i = 1, 2, 3, 4, 5$

Where $\beta_i$ are the coefficients of the predictors in the multiple linear regression model.

$H_A$: There is a linear relationship between the environmental variables (`augMean`, `febRange`, `febSD`, `augSD`, and `annMean`) and the species composition as measured by $\beta_\text{sør}$:

* $H_A: \beta_i \neq 0 \text{ for } i = 1, 2, 3, 4, 5$

**Overall hypothesis**

In addition to testing the individual predictors, $X_i$, we can also test a hypothesis about the overall significance of the model (*F*-test), which examines whether the model as a whole explains a significant amount of variance in the response variable $Y$. A significant *F*-test would suggest that *at least one* predictor (excluding the intercept) in the model is likely to be significantly related to the response, but it requires further investigation of individual predictors and potential multicollinearity to fully understand the relationships. For the overall model hypothesis:

Null Hypothesis ($H_0$):

* $H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = 0$

Alternative Hypothesis ($H_A$):

* $H_A: \exists \, \beta_i \neq 0 \text{ for at least one } i$

### Fit the Model

We fit two models:

* a full model that includes an intercept term and the five environmental variables, and
* a null model that includes only an intercept term.

The reason the null model is included is to compare the full model with a model that has no predictors. This comparison will help us determine which of the predictors are useful in explaining the response variable---we will see this in action in the forward model selection process later on (@sec-forward-selection).

```{r}
# Select only the variables that will be used in model building
sw_sub1 <- sw_ectz[, c("Y", "augMean", "febRange",
                      "febSD", "augSD", "annMean")]

# Fit the full and null models
full_mod1 <- lm(Y ~ augMean + febRange + febSD +
                 augSD + annMean, data = sw_sub1)
null_mod1 <- lm(Y ~ 1, data = sw_sub1)

# Add fitted values from the full model to the dataframe
sw_ectz$.fitted <- fitted(full_mod1)
```

### Dealing With Multicollinearity {#sec-multicollinearity}

Some of the predictor variables may be correlated with each other and this can lead to multicollinearity. When predictor variables are highly correlated, the model may not be able to distinguish the individual effects of each predictor. Consequently, the model becomes less precise and harder to interpret due to the coefficients' inflated standard errors (@graham2003confronting). One can create a plot of pairwise correlations to visually inspect the correlation structure of the predictors. I'll not do this here, but you can try it on your own. 

A formal way to detect multicollinearity is to calculate the variance inflation factor (VIF) for each predictor variable. The VIF measures how much the variance of the estimated regression coefficients is increased due to multicollinearity. A VIF value greater than 5 or 10 indicates a problematic amount of multicollinearity.

```{r}
initial_formula <- as.formula("Y ~ .")

threshold <- 10 # Define a threshold for VIF values

# Extract the names of the predictor variables
predictors <- names(vif(full_mod1))

# Iteratively remove collinear variables
while (TRUE) {
  # Calculate VIF values
  vif_values <- vif(full_mod1)
  print(vif_values) # Print VIF values for debugging
  max_vif <- max(vif_values)
  
  # Check if the maximum VIF is above the threshold
  if (max_vif > threshold) {
    # Find the variable with the highest VIF
    high_vif_var <- names(which.max(vif_values))
    cat("Removing variable:",
        high_vif_var,
        "with VIF:",
        max_vif,
        "\n")
    
    # Update the formula to exclude the high VIF variable
    updated_formula <- as.formula(paste("Y ~ . -", high_vif_var))
    
    # Refit the model without the high VIF variable
    full_mod1 <- lm(updated_formula, data = sw_sub1)
    
    # Update the environment data frame to reflect the removal
    sw_sub1 <- sw_sub1[, !(names(sw_sub1) %in% high_vif_var)]
  } else {
    break
  }
}
```

Regularisation techniques such as ridge regression, lasso regression, or elastic net can also be used to deal with multicollinearity. These advanced techniques add a penalty term to the regression model that shrinks the coefficients towards zero, which can help to reduce the impact of multicollinearity. However, these techniques are not covered in this guide. Please refer to @sec-regularisation for more information on regularisation techniques.

### Perform Forward Selection {#sec-forward-selection}

It might be that not all of the variables included in the full model are necessary to explain the response variable. We can use a stepwise regression to select the best combination (subset) of predictors that best explains the response variable. To do this, we will use the `stepAIC` function that lives in the `MASS` package.

`stepAIC()` works by starting with the null model and then adding predictors one by one, selecting the one that improves the model the most as seen in the reduction of the AIC values along the way. This process continues until no more predictors can be added to improve the model (i.e. to further reduce the AIC). Progress is tracked as the function runs.

```{r}
# Perform forward selection
mod1 <- stepAIC(null_mod1,
                scope = list(lower = null_mod1, upper = full_mod1),
                direction = "forward")
```

The model selection process shows that as we add more variables to the model, the AIC value decreases. We can infer from this that the multiple regression model provides a better fit that simple linear models that use the variables in isolation.

We also see that `stepAIC()` has not removed any variables from the full model. Probably one reason for failing to remove any variables is that the VIF process has already accomplished this by virtue of dealing with multicollinearity. This means that all the variables retained in `mod1` are important in explaining the response variable. 

### Added-Variable Plots (Partial Regression Plots) {#sec-added-variable-plots}

Before looking at the output in more detail, I'll introduce partial regression plots as a means to examine the relationship between the response variable and each predictor variable. Although they can be calculated by hand, the **car** package provides a convenient function, `avPlots()`, to create these plots.

Added variable plots are also sometimes called 'partial regression plots' or 'individual coefficient plots.' They are used to display the relationship between a response variable and an individual predictor variable while accounting for the effect of other predictor variables in a multiple regression model (the marginal effect).

```{r}
#| fig-width: 8
#| fig-height: 5
#| fig.cap: "Partial regression plots for `mod1` with the selected variables `augMean`, `febSD`, and `augSD`."
#| label: fig-mod1-partial

# Create partial regression plots
avPlots(mod1, col = "dodgerblue4", col.lines = "magenta")
```

What insights can we draw from the added-variable plots? Although there are better ways to assess the model fit, we can already make some observations about the linearity of the model or the presence of outliers. The slope of the line in an added variable plot corresponds to the regression coefficient for that predictor in the full multiple regression model. Seen in this way, it visually indicates the magnitude and direction of each predictor's effect. In @fig-mod1-partial, the added-variable plot for `augMean` shows a tighter clustering of points around the regression line and a strong linear relationship (steep slope) with the response variable; the plots for `febSD` and `augSD`, on the other hand, show a weaker response and more scatter about the regression line. Importantly, this suggests that `augMean` has a stronger and more unique contribution to the multiple-variable model than the other two variables.

There are also insights to be made about possible multicollinearity using added-variable plots. These plots are not a definitive test for multicollinearity, but they can provide some clues. Notably, if a predictor shows a strong relationship with the response variable in a simple correlation but appears to have little relationship in the added-variable plot, it might indicate collinearity with other predictors. This discrepancy suggests that the predictor's effect on the response is being masked by the presence of other correlated predictors.

### Model Diagnostics {#sec-diagnostics}

We are back in the territory of parametric statistics, so we need to check the assumptions of the multiple linear regression model (similar to those of simple linear regression). We can do this by making the various diagnostic plots. all of them consider various aspects of the residuals, which are simply the differences between the observed and predicted values. 

**Diagnostic plots of final model**

You have been introduced to diagnostic plots in the context of simple linear regression (@sec-simple-linear-regression). They are also useful in multiple linear regression. Although `plot.lm()` can easily do this, here I use `autoplot()` from the **ggfortify** package. When applied to the final model, `mod1`, the plot will in its default setting show four diagnostic plots: residuals vs. fitted values, normal Q-Q plot, scale-location plot, and residuals vs. leverage plot. Note, this is for the full model inclusive of the combined contributions of all the predictors, so we will not see separate plots for each predictor as we have seen in the added-variable plots or component plus residual plots.

```{r}
#| fig-width: 8
#| fig-height: 5
#| fig.cap: "Diagnostic plots to assess the fit of the final multiple linear regression model, `mod1`."
#| label: fig-mlr3

# Generate diagnostic plots 
autoplot(mod1, shape = 21, colour = "dodgerblue4",
         smooth.colour = "magenta") +
  theme_bw()
```

**Residuals vs. Fitted Values:** In this plot we can assess linearity and homoscedasticity of the residuals. If the seaweed gods were with us, we'd expect the points to be randomly scattered about a horizontal line situation at zero. This would indicate that the relationship between the predictors selected by the forward selection process (`augMean`, `febSD`, and `augSD`) and the response variable (`Y`) is linear, and the variance of the residuals is constant across the range of fitted values. In this plot, there's a very slight curvature which might suggest a potential issue with the linearity assumption---it is minute and I'd suggest not worrying about it. The variance of the residuals seems to decrease slightly at higher fitted values, indicating a mild case of heteroscedasticity.

**Q-Q Plot (Quantile-Quantile Plot):** This plot is used to check the normality of the residuals. The points should fall approximately along a straight diagonal line if the residuals are normally distributed. Here we see that the points generally follow the line although some deviations may be seen at the tails. These deviations are not that extreme and again I don't think this is not a big concern.

**Scale-Location Plot:** This plot should reveal potential issues with homoscedasticity. The square root of the standardised residuals is used here to make it easier to spot patterns, so we would like the points to be randomly scattered around the horizontal red line.  Here, the line slopes slightly downward and this indicates that the variance of the residuals might decrease as the fitted values increase. We can also see evidence of this in a plot of the observed values vs. the predictors in @fig-mlr3.

**Residuals vs. Leverage:** This diagnostic highlights influential points (outliers). Points with high leverage (far from the mean of the predictors) can be expected to exert a strong influence on the regression line, tilting it in some direction. Cook's distance (indicated by the yellow line) helps identify such outliers. In our seaweed data a few points could have a high leverage, but since they don't seem to cross the Cook's distance thresholds, I doubt they are overly worrisome.

Considering that no glaring red flags were raised by the diagnostic plots, I doubt that they are severe enough to invalidate the model. However, if you cannot stand these small issues, you could i) consider transforming the predictor or response variables to address your concerns about heteroscedasticity, ii) investigate the outliers (high leverage points) to confirm if they are valid data points or errors, or iii) try robust regression methods that are less sensitive to outliers and heteroscedasticity.

**Component plus residual plots**

Component plus residual plots offer another way to assess the fit of the model in multiple regression models. Unlike simple linear regression where we only had one predictor variable, here we have several. So, we need to assure ourselves that there is a linear relationship between each predictor variable and the response variable (we could already see this in the added-variable plots in @sec-added-variable-plots). We can make component plus residual plots using the `crPlots()` function in the **car** package. It displays the relationship between the response variable and each predictor variable. If the relationship is linear, the points should be randomly scattered about a best fit line and the spline (in pink in @fig-mod1-crplots) should plot nearly on top of the linear regression line. 

```{r}
#| fig-width: 8
#| fig-height: 5
#| fig.cap: "Component plus residual diagnostic plots to assess the fit of the final multiple linear regression model, `mod1`."
#| label: fig-mod1-crplots

# Generate component plus residual plots
crPlots(mod1, col = "dodgerblue4", col.lines = "magenta")
```

### Understanding the Model Fit {#sec-mod1-summary}

The above model selection process has led us to the `mod1` model, which can be stated formally as:
$$Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \epsilon$$ {#eq-mod1}
Where:

* $Y$: The response variable, the mean Sørensen dissimilarity.
* $X_1$, $X_2$, and $X_3$: The predictors corresponding to `augMean`, `febSD`, and `augSD`, respectively.
* $\epsilon$: The error term. 

We have convinced ourselves that the model is a good fit for the data, and we can proceed to examine the model's output. The fitted model can be explored in two ways: by applying the `summary()` function or by using the `anova()` function. The `summary()` function provides a detailed output of the model, while the `anova()` function provides a table of deviance values that can be used to compare models.

**The model summary** 

```{r}
# Summary of the selected model
summary(mod1)
```

The first part of the `summary()` function's output is the `Coefficients` section. This is where the main effects hypotheses are tested (this model does not have interactions---if there were, they'd appear here, too). The important components of the coefficients part of the model summary are:

- `(Intercept)`: This row provides information about where the regression line intersects the *y*-axis. 
- Main Effects:
    - `augMean`, `febSD`, and `augSD`: These rows give the model coefficients associated with the slopes of the regression lines fit to those predictor variables. They indicate the rate of change in the response variable for a one-unit change in the predictor variable. 
    - `Estimate`, `Std. Error`, `t value`, and `Pr(>|t|)`: These columns contain the statistics used to interpret the hypotheses about the main effects. In the `Estimate` column are the coefficients for the *y*-intercept and the main effects' slopes, and `Std. Error` indicates the variability of the estimate. The `t value` is obtained by dividing the coefficient by its standard error. The *p*-value tests the null hypothesis that the coefficient is equal to zero and significance codes are provided as a quick visual reference (their use is sometimes frowned upon by statistics purists). Using this information, we can quickly see that, for example, `augMean` has a coefficient of $0.2833 \pm 0.0111$ and the slope of the line is highly significant, i.e. there is a significant effect of `Y` due to the temperature gradient set up by `augMean`. 

::: callout-note
## The intercept and slope coefficients

The interpretation of the coefficients is a bit more complicated in multiple linear regression compared to what we are accustomed to in simple linear regression. Let us look at some greater detail at the intercept and the slope coefficients:

Intercept ($\alpha$): ) The intercept is the expected value of the response variable, $Y$, when all predictor variables are zero. It is not always meaningful, but it can be useful in some cases. 

Slope Coefficients ($\beta_1, \beta_2, \ldots, \beta_k$): Each slope coefficient, $\beta_j$, represents the expected change in the response variable, $Y$, for a one-unit increase in the predictor variable, $X_j$, holding all other predictor variables constant. This partial effect interpretation implies that $\beta_j$ accounts for the direct contribution of $X_j$ to $Y$ while removing the confounding effects of other predictors in the model. @fig-mod1-partial provides a visual representation of this concept and isolates the effect of each predictor variable on the response variable.

Therefore, in the context of our model (@eq-mod1) for this analysis, the partial interpretation is as follows:

* $\beta_1$: Represents the change in $Y$ for a one-unit increase in $X_1$, holding $X_2$ and $X_3$ constant.
* $\beta_2$: Represents the change in $Y$ for a one-unit increase in $X_2$, holding $X_1$ and $X_3$ constant.
* $\beta_3$: Represents the change in $Y$ for a one-unit increase in $X_3$, holding $X_1$ and $X_2$ constant.
:::

There are also several overall model fit statistics---it is here where you'll find the information you need to assess the hypothesis about the overall significance of the model. `Residual standard error` indicates the average distance between observed and fitted values. `Multiple R-squared` and `Adjusted R-squared` values tell us something about the model's goodness of fit. The latter adjusts for the number of predictors in the model, and is the one you must use and report in multiple linear regressions. As you also know, higher numbers approaching 1 are better, with 1 suggesting that the model perfectly captures all of the variability in the data. The `F-statistic` and its associated *p*-value test the overall significance of the model and examines whether all regression coefficients are simultaneously equal to zero. You can also use the brief overview of the residuals, but I don't find this particularly helpful---best examine the residuals in a histogram.

**The ANOVA tables**

```{r}
anova(mod1)
```

This function provides a sequential analysis of variance (Type I ANOVA) table for the regression model (see more about Type I ANOVA, below). As such, this function can also be used to compare nested models. Used on a single model, it gives a more interpretable breakdown of the variability in the response variable `Y` and assesses the contribution of each predictor variable in explaining this variability. 

The ANOVA table firstly shows the degrees of freedom (`Df`) for each predictor variable added sequentially to the model, as well as the residuals. For each predictor, the degrees of freedom is typically 1. For the residuals, however, it represents the total number of observations minus the number of estimated parameters. The Sum of Squares (`Sum Sq`) indicates the variability in `Y` attributable to each predictor, and the mean sum of squares (`Mean Sq`) is the sum of squares divided by the degrees of freedom. 

The `F value` is calculated as the ratio of the predictor's mean square to the residual mean square tests. It is used in testing the null hypothesis that the predictor has no effect on `Y`. Whether or not we accept the alternative hypothesis (reject the null) is given by the *p*-value (`Pr(>F)`) that goes with each *F*-statistic. You know how that works.

Because this is a sequential ANOVA, the amount of variance in `Y` explained by each predictor (or group of predictors) is calculated by adding the predictors to the model in sequence (as specified in the model formula). For example, the Sum of Squares for `augMean` (6.0084) represents the amount of variance explained by adding `augMean` to a model that doesn't include any predictors yet. The Sum of Squares for `febSD` 0.3604) represents the amount of variance explained by adding `febSD` to a model that already includes `augMean`---this improvement indicates that `febSD` explains some of the variance in `Y` that `augMean` doesn't. 

::: callout-note
## Order in which predictors are assessed in multiple linear regression

The interpretation of sequential ANOVA (Type I) is inherently dependent on the order in which predictors are entered. In `mod1` the order is first `augMean`, then `febSD`, and last comes `augSD`. This order might not be the most meaningful for interpreting the sequential sums of squares and their significance in the ANOVA table. How, then, does one decide on the order of predictors in the model?

* If you have a strong theoretical or causal basis for thinking that certain predictors influence others, you can enter them in that order.
* If you have a hierarchy of predictors based on their importance or general vs. specific nature, you can enter them hierarchically.
* You can manually fit models with different predictor orders and compare the ANOVA tables to see how the results change. This can be time-consuming but might offer insights into the sensitivity of your conclusions to the order of entry.
* You can use automated model selection procedures, such as stepwise regression, to determine the best order of predictors. This is a more objective approach but can be criticised for being data-driven and not theory-driven.
* Use Type II or Type III ANOVAs, which are are not order-dependent and can be used to assess the significance of predictors after accounting for all other predictors in the model. However, they have their own limitations and assumptions that need to be considered.

My advice would be to have sound theoretical reasons for the order of predictors in the model.
:::

Both ways of looking at the model fit of `mod1`---`summary()` and `anova()`---show that forward selection retained the variables `augMean`, `febSD`, and `augSD`. These three predictors should be used together to explain the response, `Y`. 

Let's make a plot of the full model with all the initial predictors and the selected model with the predictors chosen by the forward selection process.

```{r}
#| fig-width: 3.5
#| fig-height: 2.15
#| fig.cap: "Plot of observed vs. predicted value obtained from the final multiple linear regression model (`mod`) with the selected variables `augMean`, `febSD`, and `augSD` as predictors (black points), and the initial model with also `annMean` and `febRange` (red points)."
#| label: fig-mlr2

# Add fitted values from the selected model to the dataframe
sw_ectz$.fitted_selected <- fitted(mod1)

# Create the plot of observed vs fitted values for the selected model
ggplot(sw_ectz, aes(x = .fitted_selected, y = Y)) +
  geom_point(shape = 1, colour = "black", alpha = 1.0) +
  geom_point(aes(x = .fitted), colour = "red",
             shape = 1, alpha = 0.4) +
  geom_abline(intercept = 0, slope = 1,
              color = "blue", linetype = "dashed") +
  labs(x = "Fitted Values",
       y = "Observed Values") +
  theme_bw()
```

### Reporting

A Results section should be written in a format sutable for inclusion in your report or publication. Present the results in a clear and concise manner, with tables and figures used to help substantiate your findings. The results should be interpreted in the context of the research question and the study design. The limitations of the analysis should also be discussed, along with any potential sources of bias or confounding. Here is an example.

**Results**

The model demonstrates a strong overall fit, as indicated by the high $R^2$ value of 0.839 and an adjusted $R^2$ of 0.837, suggesting that approximately 83.7% of the variance in the mean Sørensen dissimilarity is explained by the predictors `augMean`, `febSD`, and `augSD`. All predictors in the model are statistically significant, with `augMean` showing the strongest effect ($\beta_1 = 0.283$, $p < 0.0001$) (@fig-mod1-partial). The predictors `febSD` and `augSD` also have significant positive relationships with the response variable ($\beta_2 = 0.050$, $p = 0.0001$; $\beta_3 = 0.022$, $p = 0.0001$). A sequential ANOVA further confirms the significance of each predictor variable in the model, with all *F*-values indicating that the inclusion of each predictor significantly improves the model fit ($p < 0.0001$ in all cases). Our model therefore provides clear support for the mean temperatures in August, the standard deviation of temperatures in February, and the standard deviation of temperatures in August as strong predictors of the mean Sørensen dissimilarity, with each contributing uniquely to the explanation of variability in the response variable.

## Example 2: Interaction of Distance and Bioregion {#sec-mlr-interaction}

Our seaweed dataset includes two additional variables that we have not yet considered. These are the continuous variable `dist` which represents the geographic distance between the seaweed samples taken along the coast of South Africa, and the categorical variable `bio` which is the bioregional classification of the seaweed samples.

These two new variables lend themselves to a few interesting questions. For example:

1. Is the geographic distance between samples related to the Sørensens dissimilarity of the seaweed flora?
2. Does the average Sørensens dissimilarity vary among the bioregions to which the samples belong?
3. Is the effect of geographic distance on the Sørensens dissimilarity different for each bioregion?

The most complex model is (3), the one that answers the question about whether the effect of `dist` on the response variable $Y$ is different for each bioregion. Questions (1) and (2) are subsets of this more inclusive question. To fully answer these quesitons, let's first consider the full model, which includes an *interaction term* between the continuous predictor `dist` and the categorical predictor `bio`. When we finally test our model, we will also have to consider the simpler models that do not include the interaction term.

'Interaction' means that the effect of one predictor on the response variable is contingent on the value of another predictor. For example, we might have reason to suspect that the relationship of the Sørensens dissimilarity with the geographic distance between samples is different between the west coast compared to, say, the east coast. This is indeed a plausible expectation, but we will test this formally below.

The full multiple linear regression model with the interaction terms can be formally expressed as Equation \ref{mod2}:
```{=latex}
\begin{align}
Y &= \alpha + \beta_1 \text{dist} + \beta_2 \text{bio}_{\text{B-ATZ}} + \beta_3 \text{bio}_{\text{BMP}} \nonumber \\
  &\quad + \beta_4 \text{bio}_{\text{ECTZ}} + \beta_5 (\text{dist} \times \text{bio}_{\text{B-ATZ}}) \nonumber \\
  &\quad + \beta_6 (\text{dist} \times \text{bio}_{\text{BMP}}) + \beta_7 (\text{dist} \times \text{bio}_{\text{ECTZ}}) + \epsilon \label{mod2}
\end{align}
```
Where:

* $Y$: The response variable, the mean Sørensen dissimilarity.
* $\alpha$: The intercept term.
* $\text{dist}$: The continuous predictor variable representing distance.
* $\text{bio}$: The categorical predictor variable representing bioregional classification with four levels: `AMP` (reference category), `B-ATZ`, `BMP`, and `ECTZ`.
* $\text{bio}_\text{B-ATZ}, \text{bio}_\text{BMP}, \text{bio}_\text{ECTZ}$: Dummy variables for the bioregional classification, where:
  * $\text{bio}_\text{B-ATZ} = 1$ if `bio` = `B-ATZ`, and 0 otherwise,
  * $\text{bio}_\text{BMP} = 1$ if `bio` = `BMP`, and 0 otherwise, and
  * $\text{bio}_\text{ECTZ} = 1$ if `bio` = `ECTZ`, and 0 otherwise.
* $\text{dist} \times \text{bio}_\text{B-ATZ}, \text{dist} \times \text{bio}_\text{BMP}, \text{dist} \times \text{bio}_\text{ECTZ}$: Interaction terms between distance and the bioregional classification dummy variables.
* $\beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6, \beta_7$: The coefficients to be estimated for the main effects and interactions.
* $\epsilon$: The error term.

If this seems tricky, it is because of the dummy variable coding used to represent interactions in multiple linear regression. The `bio` variable is a categorical variable with four levels, so we need to create three dummy variables to represent the bioregional classification. The `dist` variable is then interacted with each of these dummy variables to create the interaction terms. The `lm()` function in R takes care of this for us in a far less complicated model statement. I'll explain the details around the interpretation of dummy variable coding when we look at the output of the model with the `summary()` function.

### State the Hypotheses for a Multiple Linear Regression with Interaction Terms

Equation \ref{mod2} expands into the following series of hypotheses that concern the main effects, the interactions between the main effects, and the overall hypothesis:

**Main effects hypotheses**

In the main effects hypotheses we are concerned with the effect of each predictor variable on the response variable. For the main effect of distance we have the null:

* $H_0: \beta_1 = 0$

vs. the alternative:

* $H_A: \beta_1 \neq 0$

For the main effect of bioregional classification, the nulls are:

* $H_0: \beta_2 = 0 \quad (\text{bio}_{\text{B-ATZ}})$
* $H_0: \beta_3 = 0 \quad (\text{bio}_{\text{BMP}})$
* $H_0: \beta_4 = 0 \quad (\text{bio}_{\text{ECTZ}})$

vs. the alternatives:

* $H_A: \beta_2 \neq 0 \quad (\text{bio}_{\text{B-ATZ}})$
* $H_A: \beta_3 \neq 0 \quad (\text{bio}_{\text{BMP}})$
* $H_A: \beta_4 \neq 0 \quad (\text{bio}_{\text{ECTZ}})$

**Hypotheses about interactions**

This is where the hypothesis tests whether the effect of distance on the response variable is different for each bioregional classification. The null hypotheses are:

* $H_0: \beta_5 = 0 \quad (\text{dist} \times \text{bio}_{\text{B-ATZ}})$
* $H_0: \beta_6 = 0 \quad (\text{dist} \times \text{bio}_{\text{BMP}})$
* $H_0: \beta_7 = 0 \quad (\text{dist} \times \text{bio}_{\text{ECTZ}})$

vs. the alternatives:

* $H_A: \beta_5 \neq 0 \quad (\text{dist} \times \text{bio}_{\text{B-ATZ}})$
* $H_A: \beta_6 \neq 0 \quad (\text{dist} \times \text{bio}_{\text{BMP}})$
* $H_A: \beta_7 \neq 0 \quad (\text{dist} \times \text{bio}_{\text{ECTZ}})$

**Overall hypothesis**

The overall hypothesis states that all coefficients associated with the predictors (distance, bioregional categories, and their interactions) are equal to zero, therefore indicating no relationship between these predictors and the response variable, the Sørensen index. The null hypothesis is:

* $H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = \beta_7 = 0$

vs. the alternative:

* $H_A: \exists \, \beta_i \neq 0 \text{ for at least one } i$

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 2.5
#| fig.cap: "Plot of main effects of A) distance along the coast and B) bioregional classification on the Sørensen dissimilarity index."
#| label: fig-mod2_main_effects

p1 <- ggplot(sw, aes(x = dist, y = Y)) +
  geom_point(shape = 1, aes(colour = bio)) +
  geom_smooth(method = "lm", se = TRUE,
              colour = "black", fill = "grey60",
              linewidth = 0.4) +
  scale_colour_brewer(palette = "Set1") +
  labs(x = "Distance (km)",
       y = "Sørensen Dissimilarity") +
  theme_bw()

p2 <- ggplot(sw, aes(x = bio, y = Y)) +
  geom_boxplot(aes(colour = bio)) +
  labs(x = "Bioregional Classification",
       y = "Sørensen Dissimilarity") +
  theme_bw() +
  theme(legend.position = "none")

ggarrange(p1, p2, ncol = 2, labels = "AUTO")
```

### Visualise the Main Effects

To facilitate the interpretation of the main effects hypotheses and make an argument for why an interaction term might be necessary, I've visualised the main effects (@fig-mod2_main_effects). I see this as part of my exploratory data analysis ensemble of tests. We see that fitting a straight line to the `Y` vs. distance relationship seems unsatisfactory as there is too much scatter around that single line to adequately capture all the structure in the variability of the points. Colouring the points by bioregion reveals the hidden structure. The model could benefit from including an additional level of complexity: see how points in the same bioregion show less scatter compared to points in different bioregions.

Now look at the boxplots of the Sørensen dissimilarity index for each bioregional classification. It shows that the median values of the Sørensen dissimilarity index are different for each bioregion. Taken together, @fig-mod2_main_effects (A, B) provide a good indication that adding the bioregional classification might be an important predictor of the Sørensen dissimilarity index as a function of distance between pairs of sites along the coast.

Next, we will move ahead and fit the model inclusive of the distance along the coast and bioregion as per Equation \eqref{mod2}.

### Fit and Assess Nested Models

I have a suspicion that the full model (`mod2`; see below) with the interaction terms will be a better fit than reduced models with only the effect due to distance (seen independently). How can we have greater certainty that we should indeed favour a slightly more complex model (with two predictors) over a simpler one with only (distance only)?

One way to do this is to use a nested model comparison. We will fit a reduced model (one slope for all bioregions) and compare this model to the full model (slopes are allowed to vary among bioregions).

```{r}
# Fit the linear regression model with only distance
mod2a <- lm(Y ~ dist, data = sw)

# Fit the multiple linear regression model with interaction terms
mod2 <- lm(Y ~ dist * bio, data = sw)
```

This is a nested model where `mod2a` is nested within `mod2`. 'Nested' means that the reduced model is a subset of the full model. Nested models can be used to test hypotheses about the significance of the predictors in the full model---does adding more predictors to the model improve the fit? Comparing a nested model with a full model can be done with a sequential ANOVA, which is what the `anova()` function also does (in addition to its use in @sec-mod1-summary).

So, comparing `mod2a` to `mod2` with an *F*-test tests the significance of adding the `bio` and using it together with `dist`. The interaction is built into `mod2` but we are not yet testing the significance of the interaction terms. We will do that later.


```{r}
anova(mod2a, mod2, test = "F")
```

The sequential ANOVA shows that there is significant merit to consider an interaction term in the model. This model would then allow us to have a separate slope for the Sørensen index as function of distance for each bioregion. The residual sum of squares (`RSS`) decreases from 7.7388 in Model 1 to 2.2507 in Model 2, which indicates that Model 2 explains a significantly larger proportion of the variance in the response variable. The *F*-test for comparing the two models yields an *F*-value of 390.95 with a highly significant *p*-value (< 0.0001). The improvement in model fit due to the inclusion of the interaction term is therefore statistically significant.

The above analyses skirted around the questions stated in the beginning of @sec-mlr-interaction. I've provided statistical evidence that full model is a better fit than the reduced model (the sequential *F*-test tested this), so we should use both `dist` and `bio` in the model. I have not looked explicitly at the main effects of the predictors. However, we can easily address questions (1) and (2): 

* Question 1: looking at the summary of `mod2a` tells us that the main effect of `dist` is a significant (*p* < 0.0001) predictor of the Sørensen dissimilarity index.
* Question 2: the main effect of `bio` is also significant (*p* < 0.0001), which is what we'd see if we fit the model `mod2b <- lm(Y ~ bio, data = sw)`.

Question 3 warrants deeper investigation. Next, we will look at the interaction terms in the full model `mod2` to see if the effect of `dist` on `Y` is different for each level of `bio`.

### Interpret the Full Model

**The model summary**

```{r}
# Summary of the model
summary(mod2)
```

In the output returned by `summary(mod2)`, we need to pay special attention to the use of dummy variable encoding for the categorical predictor. The `Coefficients` section is similar to that of `mod1` (see @sec-mod1-summary), but now it includes the categorical predictor `bio*` and the interaction terms `dist:bio*` (`*` indicating the levels of the categorical variable). The `bio` variable has four levels, `BMP`, `B-ATZ`, `AMP`, and `ECTZ`, and `AMP` is selected as reference level. This decision to selected `AMP` as reference is entirely arbitrary, and alphabetical sorting offers a convenient approach to selecting the reference. The coefficients for the other levels of `bio` are interpreted as the sum of the response variable and the reference level. 

The following are the key coefficients in the model summary: 

* `(Intercept)`: This is the estimated average value of `Y` when `dist` is zero and `bio` is the reference category (`AMP`). Its *p*-value (> 0.05) suggests it's not significantly different from zero.
* Main Effects:
    * `dist`: This represents the estimated change in `Y` for a one-unit increase in `dist` when the bioregion is the reference category, `AMP`. The highly significant *p*-value (< 0.0001) indicates a strong effect of distance in the `AMP`.
    * `bioB-ATZ`, `bioBMP`, `bioECTZ`: These are dummy variables representing different bioregions. Their coefficients indicate the difference in the average value of `Y` between each of these bioregions and the reference bioregion when `dist` is zero. Only `bioBMP` and `bioECTZ` are significantly different from the reference bioregion, `AMP`.

* Interaction Effects:
    * `dist:bioB-ATZ`, `dist:bioBMP`, `dist:bioECTZ`: These interaction terms capture how the effect of `dist` on `Y` varies across different bioregions. For instance, `dist:bioB-ATZ` indicates the additional change in the effect of `dist` in the `B-ATZ` bioregion compared to the reference bioregion, `AMP`. All interaction terms are highly significant, suggesting the effect of distance is different across bioregions.

Given this explanation, we can now interpret the coefficients of, for example, the `bioB-ATZ` main effect and `dist:bioB-ATZ` interaction. Since `AMP` is the reference bioregion, its effect is absorbed into the intercept term. Therefore, the coefficient for `bioB-ATZ` directly reflects the difference we are interested in. The coefficient for `bioB-ATZ` is `r round(summary(mod2)$coefficients["bioB-ATZ", "Estimate"], 4)` $\pm$ `r round(summary(mod2)$coefficients["bioB-ATZ", "Std. Error"], 4)` lower than that of the reference, but the associated *p*-value (> 0.05) indicates that the average value of `Y` in the `B-ATZ` bioregion is not significantly different from the reference bioregion, `AMP`.

If we'd want to report the actual coefficient for `B-ATZ`, we'd calculate the sum of the coefficients for `(Intercept)` and `bioB-ATZ`. This would give us the estimated average value of `Y` in the `B-ATZ` bioregion when `dist` is zero. The associated SE is calculated as the square root of the sum of the squared SEs of the two coefficients. Therefore, the coefficient for `B-ATZ` is `r round(summary(mod2)$coefficients["(Intercept)", "Estimate"] + summary(mod2)$coefficients["bioB-ATZ", "Estimate"], 4)` $\pm$ `r round(sqrt(summary(mod2)$coefficients["(Intercept)", "Std. Error"]^2 + summary(mod2)$coefficients["bioB-ATZ", "Std. Error"]^2), 4)`.

The coefficient of `r round(summary(mod2)$coefficients["dist:bioB-ATZ", "Estimate"], 4)` for `dist:bioB-ATZ` indicates that the effect of distance on `Y` is `r round(summary(mod2)$coefficients["dist:bioB-ATZ", "Estimate"], 4)` units greater in the `B-ATZ` bioregion compared to the `AMP` bioregion. The SE of `r round(summary(mod2)$coefficients["dist:bioB-ATZ", "Std. Error"], 4)` suggests a high level of precision in this estimate, and the *p*-value (< 0.0001) indicates that this difference is statistically significant.

As before, to calculate the actual coefficient for `dist` in the `B-ATZ` bioregion, we'd sum the coefficients for `dist` and `dist:bioB-ATZ`. The associated SE of this sum is calculated as the square root of the sum of the squared SEs of the two coefficients. Therefore, the coefficient for `dist` in the `B-ATZ` bioregion is `r round(summary(mod2)$coefficients["dist", "Estimate"] + summary(mod2)$coefficients["dist:bioB-ATZ", "Estimate"], 4)` $\pm$ `r round(sqrt(summary(mod2)$coefficients["dist", "Std. Error"]^2 + summary(mod2)$coefficients["dist:bioB-ATZ", "Std. Error"]^2), 4)`.

Concerning the overall hypothesis, the `Adjusted R-squared` value of 0.8597 indicates that the model explains 85.97% of the variance in the response variable `Y`. The `F-statistic` and associated `p-value` (< 0.0001) indicate that the model as a whole is highly significant, meaning at least one of the predictors (including interactions) has a significant effect on `Y`.

**The ANOVA table**

```{r}
# The ANOVA table
anova(mod2)
```

The ANOVA table's interpretation is intuitive and simple: the `Pr(>F)` column shows the *p*-value for each predictor in the model. The `dist` predictor has a highly significant effect on `Y` (< 0.0001), as do all the bioregions and their interactions with `dist`. This confirms the results we obtained from the coefficients. We don't need to overthink this result.

## Example 3: The Final Model

I'll now expand `mod1` to include `bio` as a predictor alongside `augMean`, `febSD`, and `augSD` (`mod1` was applied only to data pertaining to `ECTZ`, one of the four levels in `bio`).

```{=latex}
\begin{align}
Y &= \alpha + \beta_1 \text{augMean} + \beta_2 \text{febSD} + \beta_3 \text{augSD} \nonumber \\
  &\quad + \beta_4 \text{bio}_{\text{B-ATZ}} + \beta_5 \text{bio}_{\text{BMP}} + \beta_6 \text{bio}_{\text{ECTZ}} \nonumber \\
  &\quad + \beta_7 (\text{augMean} \times \text{bio}_{\text{B-ATZ}}) + \beta_8 (\text{augMean} \times \text{bio}_{\text{BMP}}) \nonumber \\
  &\quad + \beta_9 (\text{augMean} \times \text{bio}_{\text{ECTZ}}) + \beta_{10} (\text{febSD} \times \text{bio}_{\text{B-ATZ}}) \nonumber \\
  &\quad + \beta_{11} (\text{febSD} \times \text{bio}_{\text{BMP}}) + \beta_{12} (\text{febSD} \times \text{bio}_{\text{ECTZ}}) \nonumber \\
  &\quad + \beta_{13} (\text{augSD} \times \text{bio}_{\text{B-ATZ}}) + \beta_{14} (\text{augSD} \times \text{bio}_{\text{BMP}}) \nonumber \\
  &\quad + \beta_{15} (\text{augSD} \times \text{bio}_{\text{ECTZ}}) + \epsilon \label{mod3}
\end{align}
```
Where:

* $Y$: The response variable (mean Sørensen dissimilarity).
* $\alpha$: The intercept term, representing the expected value of `Y` when all predictors are zero and `bio` is at the reference level `AMP`).
* $\beta_1$: The coefficient for the main effect of `augMean.`
* $\beta_2$: The coefficient for the main effect of `febSD.`
* $\beta_3$: The coefficient for the main effect of `augSD.`
* $\beta_4, \beta_5, \beta_6$: The coefficients for the main effects of the categorical predictor `bio` (for levels `B-ATZ`, `BMP`, and `ECTZ` respectively, with `AMP` as the reference category).
* $\beta_7, \beta_8, \beta_9$: The coefficients for the interaction effects between `augMean` and `bio` (for levels `B-ATZ`, `BMP`, and `ECTZ` respectively).
* $\beta_{10}, \beta_{11}, \beta_{12}$: The coefficients for the interaction effects between `febSD` and `bio` (for levels `B-ATZ`, `BMP`, and `ECTZ` respectively).
* $\beta_{13}, \beta_{14}, \beta_{15}$: The coefficients for the interaction effects between `augSD` and `bio` (for levels `B-ATZ`, `BMP`, and `ECTZ` respectively).
* $\epsilon$: The error term, representing the unexplained variability in the response variable.

In this multiple regression model, we aim to understand the complex and interacting relationships between the response variables and the set of predictors. It allows us to investigate not only the individual effects of the continuous predictors on `Y`, but also how these effects might vary across the different bioregions.

The model therefore incorporates interaction terms between each continuous predictor (`augMean`, `febSD`, and `augSD`) and the categorical variable `bio`. This allows us to assess whether the relationships between `augMean`, `febSD`, or `augSD` and `Y` change depending on the specific bioregion. Essentially, we are testing whether the slopes of these relationships are different in different bioregions. 

Additionally, the model examines the main effects of the bioregions themselves on `Y`. This means we're testing whether the average value of `Y` differs significantly across bioregions, after accounting for the influence of the continuous predictors.  

This is how these different insights pertain to the model components:

* Main Effects: The coefficients for the main effects of `augMean`, `febSD`, and `augSD` represent the effect of each predictor when `bio` is at its reference level.
* Coefficients for `bio`: The coefficients for `bio` (e.g., $\beta_4 \text{bio}_{\text{B-ATZ}}$) represent the difference in the intercept for the corresponding level of `bio` compared to the reference level.
* Interaction Terms: The interaction terms allow the slopes of `augMean`, `febSD`, and `augSD` to vary across the different levels of `bio`. For example, $\beta_7 (\text{augMean} \times \text{bio}_{\text{B-ATZ}})$ represents how the effect of `augMean` on `Y` changes when `bio` is `B-ATZ` compared to `AMP`.

### State the Hypotheses

**Overall hypothesis**

I'll only state the overall hypothesis for this model as the expansion of the individual hypotheses for each predictor and interactions (all the $\beta$-coefficients in Equation \ref{mod3}) is quite voluminous.

The null is that there is no relationship between the response variable `Y` and the predictors (including their interactions):

- $H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = \beta_7 = \beta_8 = \beta_9 = \beta_{10} = \beta_{11} = \beta_{12} = \beta_{13} = \beta_{14} = \beta_{15} = 0$

The alternative is that at least one predictor or interaction term has a significant relationship with the response variable `Y`:

- $H_A: \text{At least one } \beta_i \neq 0 \text{ for } i \in \{1, 2, ..., 15\}$

### Fit the Model

```{r}
#| echo: false
#| eval: false
sw_sub2 <- sw |>
  dplyr::select(Y, augMean, febSD, augSD, bio)

mod3a <- lm(Y ~ augMean + febSD + augSD, data = sw_sub2)

initial_formula <- as.formula("Y ~ .")

threshold <- 10 # Define a threshold for VIF values

# Extract the names of the predictor variables
predictors <- names(vif(full_mod1))

# Iteratively remove collinear variables
while (TRUE) {
  # Calculate VIF values
  vif_values <- vif(full_mod1)
  print(vif_values) # Print VIF values for debugging
  max_vif <- max(vif_values)
  
  # Check if the maximum VIF is above the threshold
  if (max_vif > threshold) {
    # Find the variable with the highest VIF
    high_vif_var <- names(which.max(vif_values))
    cat("Removing variable:",
        high_vif_var,
        "with VIF:",
        max_vif,
        "\n")
    
    # Update the formula to exclude the high VIF variable
    updated_formula <- as.formula(paste("Y ~ . -", high_vif_var))
    
    # Refit the model without the high VIF variable
    full_mod1 <- lm(updated_formula, data = sw_sub2)
    
    # Update the environment data frame to reflect the removal
    sw_sub2 <- sw_sub2[, !(names(sw_sub2) %in% high_vif_var)]
  } else {
    break
  }
}
```

In @sec-example1 I included the ECTZ seaweed flora in my analysis, but here I expand it to the full dataset. To assure myself that there is not a high degree of multicollinearity between the predictors, I have calculated the variance inflation factors (VIFs) for the full model (not shown). This allowed me to retain the same three predictors used in `mod1`, i.e. `augMean`, `febSD`, and `augSD`. This is the point of departure for `mod3`.

Now I fit the model with those three continuous predictors and their interactions with the categorical variable `bio`.

```{r}
# Make a dataframe with only the relevant columns
sw_sub2 <- sw |>
  dplyr::select(Y, augMean, febSD, augSD, bio)

# Fit the multiple linear regression model with interaction terms
full_mod3 <- lm(Y ~ (augMean + febSD + augSD) * bio, data = sw_sub2)
full_mod3a <- lm(Y ~ augMean + febSD + augSD, data = sw_sub2)
null_mod3 <- lm(Y ~ 1, data = sw_sub2)
```

Model `full_mod3a` is similar to `full_mod3` but without the interaction terms. This will allow me to compare the two models and assess the importance of the interactions.

```{r}
# Compare the models
anova(full_mod3, full_mod3a)
AIC(full_mod3, full_mod3a)
```

The AIC value for `full_mod3` is lower than that of `full_mod3a`, indicating that including the interaction with `bio` is necessary. Likewise, the ANOVA test also shows that the full model (lower residual sum of squares) is significantly better than the reduced model.

I therefore use `full_mod3` going forward. This is a complex model so I have used the stepwise selection function, `stepAIC()`, to identify the most important predictors and interactions (code and output not shown). I hoped that this might have simplified the model somewhat, but the simplification I had hoped for did not materialise.

```{r}
#| echo: false
#| include: false

# Perform forward selection
mod3 <- stepAIC(null_mod3,
                scope = list(lower = null_mod3, upper = full_mod3),
                direction = "forward")
```

### Interpret the Model

**The model summary**

The model summary provides a detailed look at the individual predictors and their interactions in the model.

```{r}
# Summary of the model
summary(mod3) # full_mod3 renamed to mod3 during stepAIC()
```

The first thing to notice is that the model function has been rewritten in the forward selection process (but none of the variables were deemed insignificant and removed):

* Initial specification: `Y ~ (augMean + febSD + augSD) * bio`
* Specification after `stepAIC()`: `Y ~ augMean + bio + augSD + febSD + augMean:bio + bio:augSD + bio:febSD`

Functionally, these two are identical, but the order in which the terms are presented differs. Although this has affected the order in which the coefficients are presented in the summary output, the coefficients are the same. The coefficients are:

* `(Intercept)`: This is the estimated average value of `Y` when all predictor variables are zero and the observation is in the reference bioregion (`AMP`).
* Main Effects:
    * `augMean`:  For every one-unit increase in `augMean`, `Y` increases by 0.3441, on average, assuming all other predictors are held constant. This effect is highly significant.
    * `augSD` and `febSD`: The main effects of these variables are not statistically significant, suggesting they might not have a direct impact on `Y` when averaged across all bioregions.
    * `bioB-ATZ`, `bioBMP`, `bioECTZ`: These coefficients represent the average difference in `Y` between each of these bioregions and the reference bioregion, when the continuous predictors are held at zero.
* Interaction Effects:
    * `augMean` interactions: The significant interactions of `augMean` with bioregion indicate that the effect of `augMean` on `Y` varies across bioregions. Notably, the interaction with `bioBMP` has a strong, significant negative effect, suggesting that the positive effect of `augMean` is much weaker in this bioregion compared to the reference.
    * `augSD` and `febSD` interactions: These interactions with bioregions are sometimes significant, providing good support for the alternative hypothesis that the effects of `augSD` and `febSD` on `Y` depend on the specific bioregion.

Since dummy coding returns differences with respect to reference levels, how would we calculate the actual coefficients for, say, `augMean`? Since there are significant interaction effects, we must consider the main effect of `augMean` in conjunction with bioregion.

For `bio = B-ATZ`:

* $\beta_{\text{augMean}} + \beta_{\text{augMean:bioB-ATZ}} = 0.3441099 + (-0.0461775) = 0.2979324$

For `bio = BMP`:

* $\beta_{\text{augMean}} + \beta_{\text{augMean:bioBMP}} = 0.3441099 + (-0.2406297) = 0.1034802$

For `bio = ECTZ`:

  $\beta_{\text{augMean}} + \beta_{\text{augMean:bioECTZ}} = 0.3441099 + (-0.0607745) = 0.2833354$

The respective SEs for these coefficients can be calculated using the formula for the standard error of the sum of two variables. For example:

* $SE_{\text{augMean}} = \sqrt{SE_{\text{augMean}}^2 + SE_{\text{augMean:bio}}^2}$

**The ANOVA table**

The ANOVA table assesses the overall significance of groups of predictors or the sequential addition of predictors to the model.

```{r}
anova(mod3)
```

The ANOVA table shows that the model is highly significant, with very low *p*-values throughout (< 0.0001). This indicates that the model as a whole is a good fit for the data.

### Reporting

Here is what the reporting of the findings could look like in the Results section in your favourite journal.

**Results**

A multiple linear regression model examining the effects of the August climatological mean temperature (`augMean`), the August and February climatological SD of temperature (`augSD` and `febSD`, respectively), and the bioregion classification (`bio`) on the response variable, the Sørensen dissimilarity (`Y`), including their interaction terms, revealed several significant findings (@tbl-results). This model allows a separate regression slope for each predictor within the bioregions (@fig-mlr6). The model explains a substantial portion of the variance in `Y` ($R^2 = 0.780$, adjusted $R^2 = 0.776$), and the overall model fit is highly significant ($F(15, 954) = 225.1$, $p < 0.0001$).

| Coefficient                 | Estimate | Std. Error | t value | P-value        |
|-----------------------------|----------|------------|---------|----------------|
| `(Intercept)`               | 0.0299   | 0.0063     | 4.766   | < 0.0001 ***   |
| `augMean`                   | 0.3441   | 0.0159     | 21.700  | < 0.0001 ***   |
| `bioB-ATZ`                  | -0.0460  | 0.0243     | -1.895  | > 0.05         |
| `bioBMP`                    | 0.0161   | 0.0101     | 1.596   | > 0.05         |
| `bioECTZ`                   | -0.0015  | 0.0090     | -0.171  | > 0.05         |
| `augSD`                     | -0.0059  | 0.0034     | -1.735  | > 0.05         |
| `febSD`                     | -0.0006  | 0.0028     | -0.232  | > 0.05         |
| `augMean:bioB-ATZ`          | -0.0462  | 0.0874     | -0.528  | > 0.05         |
| `augMean:bioBMP`            | -0.2406  | 0.0211     | -11.382 | < 0.0005 ***   |
| `augMean:bioECTZ`           | -0.0608  | 0.0189     | -3.215  | < 0.005 **     |
| `bioB-ATZ:augSD`            | 0.0656   | 0.0371     | 1.768   | > 0.05         |
| `bioBMP:augSD`              | 0.0410   | 0.0115     | 3.576   | < 0.0005 ***   |
| `bioECTZ:augSD`             | 0.0281   | 0.0054     | 5.219   | < 0.0005 ***   |
| `bioB-ATZ:febSD`            | 0.0409   | 0.0819     | 0.500   | > 0.05         |
| `bioBMP:febSD`              | 0.0056   | 0.0150     | 0.376   | > 0.05         |
| `bioECTZ:febSD`             | 0.0503   | 0.0082     | 6.113   | < 0.0005 ***   |

: Summary of the multiple linear regression model examining the effects of `augMean`, `augSD`, `febSD`, and `bio` on `Y`. {#tbl-results}

The main effect of `augMean` was highly significant (Estimate = 0.3441, $p < 0.0001$), indicating a strong positive relationship with `Y`. The interaction term `augMean:bioBMP` (Estimate = -0.2406, $p < 0.0001$) and `augMean:bioECTZ` (Estimate = -0.0608, $p < 0.005$) were also significant, suggesting that the effect of `augMean` on `Y` varies significantly for `BMP` and `ECTZ` bioregions compared to the reference category (`AMP`). The `bioBMP` (Estimate = 0.0161, $p > 0.05$) and `bioECTZ` (Estimate = -0.0015, $p > 0.05$) terms were not significant, indicating no significant difference from `AMP`.

For `augSD`, the main effect was not significant (Estimate = -0.0059, $p > 0.05$). Significant interaction terms for `bioBMP:augSD` (Estimate = 0.0410, $p < 0.001$) and `bioECTZ:augSD` (Estimate = 0.0281, $p < 0.0001$) indicate that the effect of `augSD` on `Y` varies by bioregion.

The main effect of `febSD` was not significant (Estimate = -0.0006, $p > 0.05$), suggesting no direct relationship with `Y`. However, the interaction term `bioECTZ:febSD` (Estimate = 0.0503, $p = 0.0001$) was significant, indicating that the effect of `febSD` on `Y` differs for the `ECTZ` bioregion.

The ANOVA further highlights the overall significance of each predictor. `augMean` had a highly significant contribution to the model ($F = 2676.902$, $p < 0.0001$), as did `bio` ($F = 106.296$, $p < 0.0001$), and their interactions (`augMean:bio`, $F = 70.647$, $p < 0.0001$; `bio:augSD`, $F = 30.602$, $p < 0.0001$; `bio:febSD`, $F = 12.517$, $p = 4.953 \times 10^{-8}$). The main effect of `augSD` was also significant ($F = 37.331$, $p = 1.451 \times 10^{-9}$), while `febSD` did not significantly contribute to the model on its own ($F = 1.422$, $p = 0.2334$).

These findings suggest that the effects of `augMean`, `augSD`, and `febSD` on `Y` are influenced by the bioregional classification, with significant variations in the relationships depending on the specific bioregion.


```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 11
#| fig.cap: "Individual linear regression fit to the variables `augMean`, `febSD`, and `augSD` for each bioregion as predictors of the seaweed species composition."
#| label: fig-mlr6

# Create a function to generate interaction plots for each predictor
plot_interaction <- function(data, predictor, response, category) {
  ggplot(data, aes_string(x = predictor, y = response,
                          colour = category)) +
    geom_point(shape = 1) +
    geom_smooth(method = "lm",
                formula = y ~ x, se = FALSE, linewidth = 0.4) +
    scale_colour_brewer(palette = "Set1") +
    facet_wrap(~bio, scales = "free") +
    labs(x = predictor,
         y = expression(paste(beta[sim]))) +
    theme_bw()
}

# List of continuous predictors
predictors <- c("augMean", "febSD", "augSD")

# Generate and print interaction plots for each predictor
plts5 <- map(predictors, plot_interaction, data = sw,
             response = "Y", category = "bio")

# Arrange the interaction plots in a grid
ggarrange(plotlist = plts5, ncol = 1, labels = "AUTO",
          nrow = 3, common.legend = TRUE)
```

## Alternative Categorical Variable Coding Schemes (Contrasts) {#sec-contrasts}

Throughout the book, we have used dummy variable coding the specify the categorical variables in the multiple linear regression models. But, should dummy variable coding not be to your liking, there are other coding schemes that can be used to represent the categorical variables. These alternative coding schemes are known as contrasts. The choice of contrast coding can affect the interpretation of the regression coefficients. 

I'll provide some synthetic data to illustrate a few different contrasts. The data consist of a continuous variable `x`, a categorical variable `cat_var` with four levels, and a response variable `y` that has some relationship with `x` and `cat_var`. I'll use dummy variable coding as the reference (haha!).

```{r}
#| echo: false

# Generate artificial data
set.seed(123)  # For reproducibility
n <- 100  # Sample size

# Generate continuous variable
x <- rnorm(n)

# Generate categorical variable with 4 levels
cat_var <- factor(sample(c("A", "B", "C", "D"), n, replace = TRUE))

# Generate response variable with some relationship to cat_var and x
y <- 2 * x + ifelse(cat_var == "A", 3, ifelse(cat_var == "B", 1, ifelse(cat_var == "C", -1, -3))) + rnorm(n)

# Combine into a data frame
data <- data.frame(y, x, cat_var)
```

```{r}
head(data)
```

Categorical variable coding (any scheme) only affects the interpretation of the categorical variable main effects and their interactions, so I'll not discuss the coefficient associated with the continuous variable `x` (the slope) in the model throughout the explanations offered below. 

**Dummy Variable Coding (Treatment Contrasts)**

This is the most commonly used coding scheme, and `lm()`'s default. One level is the reference category (`A`) and the other levels are compared against it. Contrast matrices can be assigned and/or inspected using the `contrasts()` function. For the dummy coding, the reference level `A` will remain 0 and the other levels will be independently coded as 1 in three columns. You'll now understand why, when we have four levels within a categorical variable, we only need three dummy variables to represent them.

```{r}
# Dummy coding (treatment coding) ... default
contrasts(data$cat_var)
```
When we have four levels in a categorical variable, there are three dummy variable columns in the contrast matrix. The first row, consisting of all zeros (0, 0, 0), represents the reference level, which in this case is `A`. The other rows represent the different levels of the categorical variable, with a 1 in the respective column indicating that level. For example, level `A` is represented by (0, 0, 0), `B` by (1, 0, 0), `C` by (0, 1, 0), and `D` by (0, 0, 1). In the regression model, these contrasts are used to estimate the differences between each level and the reference level. Specifically, the first contrast column indicates that the coefficient for this column will represent the difference between the mean of the response variable for level `B` and the mean for the reference level `A`, holding all other variables constant. Similarly, the second and third columns represent the differences between levels `C` and `A`, and `D` and `A`, respectively. This coding allows for a straightforward interpretation of how each level of the categorical variable affects the response variable relative to the reference level.

```{r}
model_dummy <- lm(y ~ x + cat_var, data = data)
summary(model_dummy)
```

The model summary shows that the coefficients for `cat_varB`, `cat_varC`, and `cat_varD` represent the differences in the mean of the response variable `y` between the reference category `A` and categories `B`, `C`, and `D`, respectively, while controlling for the effect of the continuous variable `x`.

Interpretation:

*   `(Intercept)` (2.8176): The intercept represents the estimated mean value of the response (`y`) when `x` is zero and the categorical variable is at the reference level `A`. This is the baseline from which other categories are compared.
*   `x` (1.8274): For each one-unit increase in `x`, `y` is expected to increase by 1.8274 units, holding the categorical variable constant. This effect is consistent across all levels of the categorical variable because the model does not have an interaction effect present.
*   `cat_varB` (-1.7201): On average, the value of `y` for level `B` is 1.7201 units lower than that for the reference level `A`, when `x` is held constant. This corresponds to the (1, 0, 0) row in the contrast matrix.
*   `cat_varC` (-3.9056): Similarly, on average, the value of `y` for level `C` is 3.9056 units lower than that for the reference level, when `x` is held constant. This corresponds to the (0, 1, 0) row in the contrast matrix.
*   `cat_varD` (-5.4880): Lastly, on average, the value of `y` for level `D` is 5.4880 units lower compared to the reference , when `x` is held constant. This is row (0, 0, 1) row in the contrast matrix.

All these coefficients are highly significant (*p* < 0.0001), indicating strong evidence for differences between each category and the reference category `A`.

The model explains a large proportion of the variance in `y` (Adjusted *R*-squared: 0.8822), suggesting a good fit. The *F*-statistic (186.4) with a very low *p*-value (< 0.0001) indicates that the model as a whole is statistically significant.

If you want to change the reference level, you can use the `relevel()` function. For example, to change the reference level of `cat_var` variable to `C_2`, you can use:

```{r}
# Set "C" as the reference level for cat_var
data$cat_var <- relevel(data$cat_var, ref = "C")
contrasts(data$cat_var)
```

This may be useful when you want to compare the other levels to a different reference level.

**Effect Coding (Sum Contrasts)**

This coding method compares the levels of a categorical variable to the overall mean of the dependent variable. The coefficients represent the difference between each level and the grand mean. Instead of using 0 and 1 as we did with dummy variable coding, effect coding uses -1, 0, and 1 to represent the different levels of the categorical variable.

```{r}
# Reset the reference level to "A"
data <- data.frame(y, x, cat_var)

# Effect coding
contrasts(data$cat_var) <- contr.sum(4)
contrasts(data$cat_var)
```

In effect coding (sum contrasts), each level of the categorical variable is compared to the overall mean rather than a specific reference category. This contrast matrix with four levels (A, B, C, D) and three columns can be interpreted as follows:

*   Level `A` (1, 0, 0): The first row indicates that level `A` is included in the first contrast (`cat_var1`), which means the mean of level `A` is being compared to the overall mean. Since the other columns are zero, level `A` does not contribute to the other contrasts.
*   Level `B` (0, 1, 0): The second row indicates that level `B` is included in the second contrast (`cat_var2`). The mean of level `B` is being compared to the overall mean, and it does not contribute to the other contrasts.
*   Level `C` (0, 0, 1): The third row indicates that level `C` is included in the third contrast (`cat_var3`). The mean of level C is being compared to the overall mean, and it does not contribute to the other contrasts.
*   Level `D` (-1, -1, -1): The fourth row is a balancing row, ensuring that the sum of the contrasts for each level equals zero. This indicates that level D is being compared to the overall mean indirectly by balancing the contributions of levels `A`, `B`, and `C`.

```{r}
model_effect <- lm(y ~ x + cat_var, data = data)
summary(model_effect)
```

Interpretation:

*   `(Intercept)` 0.03921: The intercept represents the grand mean of the response variable (`y`). Since the intercept is not statistically significant (*p* > 0.05), it indicates that the overall mean is not significantly different from zero when considering the average effect of all levels of the categorical variable.
*   `x` (1.82741): For each one-unit increase in (`x`), the response (`y`) increases by approximately 1.82741 units. This effect is highly significant (*p* < 0.0001).
*   `cat_var1` (2.77844): Level `A` has a mean (`y`) that is 2.77844 units higher than the grand mean. This effect is highly significant (*p* < 0.0001).
*   `cat_var2` (1.05832): Level `B` has a mean (`y`) that is 1.05832 units higher than the grand mean. This effect is also highly significant (*p* < 0.0001).
*   `cat_var3` (-1.12720): Level `C` has a mean (`y`) that is 1.12720 units lower than the grand mean. This effect is highly significant (p < 0.0001).

All these coefficients are highly significant (*p* < 0.0001), indicating strong evidence for differences between each category and the overall mean of all levels.

The model explains a large proportion of the variance in `y` (Adjusted *R*-squared: 0.8822), suggesting a good fit. The *F*-statistic (186.4) with a very low *p*-value (< 0.0001) indicates that the model as a whole is statistically significant.

**Helmert Coding**

Helmert coding compares each level of a categorical variable to the mean of the subsequent levels. It is useful for testing ordered differences. 

```{r}
# Helmert coding
contrasts(data$cat_var) <- contr.helmert(4)
contrasts(data$cat_var)
```

The contrast matrix for a categorical variable with four levels (`A`, `B`, `C`, `D`) and three columns can be interpreted as follows:

*   Level `A` (-1, -1, -1): Level `A` is compared to the mean of levels `B`, `C`, and `D`. The negative values indicate that level A is being subtracted in these comparisons.
*   Level `B` (1, -1, -1): Level `B` is compared to the mean of levels `C` and `D`. The positive value in the first column indicates that level `B` is being added in this comparison.
*   Level `C` (0, 2, -1): Level `C` is compared to the mean of level `D`. The positive value in the second column indicates that level `C` is being added in this comparison, while the negative value in the third column is part of the comparison for subsequent levels.
*   Level `D` (0, 0, 3): Level `D` is compared on its own in the final contrast. The positive value in the third column indicates that level `D` is being added in this comparison.

```{r}
model_helmert <- lm(y ~ x + cat_var, data = data)
summary(model_helmert)
```

Interpretation:

*   `(Intercept)` (0.03921): The grand mean of  `y`  when  `x`  is zero.
*   `x` (1.82741): For each unit increase in  x ,  y  increases by 1.82741 units.
*   `cat_var1` (-0.86006): The mean of level `A` is 0.86006 units lower than the combined mean of levels `B`, `C`, and `D`.
*   `cat_var2` (-1.01519): The mean of level `B` is 1.01519 units lower than the combined mean of levels `C` and `D`.
*   `cat_var3` (-0.90319): The mean of level `C` is 0.90319 units lower than the mean of level `D`.

The interpretation of the overall model remains more-or-less similar to before:

All these coefficients are highly significant (*p* < 0.0001), indicating strong evidence for differences between each level and the overall mean of all subsequent levels.

The model explains a large proportion of the variance in `y` (Adjusted *R*-squared: 0.8822), suggesting a good fit. The *F*-statistic (186.4) with a very low *p*-value (< 0.0001) indicates that the model as a whole is statistically significant.

## Exercises

::: callout-important
## Task G
Use the data loaded at the start of this chapter for this task.

In this task you will develop data analysis, undertake model building, and provide an interpretation of the findings. Your goal is to explore the species composition and assembly processes of the seaweed flora around the coast of South Africa. See @smit2017seaweeds for more information about the data and the analysis.

a. **Analysis:** Please develop multiple linear regression models for the seaweed species composition ($\beta_\text{sim}$ and $\beta_\text{sne}$, i.e. columns called `Y1` and `Y2`, respectively) using the all the predictors in this dataset. At the end, the final model(s) that best describe(s) the species assembly processes operating along the South African coast should be presented. The final model may/may not contain all the predictors in the dataset, and it is your goal to justify the variable and model selection.

    - Accomplishing a) will require that you work through the whole model-building process as outlined in the chapter. This includes the following steps:
        - Data exploration and visualisation (EDA)
        - Model building (providing hypothesis statements, variable selection using VIF and forward selection, comparisons of nested models, justifications for model selection)
        - Model diagnostics
        - Explanation of `summary()` and `anova()` outputs
        - Producing the Results section
        - **[60%]**

b. **Interpretation:** Once you have arrived at the best model, discuss your findings in the light of the appropriate ecological hypotheses that explain the relationships between the predictors and the seaweed species composition. Include insights drawn from the analysis of $\beta_\text{sør}$ that I developed in this chapter, and also rely on the theory you have developed for the lecture material the class presented in Task A2.

    - Accomplishing b) is thus all about model interpretation and discussing the ecological relevance of the results.
    - **[40%]**
    
The format of this task is a Quarto file that will be converted to an HTML file. The HTML file will contain the graphs, all calculations, and the text sections. The task should be written up as a publication (i.e. use appropriate headings) using a journal style of your choice. Aside from this, there are no limitations.
:::
